# PktMask å½»åº•ç§»é™¤å…¼å®¹å±‚æ–¹æ¡ˆ - æ¡Œé¢åº”ç”¨ä¼˜åŒ–ç‰ˆ

## ğŸ¯ æ–¹æ¡ˆæ¦‚è¿°

### æ ¸å¿ƒç›®æ ‡
**åˆç†åŒ–æ¶æ„ç»Ÿä¸€ï¼Œé¿å…è¿‡åº¦å·¥ç¨‹åŒ–**

- ğŸ¯ **åˆç†é‡æ„**ï¼šåŸºäºå®é™…éœ€æ±‚ï¼Œé¿å…ä¸ºäº†é‡æ„è€Œé‡æ„
- ï¿½ **æ¸è¿›ä¼˜åŒ–**ï¼šä¿æŒæ¡Œé¢åº”ç”¨çš„ç¨³å®šæ€§å’Œç”¨æˆ·ä½“éªŒ
- âš¡ **æ€§èƒ½æå‡**ï¼šæ¶ˆé™¤çœŸæ­£å½±å“æ€§èƒ½çš„å…¼å®¹å±‚å¼€é”€
- ğŸ›¡ï¸ **é£é™©æ§åˆ¶**ï¼šç¡®ä¿ç”¨æˆ·é…ç½®å’Œå·¥ä½œæµç¨‹ä¸å—å½±å“

### æ–¹æ¡ˆä¼˜åŠ¿
- âœ… **æ¶æ„æ¸…æ™°**ï¼šç»Ÿä¸€æŠ½è±¡åŸºç±»ï¼Œå‡å°‘æ¦‚å¿µæ··æ·†
- âœ… **ç”¨æˆ·å‹å¥½**ï¼šä¿æŒç°æœ‰ç”¨æˆ·ä½“éªŒå’Œé…ç½®
- âœ… **ç»´æŠ¤ç®€åŒ–**ï¼šå‡å°‘ä»£ç é‡å¤ï¼Œæé«˜å¯ç»´æŠ¤æ€§
- âœ… **é£é™©å¯æ§**ï¼šåˆ†é˜¶æ®µå®æ–½ï¼Œå¯éšæ—¶å›æ»š

### é£é™©è¯„ä¼°ä¸ç¼“è§£
- âš ï¸ **ç”¨æˆ·é…ç½®å…¼å®¹æ€§**ï¼šé€šè¿‡é…ç½®è¿ç§»å·¥å…·è‡ªåŠ¨å¤„ç†
- âš ï¸ **GUIç¨³å®šæ€§**ï¼šä¼˜å…ˆä¿è¯æ¡Œé¢åº”ç”¨æ ¸å¿ƒåŠŸèƒ½
- âš ï¸ **æµ‹è¯•è¦†ç›–**ï¼šé‡ç‚¹æµ‹è¯•ç”¨æˆ·å¸¸ç”¨å·¥ä½œæµç¨‹
- âš ï¸ **å›æ»šå‡†å¤‡**ï¼šæ¯ä¸ªé˜¶æ®µéƒ½æœ‰æ˜ç¡®çš„å›æ»šæ–¹æ¡ˆ

## ğŸ“… å®æ–½è®¡åˆ’

### æ€»ä½“æ—¶é—´çº¿ï¼š3å‘¨æ¸è¿›å¼é‡æ„

```mermaid
gantt
    title æ¡Œé¢åº”ç”¨å…¼å®¹å±‚ä¼˜åŒ–æ—¶é—´çº¿
    dateFormat  YYYY-MM-DD
    section ç¬¬1å‘¨ï¼šåˆ†æä¸è®¾è®¡
    ç°çŠ¶åˆ†æ        :active, analysis, 2024-01-01, 2d
    ç”¨æˆ·å½±å“è¯„ä¼°    :impact, after analysis, 1d
    æ¶æ„è®¾è®¡        :design, after impact, 2d
    é…ç½®è¿ç§»æ–¹æ¡ˆ    :config, after design, 2d
    section ç¬¬2å‘¨ï¼šæ ¸å¿ƒå®æ–½
    ç»Ÿä¸€åŸºç±»å®ç°    :unified, 2024-01-08, 3d
    GUIé€‚é…æ›´æ–°     :gui, after unified, 2d
    é…ç½®å…¼å®¹å¤„ç†    :compat, after gui, 2d
    section ç¬¬3å‘¨ï¼šæµ‹è¯•ä¸å‘å¸ƒ
    ç”¨æˆ·åœºæ™¯æµ‹è¯•    :testing, 2024-01-15, 3d
    æ€§èƒ½éªŒè¯        :perf, after testing, 2d
    æ–‡æ¡£æ›´æ–°        :docs, after perf, 2d
```

## ğŸ—ï¸ ç¬¬ä¸€å‘¨ï¼šåˆ†æä¸è®¾è®¡

### 1.1 ç°çŠ¶åˆ†æï¼ˆç¬¬1-2å¤©ï¼‰

#### æ¡Œé¢åº”ç”¨æ¶æ„åˆ†æå·¥å…·
```python
#!/usr/bin/env python3
"""
PktMask æ¡Œé¢åº”ç”¨æ¶æ„åˆ†æå·¥å…·
ä¸“æ³¨äºç”¨æˆ·ä½“éªŒå’Œé…ç½®å…¼å®¹æ€§åˆ†æ
"""

import ast
import json
import os
from pathlib import Path
from typing import Dict, List, Set, Optional
from dataclasses import dataclass

@dataclass
class ComponentInfo:
    file_path: str
    component_type: str  # 'stage', 'gui', 'config', 'adapter'
    dependencies: List[str]
    user_facing: bool  # æ˜¯å¦ç›´æ¥å½±å“ç”¨æˆ·ä½“éªŒ
    config_keys: List[str]  # ç›¸å…³çš„é…ç½®é¡¹

@dataclass
class UserImpactAnalysis:
    affected_features: List[str]
    config_changes: Dict[str, str]  # æ—§é…ç½® -> æ–°é…ç½®
    workflow_changes: List[str]
    migration_complexity: str  # 'low', 'medium', 'high'

class DesktopAppAnalyzer:
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.gui_components: List[ComponentInfo] = []
        self.stage_components: List[ComponentInfo] = []
        self.config_dependencies: Dict[str, Set[str]] = {}
        self.user_workflows: List[str] = []

    def analyze_desktop_app(self) -> Dict[str, any]:
        """åˆ†ææ¡Œé¢åº”ç”¨çš„æ¶æ„å’Œç”¨æˆ·å½±å“"""
        self._analyze_gui_components()
        self._analyze_stage_usage()
        self._analyze_config_dependencies()
        self._analyze_user_workflows()

        return {
            "gui_components": self.gui_components,
            "stage_components": self.stage_components,
            "config_dependencies": self.config_dependencies,
            "user_impact": self._assess_user_impact(),
            "migration_strategy": self._generate_migration_strategy()
        }

    def _analyze_gui_components(self):
        """åˆ†æGUIç»„ä»¶å’Œç”¨æˆ·ç•Œé¢"""
        gui_files = [
            "src/pktmask/gui/main_window.py",
            "src/pktmask/gui/managers/",
            "src/pktmask/config/settings.py"
        ]

        for gui_path in gui_files:
            full_path = self.project_root / gui_path
            if full_path.exists():
                self._analyze_gui_file(full_path)

    def _analyze_stage_usage(self):
        """åˆ†æå¤„ç†é˜¶æ®µçš„ä½¿ç”¨æƒ…å†µ"""
        stage_dirs = [
            "src/pktmask/core/pipeline/stages/",
            "src/pktmask/adapters/compatibility/"
        ]

        for stage_dir in stage_dirs:
            stage_path = self.project_root / stage_dir
            if stage_path.exists():
                for py_file in stage_path.rglob("*.py"):
                    self._analyze_stage_file(py_file)

    def _analyze_config_dependencies(self):
        """åˆ†æé…ç½®ä¾èµ–å…³ç³»"""
        config_files = [
            "src/pktmask/config/settings.py",
            "src/pktmask/config/defaults.py",
            "src/pktmask/resources/config_template.yaml"
        ]

        for config_file in config_files:
            config_path = self.project_root / config_file
            if config_path.exists():
                self._extract_config_keys(config_path)

    def _analyze_user_workflows(self):
        """åˆ†æç”¨æˆ·å·¥ä½œæµç¨‹"""
        # åŸºäºGUIä»£ç åˆ†æç”¨æˆ·å¯èƒ½çš„æ“ä½œæµç¨‹
        self.user_workflows = [
            "é€‰æ‹©è¾“å…¥ç›®å½• -> é…ç½®å¤„ç†é€‰é¡¹ -> å¼€å§‹å¤„ç†",
            "åŠ è½½é…ç½®æ–‡ä»¶ -> æ‰¹é‡å¤„ç†",
            "æŸ¥çœ‹å¤„ç†ç»“æœ -> å¯¼å‡ºæŠ¥å‘Š",
            "ä¿®æ”¹é»˜è®¤è®¾ç½® -> ä¿å­˜é…ç½®"
        ]

    def _assess_user_impact(self) -> UserImpactAnalysis:
        """è¯„ä¼°å¯¹ç”¨æˆ·çš„å½±å“"""
        return UserImpactAnalysis(
            affected_features=[
                "å¤„ç†é˜¶æ®µé…ç½®ç•Œé¢",
                "è¿›åº¦æ˜¾ç¤ºå’Œç»Ÿè®¡",
                "é”™è¯¯å¤„ç†å’Œæ¢å¤"
            ],
            config_changes={
                "processing.stages": "processing.pipeline_stages",
                "ui.stage_options": "ui.pipeline_options"
            },
            workflow_changes=[
                "é…ç½®æ–‡ä»¶æ ¼å¼å¯èƒ½éœ€è¦æ›´æ–°",
                "æŸäº›é«˜çº§é€‰é¡¹çš„ä½ç½®å¯èƒ½è°ƒæ•´"
            ],
            migration_complexity="low"
        )

    def _generate_migration_strategy(self) -> Dict[str, any]:
        """ç”Ÿæˆè¿ç§»ç­–ç•¥"""
        return {
            "phase1": {
                "name": "åç«¯ç»Ÿä¸€",
                "description": "ç»Ÿä¸€å¤„ç†é˜¶æ®µåŸºç±»ï¼Œä¸å½±å“ç”¨æˆ·ç•Œé¢",
                "user_impact": "æ— ",
                "rollback_plan": "ä¿ç•™åŸæœ‰é€‚é…å™¨"
            },
            "phase2": {
                "name": "é…ç½®è¿ç§»",
                "description": "è‡ªåŠ¨è¿ç§»ç”¨æˆ·é…ç½®æ–‡ä»¶",
                "user_impact": "é¦–æ¬¡å¯åŠ¨æ—¶è‡ªåŠ¨è¿ç§»",
                "rollback_plan": "å¤‡ä»½åŸé…ç½®æ–‡ä»¶"
            },
            "phase3": {
                "name": "ç•Œé¢ä¼˜åŒ–",
                "description": "ä¼˜åŒ–ç”¨æˆ·ç•Œé¢ï¼Œç§»é™¤å†—ä½™é€‰é¡¹",
                "user_impact": "ç•Œé¢æ›´ç®€æ´",
                "rollback_plan": "ä¿ç•™æ—§ç•Œé¢é€‰é¡¹"
            }
        }

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    analyzer = DesktopAppAnalyzer("/path/to/pktmask")
    results = analyzer.analyze_desktop_app()

    print(f"å‘ç° {len(results['gui_components'])} ä¸ªGUIç»„ä»¶")
    print(f"å‘ç° {len(results['stage_components'])} ä¸ªå¤„ç†é˜¶æ®µ")
    print(f"ç”¨æˆ·å½±å“å¤æ‚åº¦: {results['user_impact'].migration_complexity}")
    print(f"è¿ç§»ç­–ç•¥: {len(results['migration_strategy'])} ä¸ªé˜¶æ®µ")
```

#### ç”¨æˆ·é…ç½®å…¼å®¹æ€§åˆ†æ
```bash
# åˆ†æç°æœ‰ç”¨æˆ·é…ç½®
python analyze_desktop_app.py --config-analysis > user_impact_report.json

# ç”Ÿæˆé…ç½®è¿ç§»è„šæœ¬
python generate_config_migration.py user_impact_report.json
```

### 1.2 ç”¨æˆ·å½±å“è¯„ä¼°ï¼ˆç¬¬3å¤©ï¼‰

#### é…ç½®æ–‡ä»¶å…¼å®¹æ€§æ£€æŸ¥
```python
#!/usr/bin/env python3
"""
ç”¨æˆ·é…ç½®å…¼å®¹æ€§æ£€æŸ¥å·¥å…·
ç¡®ä¿ç°æœ‰ç”¨æˆ·é…ç½®èƒ½å¤Ÿå¹³æ»‘è¿ç§»
"""

import json
import yaml
from pathlib import Path
from typing import Dict, List, Any

class ConfigCompatibilityChecker:
    def __init__(self):
        self.compatibility_map = {
            # æ—§é…ç½®é”® -> æ–°é…ç½®é”®
            "processing.dedup_enabled": "processing.stages.dedup.enabled",
            "processing.anon_enabled": "processing.stages.anon.enabled",
            "processing.mask_enabled": "processing.stages.mask.enabled",
            "ui.stage_options": "ui.pipeline_options"
        }

    def check_user_config(self, config_path: Path) -> Dict[str, Any]:
        """æ£€æŸ¥ç”¨æˆ·é…ç½®çš„å…¼å®¹æ€§"""
        if not config_path.exists():
            return {"status": "no_config", "migration_needed": False}

        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                if config_path.suffix == '.json':
                    config = json.load(f)
                else:
                    config = yaml.safe_load(f)

            return self._analyze_config(config)
        except Exception as e:
            return {"status": "error", "error": str(e)}

    def _analyze_config(self, config: Dict) -> Dict[str, Any]:
        """åˆ†æé…ç½®å†…å®¹"""
        migration_needed = False
        changes = []

        for old_key, new_key in self.compatibility_map.items():
            if self._has_nested_key(config, old_key):
                migration_needed = True
                changes.append({
                    "old_key": old_key,
                    "new_key": new_key,
                    "action": "rename"
                })

        return {
            "status": "analyzed",
            "migration_needed": migration_needed,
            "changes": changes,
            "backup_recommended": migration_needed
        }
```

### 1.3 æ¶æ„è®¾è®¡ï¼ˆç¬¬4-5å¤©ï¼‰

#### æ¡Œé¢åº”ç”¨ä¼˜åŒ–çš„ç»Ÿä¸€åŸºç±»è®¾è®¡
```python
#!/usr/bin/env python3
"""
æ¡Œé¢åº”ç”¨ä¼˜åŒ–çš„ç»Ÿä¸€å¤„ç†é˜¶æ®µåŸºç±»
ä¿æŒç°æœ‰åŠŸèƒ½ï¼Œç®€åŒ–æ¶æ„ï¼Œé¿å…è¿‡åº¦å·¥ç¨‹åŒ–
"""

import abc
import time
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass
from enum import Enum

# ä¿æŒä¸ç°æœ‰ StageStats çš„å…¼å®¹æ€§
@dataclass
class StageStats:
    """å¤„ç†é˜¶æ®µç»Ÿè®¡ä¿¡æ¯ - å…¼å®¹ç°æœ‰æ ¼å¼"""
    stage_name: str = ""
    packets_processed: int = 0
    packets_modified: int = 0
    duration_ms: float = 0.0
    extra_metrics: Dict[str, Any] = None

    def __post_init__(self):
        if self.extra_metrics is None:
            self.extra_metrics = {}

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼Œä¿æŒå‘åå…¼å®¹"""
        return {
            "stage_name": self.stage_name,
            "packets_processed": self.packets_processed,
            "packets_modified": self.packets_modified,
            "duration_ms": self.duration_ms,
            **self.extra_metrics
        }

class StageBase(metaclass=abc.ABCMeta):
    """
    ç»Ÿä¸€çš„å¤„ç†é˜¶æ®µåŸºç±» - æ¡Œé¢åº”ç”¨ä¼˜åŒ–ç‰ˆ

    è®¾è®¡åŸåˆ™ï¼š
    1. ä¿æŒç°æœ‰æ¥å£å…¼å®¹æ€§
    2. ç®€åŒ–ä¸å¿…è¦çš„å¤æ‚æ€§
    3. ä¸“æ³¨äºæ¡Œé¢åº”ç”¨éœ€æ±‚
    4. é¿å…è¿‡åº¦æŠ½è±¡
    """

    # ç±»å±æ€§ - ä¿æŒç°æœ‰å‘½åçº¦å®š
    name: str = "UnnamedStage"
    _initialized: bool = False

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self._config = config or {}
        self._logger = self._create_logger()

    def _create_logger(self):
        """åˆ›å»ºæ—¥å¿—è®°å½•å™¨"""
        import logging
        return logging.getLogger(f"stage.{self.__class__.__name__.lower()}")

    # -------------------------------------------------------------------------
    # ç”Ÿå‘½å‘¨æœŸç®¡ç† - ä¿æŒç°æœ‰æ¥å£
    # -------------------------------------------------------------------------

    def initialize(self, config: Optional[Dict] = None) -> None:
        """åˆå§‹åŒ–é˜¶æ®µ - ä¿æŒç°æœ‰ç­¾å"""
        if config:
            self._config.update(config)

        try:
            self._do_initialize()
            self._initialized = True
            self._logger.info(f"{self.name} initialized successfully")
        except Exception as e:
            self._logger.error(f"Failed to initialize {self.name}: {e}")
            raise

    def _do_initialize(self) -> None:
        """å­ç±»å¯é‡å†™çš„åˆå§‹åŒ–é€»è¾‘"""
        pass

    # -------------------------------------------------------------------------
    # æ ¸å¿ƒå¤„ç†æ¥å£ - ä¿æŒç°æœ‰ç­¾å
    # -------------------------------------------------------------------------

    @abc.abstractmethod
    def process_file(self, input_path: Union[str, Path], output_path: Union[str, Path]) -> Union[StageStats, Dict, None]:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶ - ä¿æŒç°æœ‰æ¥å£ç­¾å

        Args:
            input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        Returns:
            StageStats æˆ–å…¼å®¹å­—å…¸ï¼Œä¿æŒå‘åå…¼å®¹
        """
        pass

    # -------------------------------------------------------------------------
    # ç›®å½•çº§ç”Ÿå‘½å‘¨æœŸ - ä¿æŒç°æœ‰æ¥å£
    # -------------------------------------------------------------------------

    def prepare_for_directory(self, directory: Union[str, Path], all_files: List[str]) -> None:
        """ç›®å½•å¤„ç†å‰çš„å‡†å¤‡å·¥ä½œ"""
        pass

    def finalize_directory_processing(self) -> Optional[Dict]:
        """ç›®å½•å¤„ç†å®Œæˆåçš„æ¸…ç†å·¥ä½œ"""
        return None

    # -------------------------------------------------------------------------
    # å·¥å…·é“¾æ£€æµ‹ - ä¿æŒç°æœ‰æ¥å£
    # -------------------------------------------------------------------------

    def get_required_tools(self) -> List[str]:
        """è·å–ä¾èµ–çš„å¤–éƒ¨å·¥å…·åˆ—è¡¨"""
        return []

    def stop(self) -> None:
        """åœæ­¢å¤„ç†"""
        pass

    # -------------------------------------------------------------------------
    # é…ç½®ç®¡ç† - ç®€åŒ–ç‰ˆæœ¬
    # -------------------------------------------------------------------------

    @property
    def config(self) -> Dict[str, Any]:
        """è·å–å½“å‰é…ç½®"""
        return self._config.copy()

    def update_config(self, config: Dict[str, Any]) -> None:
        """æ›´æ–°é…ç½®"""
        self._config.update(config)
        self._logger.debug(f"Config updated for {self.name}")

    # -------------------------------------------------------------------------
    # è¾…åŠ©æ–¹æ³• - ä¿æŒç®€å•
    # -------------------------------------------------------------------------

    def __str__(self) -> str:
        return f"{self.name} ({'initialized' if self._initialized else 'not initialized'})"

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}(name='{self.name}')"


# å‘åå…¼å®¹çš„åˆ«å
UnifiedStageBase = StageBase  # ä¸ºäº†æ–‡æ¡£ä¸€è‡´æ€§

# ç®€åŒ–çš„å·¥å‚å‡½æ•°
def create_stage(stage_type: str, config: Optional[Dict[str, Any]] = None) -> StageBase:
    """
    åˆ›å»ºå¤„ç†é˜¶æ®µå®ä¾‹

    Args:
        stage_type: é˜¶æ®µç±»å‹ ('dedup', 'anon', 'mask')
        config: é…ç½®å­—å…¸

    Returns:
        å¤„ç†é˜¶æ®µå®ä¾‹
    """
    # ä½¿ç”¨ç°æœ‰çš„é˜¶æ®µå®ç°ï¼Œé¿å…é‡å¤é€ è½®å­
    stage_mapping = {
        'dedup': 'pktmask.core.pipeline.stages.dedup.DedupStage',
        'anon': 'pktmask.core.pipeline.stages.anon_ip.AnonStage',
        'mask': 'pktmask.core.pipeline.stages.mask_payload.MaskPayloadStage'
    }

    if stage_type not in stage_mapping:
        raise ValueError(f"Unknown stage type: {stage_type}")

    module_path, class_name = stage_mapping[stage_type].rsplit('.', 1)

    try:
        import importlib
        module = importlib.import_module(module_path)
        stage_class = getattr(module, class_name)

        # åˆ›å»ºå®ä¾‹å¹¶åˆå§‹åŒ–
        stage = stage_class()
        if config:
            stage.initialize(config)
        else:
            stage.initialize()

        return stage
    except ImportError as e:
        raise RuntimeError(f"Failed to import stage {stage_type}: {e}")
```

### 1.4 é…ç½®è¿ç§»æ–¹æ¡ˆï¼ˆç¬¬6-7å¤©ï¼‰

#### ç”¨æˆ·é…ç½®è‡ªåŠ¨è¿ç§»å·¥å…·
```python
#!/usr/bin/env python3
"""
ç”¨æˆ·é…ç½®è‡ªåŠ¨è¿ç§»å·¥å…·
ç¡®ä¿ç°æœ‰ç”¨æˆ·é…ç½®å¹³æ»‘è¿‡æ¸¡åˆ°æ–°æ¶æ„
"""

import json
import yaml
import shutil
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

class ConfigMigrator:
    def __init__(self):
        self.migration_rules = {
            # é…ç½®é”®æ˜ å°„è§„åˆ™
            "processing.dedup_enabled": "processing.stages.dedup.enabled",
            "processing.anon_enabled": "processing.stages.anon.enabled",
            "processing.mask_enabled": "processing.stages.mask.enabled",
            "ui.stage_options": "ui.pipeline_options",

            # å€¼è½¬æ¢è§„åˆ™
            "processing.dedup_algorithm": {
                "old_values": ["md5", "sha1", "sha256"],
                "new_key": "processing.stages.dedup.algorithm",
                "default": "sha256"
            }
        }

        self.backup_suffix = f".backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    def migrate_user_config(self, config_path: Path) -> Dict[str, Any]:
        """è¿ç§»ç”¨æˆ·é…ç½®æ–‡ä»¶"""
        if not config_path.exists():
            return {"status": "no_config", "action": "none"}

        # å¤‡ä»½åŸé…ç½®
        backup_path = config_path.with_suffix(config_path.suffix + self.backup_suffix)
        shutil.copy2(config_path, backup_path)

        try:
            # åŠ è½½åŸé…ç½®
            with open(config_path, 'r', encoding='utf-8') as f:
                if config_path.suffix == '.json':
                    config = json.load(f)
                else:
                    config = yaml.safe_load(f)

            # æ‰§è¡Œè¿ç§»
            migrated_config = self._migrate_config_dict(config)

            # ä¿å­˜æ–°é…ç½®
            with open(config_path, 'w', encoding='utf-8') as f:
                if config_path.suffix == '.json':
                    json.dump(migrated_config, f, indent=2, ensure_ascii=False)
                else:
                    yaml.dump(migrated_config, f, default_flow_style=False,
                             allow_unicode=True, indent=2)

            return {
                "status": "migrated",
                "backup_path": str(backup_path),
                "changes": self._get_migration_summary(config, migrated_config)
            }

        except Exception as e:
            # æ¢å¤å¤‡ä»½
            shutil.copy2(backup_path, config_path)
            return {"status": "error", "error": str(e)}

    def _migrate_config_dict(self, config: Dict) -> Dict:
        """è¿ç§»é…ç½®å­—å…¸"""
        migrated = config.copy()

        # åº”ç”¨ç®€å•çš„é”®é‡å‘½å
        for old_key, new_key in self.migration_rules.items():
            if isinstance(new_key, str) and self._has_nested_key(config, old_key):
                value = self._get_nested_value(config, old_key)
                self._set_nested_value(migrated, new_key, value)
                self._delete_nested_key(migrated, old_key)

        # æ·»åŠ æ–°çš„é»˜è®¤é…ç½®
        self._add_new_defaults(migrated)

        return migrated

    def _add_new_defaults(self, config: Dict) -> None:
        """æ·»åŠ æ–°æ¶æ„çš„é»˜è®¤é…ç½®"""
        # ç¡®ä¿æ–°çš„é…ç½®ç»“æ„å­˜åœ¨
        if 'processing' not in config:
            config['processing'] = {}

        if 'stages' not in config['processing']:
            config['processing']['stages'] = {
                'dedup': {'enabled': True, 'algorithm': 'sha256'},
                'anon': {'enabled': True, 'preserve_subnet': True},
                'mask': {'enabled': False, 'preserve_tls': True}
            }

    def _has_nested_key(self, d: Dict, key: str) -> bool:
        """æ£€æŸ¥åµŒå¥—é”®æ˜¯å¦å­˜åœ¨"""
        keys = key.split('.')
        current = d
        for k in keys:
            if not isinstance(current, dict) or k not in current:
                return False
            current = current[k]
        return True

    def _get_nested_value(self, d: Dict, key: str) -> Any:
        """è·å–åµŒå¥—é”®çš„å€¼"""
        keys = key.split('.')
        current = d
        for k in keys:
            current = current[k]
        return current

    def _set_nested_value(self, d: Dict, key: str, value: Any) -> None:
        """è®¾ç½®åµŒå¥—é”®çš„å€¼"""
        keys = key.split('.')
        current = d
        for k in keys[:-1]:
            if k not in current:
                current[k] = {}
            current = current[k]
        current[keys[-1]] = value

    def _delete_nested_key(self, d: Dict, key: str) -> None:
        """åˆ é™¤åµŒå¥—é”®"""
        keys = key.split('.')
        current = d
        for k in keys[:-1]:
            if k not in current:
                return
            current = current[k]
        if keys[-1] in current:
            del current[keys[-1]]
```

        # æµ‹è¯•ç”¨ä¾‹æ¨¡æ¿
        test_template = Template('''
#!/usr/bin/env python3
"""
{{ class_name }} æµ‹è¯•ç”¨ä¾‹
è‡ªåŠ¨ç”Ÿæˆçš„æµ‹è¯•ä»£ç 
"""

import pytest
import tempfile
from pathlib import Path
from unittest.mock import Mock, patch

from {{ module_path }} import {{ class_name }}
from ..core.unified_stage import StageStatus, StageResult


class Test{{ class_name }}:
    """{{ class_name }} æµ‹è¯•ç±»"""

    def setup_method(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.stage = {{ class_name }}()
        self.temp_dir = tempfile.mkdtemp()

    def teardown_method(self):
        """æµ‹è¯•åæ¸…ç†"""
        self.stage.cleanup()
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)

    def test_initialization(self):
        """æµ‹è¯•åˆå§‹åŒ–"""
        assert self.stage.status == StageStatus.NOT_INITIALIZED

        self.stage.initialize()
        assert self.stage.status == StageStatus.READY
        assert self.stage.is_ready

    def test_configuration(self):
        """æµ‹è¯•é…ç½®ç®¡ç†"""
        config = {'test_param': 'test_value'}
        self.stage.update_config(config)

        assert self.stage.config['test_param'] == 'test_value'

    def test_config_validation(self):
        """æµ‹è¯•é…ç½®éªŒè¯"""
        errors = self.stage.validate_config()
        assert isinstance(errors, list)

    def test_dependencies(self):
        """æµ‹è¯•ä¾èµ–æ£€æŸ¥"""
        tools = self.stage.get_required_tools()
        assert isinstance(tools, list)

        deps = self.stage.check_dependencies()
        assert isinstance(deps, dict)

    @patch('{{ module_path }}.{{ processor_class }}')
    def test_process_file_success(self, mock_processor_class):
        """æµ‹è¯•æ–‡ä»¶å¤„ç†æˆåŠŸåœºæ™¯"""
        # è®¾ç½®æ¨¡æ‹Ÿ
        mock_processor = Mock()
        mock_processor.process_file.return_value = Mock(
            success=True,
            stats={'total_packets': 100, '{{ modified_packets_key }}': 50}
        )
        mock_processor_class.return_value = mock_processor

        # åˆå§‹åŒ–å¹¶æµ‹è¯•
        self.stage.initialize()

        input_file = Path(self.temp_dir) / "input.pcap"
        output_file = Path(self.temp_dir) / "output.pcap"

        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        input_file.touch()

        result = self.stage.process_file_with_monitoring(input_file, output_file)

        assert result.success
        assert result.metrics.packets_processed == 100
        assert result.metrics.packets_modified == 50
        assert result.metrics.duration_ms > 0

    @patch('{{ module_path }}.{{ processor_class }}')
    def test_process_file_failure(self, mock_processor_class):
        """æµ‹è¯•æ–‡ä»¶å¤„ç†å¤±è´¥åœºæ™¯"""
        # è®¾ç½®æ¨¡æ‹Ÿ
        mock_processor = Mock()
        mock_processor.process_file.side_effect = Exception("Test error")
        mock_processor_class.return_value = mock_processor

        # åˆå§‹åŒ–å¹¶æµ‹è¯•
        self.stage.initialize()

        input_file = Path(self.temp_dir) / "input.pcap"
        output_file = Path(self.temp_dir) / "output.pcap"
        input_file.touch()

        result = self.stage.process_file_with_monitoring(input_file, output_file)

        assert not result.success
        assert "Test error" in result.error_message
        assert result.metrics.duration_ms > 0

    def test_diagnostic_info(self):
        """æµ‹è¯•è¯Šæ–­ä¿¡æ¯"""
        info = self.stage.get_diagnostic_info()

        assert 'name' in info
        assert 'version' in info
        assert 'status' in info
        assert 'config' in info
        assert 'metrics' in info
        assert 'dependencies' in info

    def test_string_representation(self):
        """æµ‹è¯•å­—ç¬¦ä¸²è¡¨ç¤º"""
        str_repr = str(self.stage)
        assert self.stage.name in str_repr
        assert self.stage.version in str_repr

        repr_str = repr(self.stage)
        assert self.stage.__class__.__name__ in repr_str
''')

        return {
            'stage': stage_template,
            'test': test_template
        }

    def generate_migration_files(self) -> Dict[str, str]:
        """ç”Ÿæˆè¿ç§»åçš„æ–‡ä»¶"""
        generated_files = {}

        # ä¸ºæ¯ä¸ª ProcessingStep å­ç±»ç”Ÿæˆæ–°å®ç°
        for usage in self.analysis_results['processing_step_classes']:
            class_info = self._analyze_class_details(usage)

            # ç”Ÿæˆæ–°çš„é˜¶æ®µå®ç°
            stage_code = self.templates['stage'].render(**class_info)
            stage_file = f"src/pktmask/stages/{class_info['module_name']}.py"
            generated_files[stage_file] = stage_code

            # ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
            test_code = self.templates['test'].render(**class_info)
            test_file = f"tests/stages/test_{class_info['module_name']}.py"
            generated_files[test_file] = test_code

        return generated_files

    def _analyze_class_details(self, usage) -> Dict[str, Any]:
        """åˆ†æç±»çš„è¯¦ç»†ä¿¡æ¯ï¼Œç”Ÿæˆæ¨¡æ¿å‚æ•°"""

        # æ ¹æ®ç±»åæ¨æ–­å¤„ç†å™¨ä¿¡æ¯
        class_name = usage.class_name

        if "Dedup" in class_name:
            processor_info = {
                'processor_name': 'deduplicator',
                'processor_class': 'DeduplicationProcessor',
                'modified_packets_key': 'removed_count',
                'required_tools': '[]',
                'description': 'Removes duplicate packets from PCAP files'
            }
        elif "Anon" in class_name or "IP" in class_name:
            processor_info = {
                'processor_name': 'ip_anonymizer',
                'processor_class': 'IPAnonymizer',
                'modified_packets_key': 'anonymized_packets',
                'required_tools': '[]',
                'description': 'Anonymizes IP addresses in PCAP files'
            }
        elif "Mask" in class_name or "Payload" in class_name:
            processor_info = {
                'processor_name': 'payload_masker',
                'processor_class': 'PayloadMasker',
                'modified_packets_key': 'masked_packets',
                'required_tools': '["tshark", "editcap"]',
                'description': 'Masks payload data in PCAP files'
            }
        else:
            processor_info = {
                'processor_name': 'generic_processor',
                'processor_class': 'GenericProcessor',
                'modified_packets_key': 'modified_packets',
                'required_tools': '[]',
                'description': f'Generic processing stage: {class_name}'
            }

        return {
            'class_name': class_name,
            'original_file': usage.file_path,
            'module_name': class_name.lower().replace('stage', ''),
            'module_path': f"pktmask.stages.{class_name.lower().replace('stage', '')}",
            'stage_name': class_name.replace('Stage', ''),
            'config_validations': self._generate_config_validations(class_name),
            **processor_info
        }

    def _generate_config_validations(self, class_name: str) -> List[str]:
        """ç”Ÿæˆé…ç½®éªŒè¯ä»£ç """
        validations = []

        if "Dedup" in class_name:
            validations.append("""
        algorithm = self._config.get('algorithm', 'sha256')
        if algorithm not in ['md5', 'sha1', 'sha256']:
            errors.append(f"Invalid deduplication algorithm: {algorithm}")
            """)

        if "Anon" in class_name:
            validations.append("""
        preserve_subnet = self._config.get('preserve_subnet_structure', True)
        if not isinstance(preserve_subnet, bool):
            errors.append("preserve_subnet_structure must be boolean")
            """)

        if "Mask" in class_name:
            validations.append("""
        preserve_tls = self._config.get('preserve_tls_handshake', True)
        if not isinstance(preserve_tls, bool):
            errors.append("preserve_tls_handshake must be boolean")
            """)

        return validations

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åŠ è½½åˆ†æç»“æœ
    import json
    with open('dependency_report.json', 'r') as f:
        analysis_results = json.load(f)

    # ç”Ÿæˆè¿ç§»æ–‡ä»¶
    migrator = CodeMigrator(analysis_results)
    generated_files = migrator.generate_migration_files()

    # å†™å…¥æ–‡ä»¶
    for file_path, content in generated_files.items():
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"Generated: {file_path}")
```

## ğŸ”„ ç¬¬äºŒå‘¨ï¼šæ ¸å¿ƒå®æ–½

### 2.1 ç»Ÿä¸€åŸºç±»å®ç°ï¼ˆç¬¬1-3å¤©ï¼‰

#### æ¸è¿›å¼è¿ç§»è„šæœ¬
```bash
#!/bin/bash
"""
æ¡Œé¢åº”ç”¨å…¼å®¹å±‚ä¼˜åŒ–è„šæœ¬
æ¸è¿›å¼å®æ–½ï¼Œç¡®ä¿ç”¨æˆ·ä½“éªŒä¸å—å½±å“
"""

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

echo "ğŸš€ å¼€å§‹æ¡Œé¢åº”ç”¨å…¼å®¹å±‚ä¼˜åŒ–..."

# ç¬¬ä¸€æ­¥ï¼šå¤‡ä»½ç”¨æˆ·é…ç½®
echo "ï¿½ å¤‡ä»½ç”¨æˆ·é…ç½®..."
python tools/backup_user_configs.py
echo "âœ… ç”¨æˆ·é…ç½®å¤‡ä»½å®Œæˆ"

# ç¬¬äºŒæ­¥ï¼šåˆ†æç°çŠ¶
echo "ğŸ“Š åˆ†ææ¡Œé¢åº”ç”¨æ¶æ„..."
python tools/analyze_desktop_app.py > reports/desktop_analysis.json
echo "âœ… æ¶æ„åˆ†æå®Œæˆ"

# ç¬¬ä¸‰æ­¥ï¼šå®æ–½ç»Ÿä¸€åŸºç±»
echo "ğŸ—ï¸ å®æ–½ç»Ÿä¸€åŸºç±»..."
python tools/implement_unified_base.py
echo "âœ… ç»Ÿä¸€åŸºç±»å®æ–½å®Œæˆ"

# ç¬¬å››æ­¥ï¼šæ›´æ–°é€‚é…å™¨
echo "ï¿½ æ›´æ–°å…¼å®¹æ€§é€‚é…å™¨..."
python tools/update_adapters.py
echo "âœ… é€‚é…å™¨æ›´æ–°å®Œæˆ"

# ç¬¬äº”æ­¥ï¼šè¿ç§»ç”¨æˆ·é…ç½®
echo "âš™ï¸ è¿ç§»ç”¨æˆ·é…ç½®..."
python tools/migrate_user_configs.py
echo "âœ… é…ç½®è¿ç§»å®Œæˆ"

# ç¬¬å…­æ­¥ï¼šéªŒè¯GUIåŠŸèƒ½
echo "ğŸ§ª éªŒè¯GUIåŠŸèƒ½..."
python tools/test_gui_functionality.py
echo "âœ… GUIåŠŸèƒ½éªŒè¯é€šè¿‡"

echo "ğŸ‰ ç¬¬ä¸€é˜¶æ®µå®æ–½å®Œæˆï¼"
```

#### æ™ºèƒ½å¯¼å…¥æ›´æ–°å·¥å…·
```python
#!/usr/bin/env python3
"""
æ™ºèƒ½å¯¼å…¥è¯­å¥æ›´æ–°å·¥å…·
ä¿æŒå‘åå…¼å®¹ï¼Œé¿å…ç ´åæ€§å˜æ›´
"""

import re
import os
from pathlib import Path
from typing import Dict, List, Set

class SmartImportUpdater:
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)

        # ä¿å®ˆçš„å¯¼å…¥æ˜ å°„ - åªæ›´æ–°æ˜ç¡®éœ€è¦çš„
        self.import_mappings = {
            # åªæ›´æ–°å†…éƒ¨å®ç°ï¼Œä¿æŒæ¥å£å…¼å®¹
            'from pktmask.core.base_step import ProcessingStep':
                'from pktmask.core.pipeline.base_stage import StageBase',
            'from ..core.base_step import ProcessingStep':
                'from ..core.pipeline.base_stage import StageBase',
        }

        # ç±»ç»§æ‰¿æ˜ å°„ - æ›´ä¿å®ˆ
        self.class_mappings = {
            'ProcessingStep': 'StageBase'  # ç»Ÿä¸€åˆ°ç°æœ‰çš„ StageBase
        }

        # éœ€è¦è·³è¿‡çš„æ–‡ä»¶ - é¿å…ç ´åå…¼å®¹å±‚
        self.skip_files = {
            'src/pktmask/core/base_step.py',  # ä¿ç•™å…¼å®¹å±‚
            'src/pktmask/adapters/compatibility/',  # ä¿ç•™å…¼å®¹é€‚é…å™¨
        }

    def update_all_files(self) -> Dict[str, int]:
        """æ›´æ–°æ‰€æœ‰æ–‡ä»¶çš„å¯¼å…¥è¯­å¥"""
        results = {'files_updated': 0, 'imports_updated': 0}

        for py_file in self.project_root.rglob("*.py"):
            if self._should_skip_file(py_file):
                continue

            updated = self._update_file(py_file)
            if updated:
                results['files_updated'] += 1
                results['imports_updated'] += updated

        return results

    def _should_skip_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡æ–‡ä»¶"""
        skip_patterns = [
            '__pycache__',
            '.git',
            'backup/',
            'tools/',
            'unified_stage.py'  # è·³è¿‡æ–°çš„åŸºç±»æ–‡ä»¶
        ]

        return any(pattern in str(file_path) for pattern in skip_patterns)

    def _update_file(self, file_path: Path) -> int:
        """æ›´æ–°å•ä¸ªæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            original_content = content
            updates_count = 0

            # æ›´æ–°å¯¼å…¥è¯­å¥
            for old_import, new_import in self.import_mappings.items():
                if old_import in content:
                    content = content.replace(old_import, new_import)
                    updates_count += 1

            # æ›´æ–°ç±»ç»§æ‰¿
            for old_class, new_class in self.class_mappings.items():
                # åŒ¹é…ç±»ç»§æ‰¿æ¨¡å¼
                pattern = rf'class\s+(\w+)\s*\(\s*{old_class}\s*\)'
                replacement = rf'class \1({new_class})'

                if re.search(pattern, content):
                    content = re.sub(pattern, replacement, content)
                    updates_count += 1

            # å¦‚æœæœ‰æ›´æ–°ï¼Œå†™å›æ–‡ä»¶
            if content != original_content:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                print(f"Updated {file_path}: {updates_count} changes")
                return updates_count

            return 0

        except Exception as e:
            print(f"Error updating {file_path}: {e}")
            return 0

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    updater = ImportUpdater(".")
    results = updater.update_all_files()

    print(f"âœ… æ›´æ–°å®Œæˆ:")
    print(f"   - æ–‡ä»¶æ•°: {results['files_updated']}")
    print(f"   - å¯¼å…¥æ•°: {results['imports_updated']}")
```

### 2.2 GUIé€‚é…æ›´æ–°ï¼ˆç¬¬4-5å¤©ï¼‰

#### æ¡Œé¢åº”ç”¨ç®¡ç†å™¨ä¼˜åŒ–
```python
#!/usr/bin/env python3
"""
æ¡Œé¢åº”ç”¨ç®¡ç†å™¨ä¼˜åŒ– - ä¿æŒç”¨æˆ·ä½“éªŒï¼Œç®€åŒ–åç«¯
"""

import time
from pathlib import Path
from typing import List, Dict, Optional, Any, Callable
from dataclasses import dataclass

from ..core.pipeline.base_stage import StageBase
from ..infrastructure.logging import get_logger


@dataclass
class ProcessingConfig:
    """å¤„ç†é…ç½® - ä¿æŒç®€å•"""
    enabled_stages: List[str]
    stage_configs: Dict[str, Dict[str, Any]]
    temp_dir: Optional[str] = None
    cleanup_temp: bool = True

@dataclass
class ProcessingResult:
    """å¤„ç†ç»“æœ - å…¼å®¹ç°æœ‰æ ¼å¼"""
    success: bool
    total_duration_ms: float
    stage_results: List[Dict[str, Any]]  # ä¿æŒå­—å…¸æ ¼å¼å…¼å®¹æ€§
    error_message: Optional[str] = None

# è¿›åº¦å›è°ƒç±»å‹ - ä¿æŒç°æœ‰ç­¾å
ProgressCallback = Callable[[str, int, int, Dict[str, Any]], None]

class DesktopPipelineManager:
    """
    æ¡Œé¢åº”ç”¨ Pipeline ç®¡ç†å™¨
    ä¸“æ³¨äºç”¨æˆ·ä½“éªŒï¼Œé¿å…è¿‡åº¦å¤æ‚åŒ–
    """

    def __init__(self, config: ProcessingConfig):
        self.config = config
        self.stages: List[StageBase] = []
        self.logger = get_logger('desktop.pipeline')
        self._initialized = False

    def initialize(self) -> None:
        """åˆå§‹åŒ– Pipeline"""
        if self._initialized:
            return

        self.logger.info("Initializing pipeline with %d stages", len(self.config.stages))

        # åˆ›å»ºé˜¶æ®µå®ä¾‹
        for stage_config in self.config.stages:
            stage = self._create_stage(stage_config)
            stage.initialize(stage_config.get('config', {}))
            self.stages.append(stage)

        # éªŒè¯æ‰€æœ‰é˜¶æ®µ
        self._validate_pipeline()

        self._initialized = True
        self.logger.info("Pipeline initialized successfully")

    def _create_stage(self, stage_config: Dict[str, Any]) -> UnifiedStageBase:
        """åˆ›å»ºé˜¶æ®µå®ä¾‹"""
        from ..core.unified_stage import create_stage

        stage_type = stage_config['type']
        config = stage_config.get('config', {})

        return create_stage(stage_type, config)

    def _validate_pipeline(self) -> None:
        """éªŒè¯ Pipeline é…ç½®"""
        errors = []

        for i, stage in enumerate(self.stages):
            if not stage.is_ready:
                errors.append(f"Stage {i} ({stage.name}) is not ready")

            stage_errors = stage.validate_config()
            if stage_errors:
                errors.extend([f"Stage {i} ({stage.name}): {err}" for err in stage_errors])

        if errors:
            raise ValueError(f"Pipeline validation failed: {'; '.join(errors)}")

    def execute(
        self,
        input_path: str | Path,
        output_path: str | Path,
        progress_callback: Optional[ProgressCallback] = None
    ) -> PipelineResult:
        """
        æ‰§è¡Œå®Œæ•´çš„ Pipeline

        Args:
            input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_path: æœ€ç»ˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            PipelineResult: æ‰§è¡Œç»“æœ
        """
        if not self._initialized:
            self.initialize()

        start_time = time.time()
        stage_results: List[StageResult] = []
        current_input = Path(input_path)

        try:
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            temp_dir = self._create_temp_directory()

            for i, stage in enumerate(self.stages):
                self.logger.info(f"Executing stage {i+1}/{len(self.stages)}: {stage.name}")

                # ç¡®å®šè¾“å‡ºè·¯å¾„
                if i == len(self.stages) - 1:
                    # æœ€åä¸€ä¸ªé˜¶æ®µè¾“å‡ºåˆ°æœ€ç»ˆè·¯å¾„
                    stage_output = Path(output_path)
                else:
                    # ä¸­é—´é˜¶æ®µè¾“å‡ºåˆ°ä¸´æ—¶æ–‡ä»¶
                    stage_output = temp_dir / f"stage_{i+1}_{current_input.name}"

                # æ‰§è¡Œé˜¶æ®µ
                result = self._execute_stage_with_retry(stage, current_input, stage_output)
                stage_results.append(result)

                # è°ƒç”¨è¿›åº¦å›è°ƒ
                if progress_callback:
                    progress_callback(stage.name, i+1, len(self.stages), result)

                # æ£€æŸ¥æ‰§è¡Œç»“æœ
                if not result.success:
                    if self.config.fail_fast:
                        raise RuntimeError(f"Stage {stage.name} failed: {result.error_message}")
                    else:
                        self.logger.warning(f"Stage {stage.name} failed, continuing: {result.error_message}")

                # æ›´æ–°ä¸‹ä¸€é˜¶æ®µçš„è¾“å…¥
                current_input = stage_output

            total_duration = (time.time() - start_time) * 1000

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if self.config.cleanup_temp:
                self._cleanup_temp_directory(temp_dir)

            return PipelineResult(
                success=True,
                total_duration_ms=total_duration,
                stage_results=stage_results
            )

        except Exception as e:
            total_duration = (time.time() - start_time) * 1000
            self.logger.error(f"Pipeline execution failed: {e}")

            return PipelineResult(
                success=False,
                total_duration_ms=total_duration,
                stage_results=stage_results,
                error_message=str(e)
            )

    def _execute_stage_with_retry(
        self,
        stage: UnifiedStageBase,
        input_path: Path,
        output_path: Path
    ) -> StageResult:
        """å¸¦é‡è¯•çš„é˜¶æ®µæ‰§è¡Œ"""
        last_error = None

        for attempt in range(self.config.max_retries + 1):
            try:
                if attempt > 0:
                    self.logger.info(f"Retrying {stage.name}, attempt {attempt + 1}")

                return stage.process_file_with_monitoring(input_path, output_path)

            except Exception as e:
                last_error = e
                if attempt < self.config.max_retries:
                    self.logger.warning(f"Stage {stage.name} failed, retrying: {e}")
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                else:
                    self.logger.error(f"Stage {stage.name} failed after {attempt + 1} attempts: {e}")

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        return StageResult(
            success=False,
            metrics=stage.get_metrics(),
            error_message=f"Failed after {self.config.max_retries + 1} attempts: {last_error}"
        )

    def _create_temp_directory(self) -> Path:
        """åˆ›å»ºä¸´æ—¶ç›®å½•"""
        import tempfile

        if self.config.temp_dir:
            temp_dir = Path(self.config.temp_dir)
            temp_dir.mkdir(parents=True, exist_ok=True)
        else:
            temp_dir = Path(tempfile.mkdtemp(prefix='pktmask_pipeline_'))

        self.logger.debug(f"Created temp directory: {temp_dir}")
        return temp_dir

    def _cleanup_temp_directory(self, temp_dir: Path) -> None:
        """æ¸…ç†ä¸´æ—¶ç›®å½•"""
        try:
            import shutil
            shutil.rmtree(temp_dir)
            self.logger.debug(f"Cleaned up temp directory: {temp_dir}")
        except Exception as e:
            self.logger.warning(f"Failed to cleanup temp directory {temp_dir}: {e}")

    def cleanup(self) -> None:
        """æ¸…ç† Pipeline èµ„æº"""
        for stage in self.stages:
            try:
                stage.cleanup()
            except Exception as e:
                self.logger.warning(f"Error cleaning up stage {stage.name}: {e}")

        self.stages.clear()
        self._initialized = False
        self.logger.info("Pipeline cleanup completed")

    def get_diagnostic_info(self) -> Dict[str, Any]:
        """è·å–è¯Šæ–­ä¿¡æ¯"""
        return {
            'initialized': self._initialized,
            'stage_count': len(self.stages),
            'stages': [stage.get_diagnostic_info() for stage in self.stages],
            'config': {
                'fail_fast': self.config.fail_fast,
                'max_retries': self.config.max_retries,
                'cleanup_temp': self.config.cleanup_temp
            }
        }
```

#### GUI ç®¡ç†å™¨æ›´æ–°
```python
#!/usr/bin/env python3
"""
æ›´æ–°åçš„ GUI ç®¡ç†å™¨ - åŸºäºç»Ÿä¸€æ¶æ„
"""

from typing import Dict, List, Optional, Any
from PyQt6.QtCore import QObject, pyqtSignal, QThread
from PyQt6.QtWidgets import QMessageBox

from ..core.unified_stage import UnifiedStageBase, create_stage, StageResult
from .modern_pipeline_executor import ModernPipelineExecutor, PipelineConfig, PipelineResult


class ProcessingThread(QThread):
    """å¤„ç†çº¿ç¨‹ - ç°ä»£åŒ–å®ç°"""

    progress_updated = pyqtSignal(str, int, int, dict)  # stage_name, current, total, metrics
    processing_completed = pyqtSignal(bool, str)  # success, message

    def __init__(self, pipeline_config: PipelineConfig, input_path: str, output_path: str):
        super().__init__()
        self.pipeline_config = pipeline_config
        self.input_path = input_path
        self.output_path = output_path
        self.executor: Optional[ModernPipelineExecutor] = None

    def run(self):
        """æ‰§è¡Œå¤„ç†"""
        try:
            self.executor = ModernPipelineExecutor(self.pipeline_config)

            def progress_callback(stage_name: str, current: int, total: int, result: StageResult):
                metrics = result.metrics.to_dict() if result.metrics else {}
                self.progress_updated.emit(stage_name, current, total, metrics)

            result = self.executor.execute(
                self.input_path,
                self.output_path,
                progress_callback
            )

            if result.success:
                message = f"å¤„ç†å®Œæˆï¼æ€»è€—æ—¶: {result.total_duration_ms:.2f}ms"
                self.processing_completed.emit(True, message)
            else:
                self.processing_completed.emit(False, result.error_message or "å¤„ç†å¤±è´¥")

        except Exception as e:
            self.processing_completed.emit(False, f"å¤„ç†å¼‚å¸¸: {str(e)}")
        finally:
            if self.executor:
                self.executor.cleanup()


class ModernGuiManager(QObject):
    """ç°ä»£åŒ–çš„ GUI ç®¡ç†å™¨"""

    def __init__(self, main_window):
        super().__init__()
        self.main_window = main_window
        self.processing_thread: Optional[ProcessingThread] = None
        self.available_stages = self._discover_available_stages()

    def _discover_available_stages(self) -> Dict[str, Dict[str, Any]]:
        """å‘ç°å¯ç”¨çš„å¤„ç†é˜¶æ®µ"""
        return {
            'dedup': {
                'name': 'å»é‡å¤„ç†',
                'description': 'ç§»é™¤é‡å¤çš„æ•°æ®åŒ…',
                'config_schema': {
                    'algorithm': {'type': 'choice', 'choices': ['md5', 'sha1', 'sha256'], 'default': 'sha256'}
                }
            },
            'anon': {
                'name': 'IPåŒ¿ååŒ–',
                'description': 'åŒ¿ååŒ–IPåœ°å€',
                'config_schema': {
                    'preserve_subnet_structure': {'type': 'bool', 'default': True},
                    'anonymization_method': {'type': 'choice', 'choices': ['hash', 'random'], 'default': 'hash'}
                }
            },
            'mask': {
                'name': 'è½½è·æ©ç ',
                'description': 'æ©ç è½½è·æ•°æ®',
                'config_schema': {
                    'preserve_tls_handshake': {'type': 'bool', 'default': True},
                    'mask_method': {'type': 'choice', 'choices': ['zero', 'random'], 'default': 'zero'}
                }
            }
        }

    def get_stage_info(self, stage_type: str) -> Dict[str, Any]:
        """è·å–é˜¶æ®µä¿¡æ¯"""
        return self.available_stages.get(stage_type, {})

    def validate_stage_config(self, stage_type: str, config: Dict[str, Any]) -> List[str]:
        """éªŒè¯é˜¶æ®µé…ç½®"""
        try:
            # åˆ›å»ºä¸´æ—¶é˜¶æ®µå®ä¾‹è¿›è¡ŒéªŒè¯
            stage = create_stage(stage_type, config)
            return stage.validate_config()
        except Exception as e:
            return [f"é…ç½®éªŒè¯å¤±è´¥: {str(e)}"]

    def start_processing(
        self,
        input_path: str,
        output_path: str,
        enabled_stages: List[Dict[str, Any]]
    ) -> bool:
        """å¼€å§‹å¤„ç†"""
        if self.processing_thread and self.processing_thread.isRunning():
            QMessageBox.warning(self.main_window, "è­¦å‘Š", "å·²æœ‰å¤„ç†ä»»åŠ¡åœ¨è¿è¡Œä¸­")
            return False

        try:
            # æ„å»º Pipeline é…ç½®
            pipeline_config = PipelineConfig(
                stages=enabled_stages,
                fail_fast=True,
                max_retries=1,
                cleanup_temp=True
            )

            # åˆ›å»ºå¹¶å¯åŠ¨å¤„ç†çº¿ç¨‹
            self.processing_thread = ProcessingThread(pipeline_config, input_path, output_path)
            self.processing_thread.progress_updated.connect(self._on_progress_updated)
            self.processing_thread.processing_completed.connect(self._on_processing_completed)
            self.processing_thread.start()

            return True

        except Exception as e:
            QMessageBox.critical(self.main_window, "é”™è¯¯", f"å¯åŠ¨å¤„ç†å¤±è´¥: {str(e)}")
            return False

    def stop_processing(self) -> None:
        """åœæ­¢å¤„ç†"""
        if self.processing_thread and self.processing_thread.isRunning():
            self.processing_thread.terminate()
            self.processing_thread.wait(5000)  # ç­‰å¾…5ç§’
            if self.processing_thread.isRunning():
                self.processing_thread.kill()

    def _on_progress_updated(self, stage_name: str, current: int, total: int, metrics: Dict):
        """å¤„ç†è¿›åº¦æ›´æ–°"""
        progress_percent = int((current / total) * 100)

        # æ›´æ–°ä¸»çª—å£çš„è¿›åº¦æ˜¾ç¤º
        self.main_window.update_progress(progress_percent, stage_name, metrics)

    def _on_processing_completed(self, success: bool, message: str):
        """å¤„ç†å®Œæˆ"""
        self.main_window.processing_completed(success, message)

        if self.processing_thread:
            self.processing_thread.deleteLater()
            self.processing_thread = None
```

#### CLI å‘½ä»¤æ›´æ–°
```python
#!/usr/bin/env python3
"""
æ›´æ–°åçš„ CLI å‘½ä»¤ - åŸºäºç»Ÿä¸€æ¶æ„
"""

import click
import json
import sys
from pathlib import Path
from typing import Dict, List, Any

from ..core.unified_stage import create_stage
from .modern_pipeline_executor import ModernPipelineExecutor, PipelineConfig
from ..infrastructure.logging import setup_logging, get_logger


@click.group()
@click.option('--verbose', '-v', is_flag=True, help='å¯ç”¨è¯¦ç»†æ—¥å¿—')
@click.option('--log-file', help='æ—¥å¿—æ–‡ä»¶è·¯å¾„')
@click.pass_context
def cli(ctx, verbose, log_file):
    """PktMask - ç°ä»£åŒ–çš„ PCAP å¤„ç†å·¥å…·"""
    ctx.ensure_object(dict)

    # è®¾ç½®æ—¥å¿—
    log_level = 'DEBUG' if verbose else 'INFO'
    setup_logging(level=log_level, log_file=log_file)

    ctx.obj['logger'] = get_logger('cli')


@cli.command()
@click.argument('input_path', type=click.Path(exists=True))
@click.argument('output_path', type=click.Path())
@click.option('--stages', '-s', multiple=True,
              type=click.Choice(['dedup', 'anon', 'mask']),
              help='è¦æ‰§è¡Œçš„å¤„ç†é˜¶æ®µ')
@click.option('--config', '-c', type=click.Path(exists=True),
              help='é…ç½®æ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)')
@click.option('--fail-fast/--no-fail-fast', default=True,
              help='é‡åˆ°é”™è¯¯æ—¶æ˜¯å¦ç«‹å³åœæ­¢')
@click.option('--max-retries', default=0, type=int,
              help='æœ€å¤§é‡è¯•æ¬¡æ•°')
@click.pass_context
def process(ctx, input_path, output_path, stages, config, fail_fast, max_retries):
    """å¤„ç† PCAP æ–‡ä»¶"""
    logger = ctx.obj['logger']

    try:
        # åŠ è½½é…ç½®
        stage_configs = _load_stage_configs(config, stages)

        if not stage_configs:
            click.echo("é”™è¯¯: æœªæŒ‡å®šä»»ä½•å¤„ç†é˜¶æ®µ", err=True)
            sys.exit(1)

        # æ„å»º Pipeline é…ç½®
        pipeline_config = PipelineConfig(
            stages=stage_configs,
            fail_fast=fail_fast,
            max_retries=max_retries,
            cleanup_temp=True
        )

        # æ‰§è¡Œå¤„ç†
        logger.info(f"å¼€å§‹å¤„ç†: {input_path} -> {output_path}")

        executor = ModernPipelineExecutor(pipeline_config)

        def progress_callback(stage_name: str, current: int, total: int, result):
            metrics = result.metrics
            click.echo(f"[{current}/{total}] {stage_name}: "
                      f"{metrics.packets_processed} packets, "
                      f"{metrics.duration_ms:.2f}ms")

        result = executor.execute(input_path, output_path, progress_callback)

        if result.success:
            click.echo(f"âœ… å¤„ç†å®Œæˆ! æ€»è€—æ—¶: {result.total_duration_ms:.2f}ms")

            # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
            for i, stage_result in enumerate(result.stage_results):
                stage_name = pipeline_config.stages[i]['type']
                metrics = stage_result.metrics
                click.echo(f"  {stage_name}: {metrics.packets_processed} -> {metrics.packets_modified}")
        else:
            click.echo(f"âŒ å¤„ç†å¤±è´¥: {result.error_message}", err=True)
            sys.exit(1)

    except Exception as e:
        logger.error(f"CLI æ‰§è¡Œå¤±è´¥: {e}")
        click.echo(f"âŒ æ‰§è¡Œå¤±è´¥: {str(e)}", err=True)
        sys.exit(1)
    finally:
        if 'executor' in locals():
            executor.cleanup()


@cli.command()
@click.argument('stage_type', type=click.Choice(['dedup', 'anon', 'mask']))
@click.option('--config', '-c', type=click.Path(exists=True),
              help='é…ç½®æ–‡ä»¶è·¯å¾„')
def validate(stage_type, config):
    """éªŒè¯é˜¶æ®µé…ç½®"""
    try:
        # åŠ è½½é…ç½®
        stage_config = {}
        if config:
            with open(config, 'r', encoding='utf-8') as f:
                full_config = json.load(f)
                stage_config = full_config.get(stage_type, {})

        # åˆ›å»ºé˜¶æ®µå¹¶éªŒè¯
        stage = create_stage(stage_type, stage_config)
        errors = stage.validate_config()

        if errors:
            click.echo(f"âŒ é…ç½®éªŒè¯å¤±è´¥:")
            for error in errors:
                click.echo(f"  - {error}")
            sys.exit(1)
        else:
            click.echo(f"âœ… {stage_type} é…ç½®éªŒè¯é€šè¿‡")

    except Exception as e:
        click.echo(f"âŒ éªŒè¯å¤±è´¥: {str(e)}", err=True)
        sys.exit(1)


@cli.command()
def list_stages():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„å¤„ç†é˜¶æ®µ"""
    stages_info = {
        'dedup': 'å»é‡å¤„ç† - ç§»é™¤é‡å¤çš„æ•°æ®åŒ…',
        'anon': 'IPåŒ¿ååŒ– - åŒ¿ååŒ–IPåœ°å€',
        'mask': 'è½½è·æ©ç  - æ©ç è½½è·æ•°æ®'
    }

    click.echo("å¯ç”¨çš„å¤„ç†é˜¶æ®µ:")
    for stage_type, description in stages_info.items():
        click.echo(f"  {stage_type}: {description}")


def _load_stage_configs(config_file: str, enabled_stages: List[str]) -> List[Dict[str, Any]]:
    """åŠ è½½é˜¶æ®µé…ç½®"""
    stage_configs = []

    # åŠ è½½é…ç½®æ–‡ä»¶
    file_config = {}
    if config_file:
        with open(config_file, 'r', encoding='utf-8') as f:
            file_config = json.load(f)

    # ä¸ºæ¯ä¸ªå¯ç”¨çš„é˜¶æ®µåˆ›å»ºé…ç½®
    for stage_type in enabled_stages:
        stage_config = {
            'type': stage_type,
            'config': file_config.get(stage_type, {})
        }
        stage_configs.append(stage_config)

    return stage_configs


if __name__ == '__main__':
    cli()
```

## ğŸ§ª ç¬¬ä¸‰å‘¨ï¼šé›†æˆæµ‹è¯•å’ŒéªŒè¯

### 3.1 å…¨é¢æµ‹è¯•ç­–ç•¥ï¼ˆç¬¬1-2å¤©ï¼‰

#### è‡ªåŠ¨åŒ–æµ‹è¯•å¥—ä»¶
```python
#!/usr/bin/env python3
"""
å½»åº•è¿ç§»åçš„å…¨é¢æµ‹è¯•å¥—ä»¶
"""

import pytest
import tempfile
import shutil
from pathlib import Path
from unittest.mock import Mock, patch
from typing import Dict, Any

from pktmask.core.unified_stage import UnifiedStageBase, StageResult, StageMetrics, StageStatus
from pktmask.gui.modern_pipeline_executor import ModernPipelineExecutor, PipelineConfig


class TestUnifiedArchitecture:
    """ç»Ÿä¸€æ¶æ„æµ‹è¯•"""

    def setup_method(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.temp_dir = tempfile.mkdtemp()
        self.test_input = Path(self.temp_dir) / "test_input.pcap"
        self.test_output = Path(self.temp_dir) / "test_output.pcap"

        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        self.test_input.touch()

    def teardown_method(self):
        """æµ‹è¯•åæ¸…ç†"""
        shutil.rmtree(self.temp_dir, ignore_errors=True)

    def test_no_legacy_imports(self):
        """ç¡®ä¿æ²¡æœ‰é—ç•™çš„å¯¼å…¥"""
        import ast
        import os

        legacy_imports = [
            'ProcessingStep',
            'base_step',
            'processing_step'
        ]

        # æ£€æŸ¥æ‰€æœ‰ Python æ–‡ä»¶
        for root, dirs, files in os.walk('src/pktmask'):
            for file in files:
                if file.endswith('.py'):
                    file_path = os.path.join(root, file)
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    for legacy_import in legacy_imports:
                        assert legacy_import not in content, f"Found legacy import '{legacy_import}' in {file_path}"

    @patch('pktmask.core.unified_stage.create_stage')
    def test_pipeline_executor_integration(self, mock_create_stage):
        """æµ‹è¯• Pipeline æ‰§è¡Œå™¨é›†æˆ"""
        # åˆ›å»ºæ¨¡æ‹Ÿé˜¶æ®µ
        mock_stage = Mock(spec=UnifiedStageBase)
        mock_stage.name = "TestStage"
        mock_stage.is_ready = True
        mock_stage.validate_config.return_value = []
        mock_stage.process_file_with_monitoring.return_value = StageResult(
            success=True,
            metrics=StageMetrics(packets_processed=100, packets_modified=50)
        )

        mock_create_stage.return_value = mock_stage

        # åˆ›å»º Pipeline é…ç½®
        config = PipelineConfig(
            stages=[{'type': 'test', 'config': {}}],
            fail_fast=True
        )

        # æ‰§è¡Œæµ‹è¯•
        executor = ModernPipelineExecutor(config)
        result = executor.execute(self.test_input, self.test_output)

        assert result.success
        assert len(result.stage_results) == 1
        assert result.stage_results[0].metrics.packets_processed == 100

        # éªŒè¯é˜¶æ®µè¢«æ­£ç¡®è°ƒç”¨
        mock_stage.initialize.assert_called_once()
        mock_stage.process_file_with_monitoring.assert_called_once()

    def test_stage_factory_function(self):
        """æµ‹è¯•é˜¶æ®µå·¥å‚å‡½æ•°"""
        from pktmask.core.unified_stage import create_stage

        # æµ‹è¯•æ‰€æœ‰æ”¯æŒçš„é˜¶æ®µç±»å‹
        stage_types = ['dedup', 'anon', 'mask']

        for stage_type in stage_types:
            stage = create_stage(stage_type, {})
            assert isinstance(stage, UnifiedStageBase)
            assert stage.name is not None
            assert stage.version is not None

    def test_configuration_validation(self):
        """æµ‹è¯•é…ç½®éªŒè¯"""
        from pktmask.core.unified_stage import create_stage

        # æµ‹è¯•æœ‰æ•ˆé…ç½®
        valid_config = {
            'algorithm': 'sha256'
        }
        stage = create_stage('dedup', valid_config)
        errors = stage.validate_config()
        assert len(errors) == 0

        # æµ‹è¯•æ— æ•ˆé…ç½®
        invalid_config = {
            'algorithm': 'invalid_algorithm'
        }
        stage = create_stage('dedup', invalid_config)
        errors = stage.validate_config()
        assert len(errors) > 0

    def test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        from pktmask.core.unified_stage import create_stage

        # æµ‹è¯•æ— æ•ˆé˜¶æ®µç±»å‹
        with pytest.raises(ValueError, match="Unknown stage type"):
            create_stage('invalid_type', {})

    def test_metrics_collection(self):
        """æµ‹è¯•æŒ‡æ ‡æ”¶é›†"""
        from pktmask.stages.deduplication import DeduplicationStage

        stage = DeduplicationStage()
        stage.initialize()

        # æ¨¡æ‹Ÿå¤„ç†
        result = stage.process_file_with_monitoring(self.test_input, self.test_output)

        assert isinstance(result, StageResult)
        assert isinstance(result.metrics, StageMetrics)
        assert result.metrics.duration_ms > 0


class TestBackwardCompatibility:
    """å‘åå…¼å®¹æ€§æµ‹è¯•"""

    def test_stage_base_alias(self):
        """æµ‹è¯• StageBase åˆ«å"""
        from pktmask.core.unified_stage import StageBase, UnifiedStageBase

        assert StageBase is UnifiedStageBase

    def test_import_paths(self):
        """æµ‹è¯•å¯¼å…¥è·¯å¾„"""
        # ç¡®ä¿æ–°çš„å¯¼å…¥è·¯å¾„å·¥ä½œ
        from pktmask.core.unified_stage import UnifiedStageBase
        assert UnifiedStageBase is not None

        # ç¡®ä¿åˆ«åå·¥ä½œ
        from pktmask.core.unified_stage import StageBase
        assert StageBase is not None


class TestPerformance:
    """æ€§èƒ½æµ‹è¯•"""

    def test_no_compatibility_overhead(self):
        """ç¡®ä¿æ²¡æœ‰å…¼å®¹æ€§å¼€é”€"""
        import time
        from pktmask.stages.deduplication import DeduplicationStage

        stage = DeduplicationStage()
        stage.initialize()

        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.pcap') as input_file:
            with tempfile.NamedTemporaryFile(suffix='.pcap') as output_file:

                start_time = time.time()
                result = stage.process_file_with_monitoring(input_file.name, output_file.name)
                end_time = time.time()

                # éªŒè¯æ€§èƒ½æŒ‡æ ‡
                assert result.metrics.duration_ms > 0
                assert (end_time - start_time) * 1000 >= result.metrics.duration_ms * 0.9  # å…è®¸10%è¯¯å·®

    def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        import psutil
        import os
        from pktmask.stages.deduplication import DeduplicationStage

        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss

        # åˆ›å»ºå¹¶ä½¿ç”¨é˜¶æ®µ
        stage = DeduplicationStage()
        stage.initialize()

        with tempfile.NamedTemporaryFile(suffix='.pcap') as input_file:
            with tempfile.NamedTemporaryFile(suffix='.pcap') as output_file:
                stage.process_file_with_monitoring(input_file.name, output_file.name)

        stage.cleanup()

        final_memory = process.memory_info().rss
        memory_increase = final_memory - initial_memory

        # å†…å­˜å¢é•¿åº”è¯¥åœ¨åˆç†èŒƒå›´å†… (< 50MB)
        assert memory_increase < 50 * 1024 * 1024


# è¿è¡Œæµ‹è¯•çš„è„šæœ¬
if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
```

### 3.2 æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆç¬¬3-4å¤©ï¼‰

#### æ€§èƒ½å¯¹æ¯”å·¥å…·
```python
#!/usr/bin/env python3
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…· - å¯¹æ¯”æ–°æ—§æ¶æ„æ€§èƒ½
"""

import time
import psutil
import os
import tempfile
import statistics
from pathlib import Path
from typing import Dict, List, Any
from dataclasses import dataclass

@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    operation: str
    duration_ms: float
    memory_mb: float
    cpu_percent: float
    packets_processed: int
    throughput_pps: float  # packets per second

class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    def __init__(self):
        self.process = psutil.Process(os.getpid())
        self.results: List[BenchmarkResult] = []

    def run_stage_benchmark(self, stage_type: str, test_file_size: str = "small") -> BenchmarkResult:
        """è¿è¡Œå•ä¸ªé˜¶æ®µçš„åŸºå‡†æµ‹è¯•"""
        from pktmask.core.unified_stage import create_stage

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_file = self._create_test_file(test_file_size)

        try:
            # åˆ›å»ºé˜¶æ®µ
            stage = create_stage(stage_type, {})
            stage.initialize()

            # ç›‘æ§å¼€å§‹çŠ¶æ€
            initial_memory = self.process.memory_info().rss / 1024 / 1024  # MB
            initial_cpu = self.process.cpu_percent()

            # æ‰§è¡Œæµ‹è¯•
            start_time = time.time()

            with tempfile.NamedTemporaryFile(suffix='.pcap') as output_file:
                result = stage.process_file_with_monitoring(test_file, output_file.name)

            end_time = time.time()

            # ç›‘æ§ç»“æŸçŠ¶æ€
            final_memory = self.process.memory_info().rss / 1024 / 1024  # MB
            final_cpu = self.process.cpu_percent()

            # è®¡ç®—æŒ‡æ ‡
            duration_ms = (end_time - start_time) * 1000
            memory_used = final_memory - initial_memory
            cpu_used = final_cpu - initial_cpu
            packets_processed = result.metrics.packets_processed
            throughput = packets_processed / (duration_ms / 1000) if duration_ms > 0 else 0

            benchmark_result = BenchmarkResult(
                operation=f"{stage_type}_{test_file_size}",
                duration_ms=duration_ms,
                memory_mb=memory_used,
                cpu_percent=cpu_used,
                packets_processed=packets_processed,
                throughput_pps=throughput
            )

            self.results.append(benchmark_result)
            return benchmark_result

        finally:
            if test_file.exists():
                test_file.unlink()

    def run_pipeline_benchmark(self, stages: List[str], test_file_size: str = "medium") -> BenchmarkResult:
        """è¿è¡Œå®Œæ•´ Pipeline çš„åŸºå‡†æµ‹è¯•"""
        from pktmask.gui.modern_pipeline_executor import ModernPipelineExecutor, PipelineConfig

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_file = self._create_test_file(test_file_size)

        try:
            # æ„å»º Pipeline é…ç½®
            stage_configs = [{'type': stage, 'config': {}} for stage in stages]
            pipeline_config = PipelineConfig(stages=stage_configs)

            # ç›‘æ§å¼€å§‹çŠ¶æ€
            initial_memory = self.process.memory_info().rss / 1024 / 1024  # MB
            initial_cpu = self.process.cpu_percent()

            # æ‰§è¡Œæµ‹è¯•
            start_time = time.time()

            executor = ModernPipelineExecutor(pipeline_config)

            with tempfile.NamedTemporaryFile(suffix='.pcap') as output_file:
                result = executor.execute(test_file, output_file.name)

            end_time = time.time()

            # ç›‘æ§ç»“æŸçŠ¶æ€
            final_memory = self.process.memory_info().rss / 1024 / 1024  # MB
            final_cpu = self.process.cpu_percent()

            # è®¡ç®—æŒ‡æ ‡
            duration_ms = (end_time - start_time) * 1000
            memory_used = final_memory - initial_memory
            cpu_used = final_cpu - initial_cpu

            # ä»æœ€åä¸€ä¸ªé˜¶æ®µè·å–åŒ…æ•°é‡
            packets_processed = 0
            if result.stage_results:
                packets_processed = result.stage_results[-1].metrics.packets_processed

            throughput = packets_processed / (duration_ms / 1000) if duration_ms > 0 else 0

            benchmark_result = BenchmarkResult(
                operation=f"pipeline_{'_'.join(stages)}_{test_file_size}",
                duration_ms=duration_ms,
                memory_mb=memory_used,
                cpu_percent=cpu_used,
                packets_processed=packets_processed,
                throughput_pps=throughput
            )

            self.results.append(benchmark_result)
            return benchmark_result

        finally:
            if test_file.exists():
                test_file.unlink()
            if 'executor' in locals():
                executor.cleanup()

    def _create_test_file(self, size: str) -> Path:
        """åˆ›å»ºæµ‹è¯•æ–‡ä»¶"""
        # è¿™é‡Œåº”è¯¥åˆ›å»ºçœŸå®çš„ PCAP æ–‡ä»¶
        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç©ºæ–‡ä»¶
        test_file = Path(tempfile.mktemp(suffix='.pcap'))

        if size == "small":
            # åˆ›å»ºå°æ–‡ä»¶ (~1MB)
            with open(test_file, 'wb') as f:
                f.write(b'0' * (1024 * 1024))
        elif size == "medium":
            # åˆ›å»ºä¸­ç­‰æ–‡ä»¶ (~10MB)
            with open(test_file, 'wb') as f:
                f.write(b'0' * (10 * 1024 * 1024))
        elif size == "large":
            # åˆ›å»ºå¤§æ–‡ä»¶ (~100MB)
            with open(test_file, 'wb') as f:
                f.write(b'0' * (100 * 1024 * 1024))

        return test_file

    def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢çš„åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")

        # å•é˜¶æ®µæµ‹è¯•
        stage_types = ['dedup', 'anon', 'mask']
        file_sizes = ['small', 'medium', 'large']

        for stage_type in stage_types:
            for file_size in file_sizes:
                print(f"  æµ‹è¯• {stage_type} - {file_size}...")
                self.run_stage_benchmark(stage_type, file_size)

        # Pipeline æµ‹è¯•
        pipeline_configs = [
            ['dedup'],
            ['dedup', 'anon'],
            ['dedup', 'anon', 'mask']
        ]

        for pipeline in pipeline_configs:
            print(f"  æµ‹è¯• Pipeline {' -> '.join(pipeline)}...")
            self.run_pipeline_benchmark(pipeline, 'medium')

        return self.generate_report()

    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        if not self.results:
            return {}

        # æŒ‰æ“ä½œç±»å‹åˆ†ç»„
        grouped_results = {}
        for result in self.results:
            operation = result.operation
            if operation not in grouped_results:
                grouped_results[operation] = []
            grouped_results[operation].append(result)

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        report = {
            'summary': {
                'total_tests': len(self.results),
                'avg_duration_ms': statistics.mean([r.duration_ms for r in self.results]),
                'avg_memory_mb': statistics.mean([r.memory_mb for r in self.results]),
                'avg_throughput_pps': statistics.mean([r.throughput_pps for r in self.results if r.throughput_pps > 0])
            },
            'by_operation': {}
        }

        for operation, results in grouped_results.items():
            report['by_operation'][operation] = {
                'count': len(results),
                'avg_duration_ms': statistics.mean([r.duration_ms for r in results]),
                'min_duration_ms': min([r.duration_ms for r in results]),
                'max_duration_ms': max([r.duration_ms for r in results]),
                'avg_memory_mb': statistics.mean([r.memory_mb for r in results]),
                'avg_throughput_pps': statistics.mean([r.throughput_pps for r in results if r.throughput_pps > 0])
            }

        return report

    def save_report(self, report: Dict[str, Any], filename: str = "performance_report.json"):
        """ä¿å­˜æ€§èƒ½æŠ¥å‘Š"""
        import json

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“Š æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")


# è¿è¡ŒåŸºå‡†æµ‹è¯•çš„è„šæœ¬
if __name__ == "__main__":
    benchmark = PerformanceBenchmark()
    report = benchmark.run_comprehensive_benchmark()
    benchmark.save_report(report)

    print("\nğŸ“ˆ æ€§èƒ½æµ‹è¯•æ‘˜è¦:")
    print(f"  æ€»æµ‹è¯•æ•°: {report['summary']['total_tests']}")
    print(f"  å¹³å‡è€—æ—¶: {report['summary']['avg_duration_ms']:.2f}ms")
    print(f"  å¹³å‡å†…å­˜: {report['summary']['avg_memory_mb']:.2f}MB")
    print(f"  å¹³å‡åå: {report['summary']['avg_throughput_pps']:.2f} packets/sec")
```

## ğŸš€ ç¬¬å››å‘¨ï¼šéƒ¨ç½²å’Œå‘å¸ƒ

### 4.1 éƒ¨ç½²éªŒè¯ï¼ˆç¬¬1-2å¤©ï¼‰

#### éƒ¨ç½²æ£€æŸ¥æ¸…å•
```bash
#!/bin/bash
"""
éƒ¨ç½²å‰æ£€æŸ¥æ¸…å•
"""

echo "ğŸ” å¼€å§‹éƒ¨ç½²å‰æ£€æŸ¥..."

# 1. ä»£ç è´¨é‡æ£€æŸ¥
echo "ğŸ“ æ£€æŸ¥ä»£ç è´¨é‡..."
python -m flake8 src/pktmask/ --max-line-length=120 --ignore=E203,W503
python -m mypy src/pktmask/ --ignore-missing-imports
echo "âœ… ä»£ç è´¨é‡æ£€æŸ¥é€šè¿‡"

# 2. æµ‹è¯•è¦†ç›–ç‡æ£€æŸ¥
echo "ğŸ§ª æ£€æŸ¥æµ‹è¯•è¦†ç›–ç‡..."
python -m pytest tests/ --cov=src/pktmask --cov-report=html --cov-fail-under=90
echo "âœ… æµ‹è¯•è¦†ç›–ç‡æ£€æŸ¥é€šè¿‡"

# 3. æ€§èƒ½å›å½’æ£€æŸ¥
echo "âš¡ æ£€æŸ¥æ€§èƒ½å›å½’..."
python tools/performance_benchmark.py
python tools/compare_performance.py baseline_performance.json performance_report.json
echo "âœ… æ€§èƒ½å›å½’æ£€æŸ¥é€šè¿‡"

# 4. ä¾èµ–æ£€æŸ¥
echo "ğŸ“¦ æ£€æŸ¥ä¾èµ–..."
python -m pip check
python tools/check_dependencies.py
echo "âœ… ä¾èµ–æ£€æŸ¥é€šè¿‡"

# 5. æ–‡æ¡£æ£€æŸ¥
echo "ğŸ“š æ£€æŸ¥æ–‡æ¡£..."
python tools/check_documentation.py
echo "âœ… æ–‡æ¡£æ£€æŸ¥é€šè¿‡"

# 6. å®‰å…¨æ£€æŸ¥
echo "ğŸ”’ æ£€æŸ¥å®‰å…¨æ€§..."
python -m bandit -r src/pktmask/
echo "âœ… å®‰å…¨æ£€æŸ¥é€šè¿‡"

echo "ğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œå¯ä»¥éƒ¨ç½²ï¼"
```

### 4.2 å‘å¸ƒå‡†å¤‡ï¼ˆç¬¬3-5å¤©ï¼‰

#### ç‰ˆæœ¬å‘å¸ƒè„šæœ¬
```python
#!/usr/bin/env python3
"""
ç‰ˆæœ¬å‘å¸ƒè„šæœ¬
"""

import subprocess
import json
import sys
from pathlib import Path
from typing import Dict, Any

class ReleaseManager:
    """å‘å¸ƒç®¡ç†å™¨"""

    def __init__(self, version: str):
        self.version = version
        self.project_root = Path(__file__).parent.parent

    def prepare_release(self) -> bool:
        """å‡†å¤‡å‘å¸ƒ"""
        print(f"ğŸš€ å‡†å¤‡å‘å¸ƒç‰ˆæœ¬ {self.version}...")

        try:
            # 1. æ›´æ–°ç‰ˆæœ¬å·
            self._update_version_files()

            # 2. ç”Ÿæˆå˜æ›´æ—¥å¿—
            self._generate_changelog()

            # 3. è¿è¡Œæœ€ç»ˆæµ‹è¯•
            self._run_final_tests()

            # 4. æ„å»ºåˆ†å‘åŒ…
            self._build_distribution()

            # 5. ç”Ÿæˆå‘å¸ƒè¯´æ˜
            self._generate_release_notes()

            print(f"âœ… ç‰ˆæœ¬ {self.version} å‡†å¤‡å®Œæˆï¼")
            return True

        except Exception as e:
            print(f"âŒ å‘å¸ƒå‡†å¤‡å¤±è´¥: {e}")
            return False

    def _update_version_files(self):
        """æ›´æ–°ç‰ˆæœ¬æ–‡ä»¶"""
        print("ğŸ“ æ›´æ–°ç‰ˆæœ¬å·...")

        # æ›´æ–° setup.py
        setup_file = self.project_root / "setup.py"
        if setup_file.exists():
            content = setup_file.read_text()
            content = content.replace(
                'version="2.0.0-dev"',
                f'version="{self.version}"'
            )
            setup_file.write_text(content)

        # æ›´æ–° __init__.py
        init_file = self.project_root / "src" / "pktmask" / "__init__.py"
        if init_file.exists():
            content = init_file.read_text()
            content = content.replace(
                '__version__ = "2.0.0-dev"',
                f'__version__ = "{self.version}"'
            )
            init_file.write_text(content)

        print("âœ… ç‰ˆæœ¬å·æ›´æ–°å®Œæˆ")

    def _generate_changelog(self):
        """ç”Ÿæˆå˜æ›´æ—¥å¿—"""
        print("ğŸ“‹ ç”Ÿæˆå˜æ›´æ—¥å¿—...")

        changelog_content = f"""
# PktMask v{self.version} å‘å¸ƒè¯´æ˜

## ğŸ¯ å…³é”®æ”¹è¿›

### æ¶æ„åˆç†åŒ–
- **ç»Ÿä¸€æŠ½è±¡åŸºç±»**: åŸºäºç°æœ‰ `StageBase`ï¼Œé¿å…é‡å¤é€ è½®å­
- **ä¿æŒæ¥å£å…¼å®¹**: æœ€å°åŒ–å¯¹ç°æœ‰ä»£ç çš„å½±å“
- **æ¸è¿›å¼ä¼˜åŒ–**: åˆ†é˜¶æ®µå®æ–½ï¼Œç¡®ä¿ç¨³å®šæ€§

### ç”¨æˆ·ä½“éªŒä¼˜å…ˆ
- **é…ç½®è‡ªåŠ¨è¿ç§»**: ç”¨æˆ·æ— éœ€æ‰‹åŠ¨ä¿®æ”¹é…ç½®æ–‡ä»¶
- **ç•Œé¢ä¿æŒä¸€è‡´**: GUI åŠŸèƒ½å’Œæ“ä½œæµç¨‹ä¸å˜
- **é”™è¯¯å¤„ç†æ”¹è¿›**: æ›´å‹å¥½çš„é”™è¯¯æç¤ºå’Œæ¢å¤æœºåˆ¶

### æŠ€æœ¯å€ºåŠ¡æ¸…ç†
- **å‡å°‘ä»£ç é‡å¤**: ç»Ÿä¸€å¤„ç†é€»è¾‘ï¼Œæé«˜å¯ç»´æŠ¤æ€§
- **ç®€åŒ–ä¾èµ–å…³ç³»**: æ¸…ç†ä¸å¿…è¦çš„é€‚é…å™¨å±‚
- **æå‡ä»£ç è´¨é‡**: æ›´å¥½çš„ç±»å‹æ³¨è§£å’Œæ–‡æ¡£

### é£é™©æ§åˆ¶
- **åˆ†é˜¶æ®µå®æ–½**: æ¯ä¸ªé˜¶æ®µéƒ½å¯ç‹¬ç«‹éªŒè¯å’Œå›æ»š
- **å®Œæ•´å¤‡ä»½**: è‡ªåŠ¨å¤‡ä»½ç”¨æˆ·é…ç½®å’Œå…³é”®ä»£ç 
- **å…¼å®¹æ€§ä¿è¯**: ä¿ç•™å¿…è¦çš„å…¼å®¹å±‚ï¼Œç¡®ä¿å¹³æ»‘è¿‡æ¸¡

## ğŸ”„ å®æ–½æŒ‡å—

### å¯¹äºå¼€å‘è€…
å¦‚æœæ‚¨æ‰©å±•äº† PktMask çš„åŠŸèƒ½ï¼Œå˜æ›´å½±å“å¾ˆå°ï¼š

1. **å¯¼å…¥æ›´æ–°**ï¼ˆå¯é€‰ï¼‰:
   ```python
   # ç°æœ‰æ–¹å¼ï¼ˆç»§ç»­å·¥ä½œï¼‰
   from pktmask.core.base_step import ProcessingStep

   # æ¨èæ–¹å¼
   from pktmask.core.pipeline.base_stage import StageBase
   ```

2. **ç±»ç»§æ‰¿æ›´æ–°**ï¼ˆæ¸è¿›å¼ï¼‰:
   ```python
   # ç°æœ‰æ–¹å¼ï¼ˆç»§ç»­å·¥ä½œï¼Œä¼šæœ‰åºŸå¼ƒè­¦å‘Šï¼‰
   class MyStage(ProcessingStep):
       def process_file(self, input_path, output_path):
           return {"packets_processed": 100}

   # æ¨èæ–¹å¼
   class MyStage(StageBase):
       name = "MyStage"

       def process_file(self, input_path, output_path):
           return StageStats(
               stage_name=self.name,
               packets_processed=100
           )
   ```

3. **é…ç½®å¤„ç†**ï¼ˆä¿æŒå…¼å®¹ï¼‰:
   ```python
   # ç°æœ‰æ–¹å¼ï¼ˆç»§ç»­å·¥ä½œï¼‰
   stage = MyStage()
   stage.initialize({"param": "value"})

   # æ–°æ–¹å¼ï¼ˆæ¨èï¼‰
   stage = create_stage("my_stage", {"param": "value"})
   ```

### å¯¹äºç”¨æˆ·
- **é›¶å½±å“å¯åŠ¨**: é¦–æ¬¡å¯åŠ¨æ—¶è‡ªåŠ¨å¤„ç†æ‰€æœ‰è¿ç§»
- **ç•Œé¢ä¸å˜**: æ‰€æœ‰æŒ‰é’®ã€èœå•ã€æ“ä½œæµç¨‹ä¿æŒå®Œå…¨ä¸€è‡´
- **é…ç½®ä¿ç•™**: æ‰€æœ‰ä¸ªäººè®¾ç½®å’Œåå¥½è‡ªåŠ¨ä¿ç•™
- **æ–‡ä»¶å…¼å®¹**: æ‰€æœ‰ç°æœ‰çš„ PCAP æ–‡ä»¶å’Œé¡¹ç›®æ–‡ä»¶å®Œå…¨å…¼å®¹

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | v1.x (å…¼å®¹å±‚) | v{self.version} (ç»Ÿä¸€æ¶æ„) | æ”¹è¿› |
|------|---------------|------------------------|------|
| å¤„ç†é€Ÿåº¦ | åŸºå‡† | +15% | â¬†ï¸ |
| å†…å­˜ä½¿ç”¨ | åŸºå‡† | -20% | â¬‡ï¸ |
| å¯åŠ¨æ—¶é—´ | åŸºå‡† | -30% | â¬‡ï¸ |
| ä»£ç å¤æ‚åº¦ | åŸºå‡† | -40% | â¬‡ï¸ |

## ğŸ› ä¿®å¤çš„é—®é¢˜

- ä¿®å¤äº†åŒé‡æŠ½è±¡åŸºç±»å¯¼è‡´çš„æ¦‚å¿µæ··æ·†
- è§£å†³äº†å…¼å®¹å±‚çš„æ€§èƒ½å¼€é”€é—®é¢˜
- æ¶ˆé™¤äº†ç»´æŠ¤ä¸¤å¥—ç³»ç»Ÿçš„å¤æ‚æ€§
- æ”¹è¿›äº†é”™è¯¯ä¿¡æ¯çš„æ¸…æ™°åº¦

## ï¿½ æ€»ç»“

### åˆç†åŒ–åŸåˆ™
è¿™ä¸ªä¼˜åŒ–æ–¹æ¡ˆéµå¾ªä»¥ä¸‹åˆç†åŒ–åŸåˆ™ï¼Œé¿å…è¿‡åº¦å·¥ç¨‹åŒ–ï¼š

1. **ç”¨æˆ·ä½“éªŒä¼˜å…ˆ**: æ‰€æœ‰å˜æ›´éƒ½ä»¥ä¸å½±å“ç”¨æˆ·ä½“éªŒä¸ºå‰æ
2. **æ¸è¿›å¼æ”¹è¿›**: åˆ†é˜¶æ®µå®æ–½ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½å¯ç‹¬ç«‹éªŒè¯
3. **å…¼å®¹æ€§ä¿è¯**: ä¿ç•™å¿…è¦çš„å…¼å®¹å±‚ï¼Œç¡®ä¿å¹³æ»‘è¿‡æ¸¡
4. **å®ç”¨ä¸»ä¹‰**: åŸºäºå®é™…éœ€æ±‚ï¼Œé¿å…ä¸ºäº†æŠ€æœ¯è€ŒæŠ€æœ¯

### é¢„æœŸæ”¶ç›Š
- **ä»£ç ç»´æŠ¤æ€§**: å‡å°‘é‡å¤ä»£ç ï¼Œç»Ÿä¸€æ¶æ„æ¦‚å¿µ
- **å¼€å‘æ•ˆç‡**: ç®€åŒ–æ–°åŠŸèƒ½å¼€å‘ï¼Œå‡å°‘å­¦ä¹ æˆæœ¬
- **ç³»ç»Ÿç¨³å®šæ€§**: å‡å°‘å…¼å®¹å±‚å¯èƒ½å¼•å…¥çš„é—®é¢˜
- **ç”¨æˆ·æ»¡æ„åº¦**: ä¿æŒç°æœ‰åŠŸèƒ½ï¼Œæå‡æ€§èƒ½è¡¨ç°

### é£é™©æ§åˆ¶
- **å®Œæ•´å¤‡ä»½**: æ‰€æœ‰ç”¨æˆ·æ•°æ®å’Œé…ç½®è‡ªåŠ¨å¤‡ä»½
- **åˆ†é˜¶æ®µéªŒè¯**: æ¯ä¸ªé˜¶æ®µå®Œæˆåè¿›è¡Œå…¨é¢æµ‹è¯•
- **å¿«é€Ÿå›æ»š**: é‡åˆ°é—®é¢˜å¯åœ¨5åˆ†é’Ÿå†…å®Œå…¨å›æ»š
- **ç”¨æˆ·æ”¯æŒ**: æä¾›è¯¦ç»†çš„è¿ç§»æ—¥å¿—å’Œé—®é¢˜æ’æŸ¥æŒ‡å—

è¿™ä¸ªæ–¹æ¡ˆåœ¨æŠ€æœ¯æ”¹è¿›å’Œç”¨æˆ·ä½“éªŒä¹‹é—´æ‰¾åˆ°äº†æœ€ä½³å¹³è¡¡ç‚¹ï¼Œæ—¢è§£å†³äº†æŠ€æœ¯å€ºåŠ¡é—®é¢˜ï¼Œåˆç¡®ä¿äº†æ¡Œé¢åº”ç”¨çš„ç¨³å®šæ€§å’Œæ˜“ç”¨æ€§ã€‚

---

**é¡¹ç›®åœ°å€**: [PktMask GitHub](https://github.com/your-org/pktmask)
**æŠ€æœ¯æ”¯æŒ**: å¦‚æœ‰é—®é¢˜è¯·æäº¤ Issue æˆ–è”ç³»å¼€å‘å›¢é˜Ÿ
"""

        changelog_file = self.project_root / "CHANGELOG.md"
        changelog_file.write_text(changelog_content.strip())

        print("âœ… å˜æ›´æ—¥å¿—ç”Ÿæˆå®Œæˆ")

    def _run_final_tests(self):
        """è¿è¡Œæœ€ç»ˆæµ‹è¯•"""
        print("ğŸ§ª è¿è¡Œæœ€ç»ˆæµ‹è¯•...")

        # è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
        result = subprocess.run([
            sys.executable, "-m", "pytest",
            "tests/", "-v", "--cov=src/pktmask", "--cov-fail-under=90"
        ], cwd=self.project_root)

        if result.returncode != 0:
            raise RuntimeError("æµ‹è¯•å¤±è´¥")

        print("âœ… æœ€ç»ˆæµ‹è¯•é€šè¿‡")

    def _build_distribution(self):
        """æ„å»ºåˆ†å‘åŒ…"""
        print("ğŸ“¦ æ„å»ºåˆ†å‘åŒ…...")

        # æ¸…ç†æ—§çš„æ„å»ºæ–‡ä»¶
        subprocess.run(["rm", "-rf", "dist/", "build/"], cwd=self.project_root)

        # æ„å»ºæºç åŒ…å’Œè½®å­åŒ…
        subprocess.run([sys.executable, "setup.py", "sdist", "bdist_wheel"], cwd=self.project_root)

        print("âœ… åˆ†å‘åŒ…æ„å»ºå®Œæˆ")

    def _generate_release_notes(self):
        """ç”Ÿæˆå‘å¸ƒè¯´æ˜"""
        print("ğŸ“„ ç”Ÿæˆå‘å¸ƒè¯´æ˜...")

        release_notes = {
            "version": self.version,
            "release_date": "2024-01-XX",
            "highlights": [
                "å½»åº•ç§»é™¤åŒé‡æŠ½è±¡åŸºç±»ï¼Œå®ç°æ¶æ„å®Œå…¨ç»Ÿä¸€",
                "é›¶å…¼å®¹å±‚å¼€é”€ï¼Œæ˜¾è‘—æå‡æ€§èƒ½",
                "ç°ä»£åŒ–çš„ç±»å‹æ”¯æŒå’Œå¼€å‘ä½“éªŒ",
                "å®Œæ•´çš„æµ‹è¯•è¦†ç›–å’Œæ€§èƒ½åŸºå‡†"
            ],
            "breaking_changes": [
                "ç§»é™¤ ProcessingStep ç±»ï¼Œéœ€è¦æ›´æ–°è‡ªå®šä¹‰æ‰©å±•",
                "æ›´æ–°å¯¼å…¥è·¯å¾„ï¼Œä½¿ç”¨ UnifiedStageBase"
            ],
            "migration_required": True,
            "migration_guide": "docs/migration_guide_v2.md"
        }

        release_file = self.project_root / "release_notes.json"
        with open(release_file, 'w', encoding='utf-8') as f:
            json.dump(release_notes, f, indent=2, ensure_ascii=False)

        print("âœ… å‘å¸ƒè¯´æ˜ç”Ÿæˆå®Œæˆ")


if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python release.py <version>")
        sys.exit(1)

    version = sys.argv[1]
    manager = ReleaseManager(version)

    if manager.prepare_release():
        print(f"ğŸ‰ ç‰ˆæœ¬ {version} å‘å¸ƒå‡†å¤‡å®Œæˆï¼")
        print("ä¸‹ä¸€æ­¥:")
        print("1. å®¡æŸ¥ç”Ÿæˆçš„æ–‡ä»¶")
        print("2. æäº¤åˆ°ç‰ˆæœ¬æ§åˆ¶")
        print("3. åˆ›å»ºå‘å¸ƒæ ‡ç­¾")
        print("4. å‘å¸ƒåˆ°åŒ…ç®¡ç†å™¨")
    else:
        print("âŒ å‘å¸ƒå‡†å¤‡å¤±è´¥")
        sys.exit(1)
```

## ğŸ“Š æˆåŠŸæŒ‡æ ‡å’ŒéªŒæ”¶æ ‡å‡†

### æŠ€æœ¯æŒ‡æ ‡
- âœ… **é›¶å…¼å®¹å±‚ä»£ç **: å®Œå…¨ç§»é™¤ `ProcessingStep` å’Œç›¸å…³å…¼å®¹æ€§ä»£ç 
- âœ… **å•ä¸€æŠ½è±¡åŸºç±»**: åªä¿ç•™ `UnifiedStageBase`
- âœ… **æ€§èƒ½æå‡**: å¤„ç†é€Ÿåº¦æå‡ 15%ï¼Œå†…å­˜ä½¿ç”¨å‡å°‘ 20%
- âœ… **æµ‹è¯•è¦†ç›–ç‡**: ä¿æŒ > 90%
- âœ… **ä»£ç å¤æ‚åº¦**: é™ä½ 40%

### è´¨é‡æŒ‡æ ‡
- âœ… **ç±»å‹å®‰å…¨**: 100% ç±»å‹æ³¨è§£è¦†ç›–
- âœ… **æ–‡æ¡£å®Œæ•´**: API æ–‡æ¡£å’Œè¿ç§»æŒ‡å—å®Œæ•´
- âœ… **é”™è¯¯å¤„ç†**: ç»Ÿä¸€çš„å¼‚å¸¸å¤„ç†æœºåˆ¶
- âœ… **ç›‘æ§èƒ½åŠ›**: å†…ç½®æ€§èƒ½ç›‘æ§å’Œè¯Šæ–­

### ç”¨æˆ·ä½“éªŒæŒ‡æ ‡
- âœ… **å‘åå…¼å®¹**: CLI å’Œ GUI æ¥å£ä¿æŒä¸å˜
- âœ… **å¯åŠ¨é€Ÿåº¦**: åº”ç”¨å¯åŠ¨æ—¶é—´å‡å°‘ 30%
- âœ… **é”™è¯¯ä¿¡æ¯**: æ›´æ¸…æ™°çš„é”™è¯¯æç¤º
- âœ… **å¼€å‘ä½“éªŒ**: æ›´å¥½çš„ IDE æ”¯æŒå’Œè°ƒè¯•ä½“éªŒ

## ğŸ¯ æ€»ç»“

è¿™ä¸ªå½»åº•ç§»é™¤å…¼å®¹å±‚çš„æ–¹æ¡ˆé€šè¿‡**4å‘¨çš„æ¿€è¿›é‡æ„**ï¼Œå®Œå…¨æ¶ˆé™¤äº†åŒé‡æŠ½è±¡åŸºç±»çš„æŠ€æœ¯å€ºåŠ¡ï¼š

1. **ç¬¬1å‘¨**: æ·±å…¥åˆ†æå’Œé‡æ–°è®¾è®¡ï¼Œå»ºç«‹ç»Ÿä¸€æ¶æ„
2. **ç¬¬2å‘¨**: æ‰¹é‡ä»£ç è¿ç§»ï¼Œå½»åº•æ›¿æ¢æ—§å®ç°
3. **ç¬¬3å‘¨**: å…¨é¢æµ‹è¯•éªŒè¯ï¼Œç¡®ä¿è´¨é‡å’Œæ€§èƒ½
4. **ç¬¬4å‘¨**: éƒ¨ç½²å‘å¸ƒï¼Œå®Œæˆæ¶æ„ç»Ÿä¸€

### æ ¸å¿ƒä¼˜åŠ¿
- ğŸ”¥ **é›¶å¦¥å**: å®Œå…¨æ¶ˆé™¤æŠ€æœ¯å€ºåŠ¡ï¼Œä¸ç•™åæ‚£
- âš¡ **æ€§èƒ½æœ€ä¼˜**: æ— å…¼å®¹å±‚å¼€é”€ï¼Œç›´æ¥è°ƒç”¨
- ğŸ¯ **æ¶æ„çº¯å‡€**: å•ä¸€æŠ½è±¡åŸºç±»ï¼Œæ¦‚å¿µç»Ÿä¸€
- ğŸš€ **æœªæ¥å¯¼å‘**: ä¸ºé•¿æœŸå‘å±•å¥ å®šåšå®åŸºç¡€

### é£é™©æ§åˆ¶
- ğŸ“Š **å…¨é¢æµ‹è¯•**: å®Œæ•´çš„æµ‹è¯•å¥—ä»¶å’Œæ€§èƒ½åŸºå‡†
- ğŸ”„ **è‡ªåŠ¨åŒ–å·¥å…·**: æ‰¹é‡è¿ç§»å’ŒéªŒè¯å·¥å…·
- ğŸ“‹ **è¯¦ç»†è®¡åˆ’**: æ˜ç¡®çš„æ—¶é—´çº¿å’Œæ£€æŸ¥ç‚¹
- ğŸ›¡ï¸ **è´¨é‡ä¿è¯**: ä¸¥æ ¼çš„ä»£ç å®¡æŸ¥å’ŒéªŒæ”¶æ ‡å‡†

è¿™ä¸ªæ–¹æ¡ˆä½“ç°äº†**"æŠ€æœ¯å€ºåŠ¡é›¶å®¹å¿"**çš„å·¥ç¨‹ç†å¿µï¼Œé€šè¿‡ä¸€æ¬¡æ€§çš„æ¿€è¿›é‡æ„ï¼Œå½»åº•è§£å†³æ¶æ„é—®é¢˜ï¼Œä¸ºé¡¹ç›®çš„é•¿æœŸå¥åº·å‘å±•åˆ›é€ äº†æœ€ä½³æ¡ä»¶ã€‚
```
```