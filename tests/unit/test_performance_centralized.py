"""
PktMask é›†ä¸­æ€§èƒ½æµ‹è¯•æ¨¡å—

æ•´åˆåˆ†æ•£åœ¨å„ä¸ªæµ‹è¯•æ–‡ä»¶ä¸­çš„æ€§èƒ½æµ‹è¯•ï¼Œæä¾›ç»Ÿä¸€çš„æ€§èƒ½åŸºå‡†å’Œæµ‹è¯•æ ‡å‡†ã€‚
æ ¹æ®é‡å¤æµ‹è¯•é¡¹åˆ†ææŠ¥å‘Šçš„å»ºè®®åˆ›å»ºã€‚
"""
import unittest
import tempfile
import os
from unittest.mock import patch, Mock

import pytest

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from conftest import PerformanceTestSuite, BasePcapProcessingTest, ErrorHandlingTestMixin

# å¯¼å…¥éœ€è¦æµ‹è¯•çš„æ¨¡å—
try:
    from src.pktmask.core.processors import EnhancedTrimmer, _process_pcap_data_enhanced, _process_pcap_data
    from src.pktmask.core.encapsulation.adapter import ProcessingAdapter
    from src.pktmask.infrastructure.logging import log_performance
except ImportError as e:
    pytest.skip(f"æ— æ³•å¯¼å…¥æ¨¡å—: {e}", allow_module_level=True)


@pytest.mark.performance
class TestCentralizedPerformance(unittest.TestCase):
    """é›†ä¸­æ€§èƒ½æµ‹è¯•ç±»
    
    å°†åˆ†æ•£åœ¨å„ä¸ªæµ‹è¯•æ–‡ä»¶ä¸­çš„æ€§èƒ½æµ‹è¯•æ•´åˆåˆ°è¿™é‡Œï¼Œ
    æä¾›ç»Ÿä¸€çš„æ€§èƒ½åŸºå‡†å’Œæµ‹è¯•æ ‡å‡†ã€‚
    """
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.performance_suite = PerformanceTestSuite()
        self.pcap_test_base = BasePcapProcessingTest()
        self.adapter = ProcessingAdapter()
    
    def tearDown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        if hasattr(self, 'adapter'):
            self.adapter.reset_stats()
    
    def test_pcap_data_processing_performance_benchmark(self):
        """PCAPæ•°æ®å¤„ç†æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        # åˆ›å»ºä¸åŒå¤§å°çš„æµ‹è¯•æ•°æ®é›†
        test_cases = [
            ("small", BasePcapProcessingTest.create_test_packets("plain"), 2),
            ("medium", BasePcapProcessingTest.create_test_packets("mixed") * 5, 10),
            ("large", BasePcapProcessingTest.create_test_packets("vlan") * 25, 50),
        ]
        
        for case_name, packets, expected_count in test_cases:
            with self.subTest(case=case_name):
                # åŸºç¡€ç‰ˆæœ¬æ€§èƒ½æµ‹è¯•
                basic_result = PerformanceTestSuite.measure_processing_performance(
                    _process_pcap_data,
                    packets,
                    iterations=3
                )
                
                # å¢å¼ºç‰ˆæœ¬æ€§èƒ½æµ‹è¯•
                enhanced_result = PerformanceTestSuite.measure_processing_performance(
                    lambda p: _process_pcap_data_enhanced(p, self.adapter),
                    packets,
                    iterations=3
                )
                
                # éªŒè¯æ€§èƒ½æŠ¥å‘Š
                PerformanceTestSuite.verify_performance_report(basic_result)
                PerformanceTestSuite.verify_performance_report(enhanced_result)
                
                # éªŒè¯æ€§èƒ½é˜ˆå€¼
                if case_name == "small":
                    PerformanceTestSuite.assert_performance_threshold(
                        basic_result["avg_time"], "processing_time"
                    )
                    PerformanceTestSuite.assert_performance_threshold(
                        enhanced_result["avg_time"], "processing_time"
                    )
                elif case_name == "medium":
                    PerformanceTestSuite.assert_performance_threshold(
                        basic_result["avg_time"], "small_file_processing"
                    )
                    PerformanceTestSuite.assert_performance_threshold(
                        enhanced_result["avg_time"], "small_file_processing"
                    )
                
                # æ€§èƒ½æ¯”è¾ƒåˆ†æ
                comparison = PerformanceTestSuite.compare_performance(
                    basic_result, enhanced_result, tolerance=0.2  # å…è®¸20%æ€§èƒ½å·®å¼‚
                )
                
                # è®°å½•æ€§èƒ½æ¯”è¾ƒç»“æœ
                print(f"\næ€§èƒ½æ¯”è¾ƒ - {case_name}:")
                print(f"  åŸºç¡€ç‰ˆæœ¬: {basic_result['avg_time']:.4f}s")
                print(f"  å¢å¼ºç‰ˆæœ¬: {enhanced_result['avg_time']:.4f}s")
                print(f"  æ€§èƒ½æ¯”ç‡: {comparison['performance_ratio']:.2f}")
                
                # å¢å¼ºç‰ˆæœ¬æ€§èƒ½ä¸åº”æ˜¾è‘—åŠ£åŒ–ï¼ˆè¶…è¿‡50%ï¼‰
                self.assertLess(comparison['performance_ratio'], 1.5, 
                               f"{case_name} å¢å¼ºç‰ˆæœ¬æ€§èƒ½åŠ£åŒ–è¿‡å¤š")
    
    def test_file_processing_performance_benchmark(self):
        """æ–‡ä»¶å¤„ç†æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        test_packets = BasePcapProcessingTest.create_test_packets("mixed") * 10
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        input_path = BasePcapProcessingTest.create_temp_pcap_file(test_packets)
        output_path = input_path.replace('.pcap', '_output.pcap')
        
        try:
            # æ–‡ä»¶å¤„ç†æ€§èƒ½æµ‹è¯•
            def file_processing_func(_):
                step = IntelligentTrimmingStep()
                return step.process_file(input_path, output_path)
            
            performance_result = PerformanceTestSuite.measure_processing_performance(
                file_processing_func,
                None,
                iterations=3
            )
            
            # éªŒè¯æ€§èƒ½æŠ¥å‘Š
            PerformanceTestSuite.verify_performance_report(performance_result)
            
            # éªŒè¯æ€§èƒ½é˜ˆå€¼
            PerformanceTestSuite.assert_performance_threshold(
                performance_result["avg_time"],
                "small_file_processing"
            )
            
            # éªŒè¯ç»“æœæ­£ç¡®æ€§
            results = performance_result["results"]
            for result in results:
                self.assertIsNotNone(result)
                self.assertIn("total_packets", result)
                self.assertEqual(result["total_packets"], 20)  # 10ä¸ªæ··åˆåŒ… * 2åŒ…/æ··åˆ
                
        finally:
            BasePcapProcessingTest.cleanup_temp_file(input_path)
            BasePcapProcessingTest.cleanup_temp_file(output_path)
    
    def test_performance_logging_overhead(self):
        """æ€§èƒ½æ—¥å¿—å¼€é”€æµ‹è¯•"""
        packets = BasePcapProcessingTest.create_test_packets("plain")
        
        # æµ‹è¯•æ— æ—¥å¿—æƒ…å†µ
        no_log_result = PerformanceTestSuite.measure_processing_performance(
            _process_pcap_data,
            packets,
            iterations=10
        )
        
        # æµ‹è¯•æœ‰æ—¥å¿—æƒ…å†µï¼ˆæ¨¡æ‹Ÿï¼‰
        def processing_with_logging(data):
            with patch('src.pktmask.infrastructure.logging.log_performance'):
                return _process_pcap_data(data)
        
        with_log_result = PerformanceTestSuite.measure_processing_performance(
            processing_with_logging,
            packets,
            iterations=10
        )
        
        # éªŒè¯æ—¥å¿—å¼€é”€åœ¨å¯æ¥å—èŒƒå›´å†…ï¼ˆ<10%ï¼‰
        comparison = PerformanceTestSuite.compare_performance(
            no_log_result, with_log_result, tolerance=0.1
        )
        
        self.assertLess(comparison['performance_ratio'], 1.1,
                       "æ€§èƒ½æ—¥å¿—å¼€é”€è¿‡å¤§")
        
        print(f"\næ€§èƒ½æ—¥å¿—å¼€é”€åˆ†æ:")
        print(f"  æ— æ—¥å¿—: {no_log_result['avg_time']:.4f}s")
        print(f"  æœ‰æ—¥å¿—: {with_log_result['avg_time']:.4f}s")
        print(f"  å¼€é”€æ¯”ç‡: {comparison['performance_ratio']:.2f}")
    
    def test_memory_efficiency_benchmark(self):
        """å†…å­˜æ•ˆç‡åŸºå‡†æµ‹è¯•"""
        # åˆ›å»ºå¤§æ•°æ®é›†
        large_packets = BasePcapProcessingTest.create_test_packets("mixed") * 100
        
        # é‡ç½®é€‚é…å™¨ç»Ÿè®¡
        self.adapter.reset_stats()
        
        # æµ‹è¯•å†…å­˜ä½¿ç”¨ï¼ˆæ¨¡æ‹Ÿï¼‰
        def memory_test_func(data):
            # æ¨¡æ‹Ÿå¤§æ•°æ®å¤„ç†
            result = _process_pcap_data_enhanced(data, self.adapter)
            
            # éªŒè¯ç»Ÿè®¡ä¿¡æ¯æ­£ç¡®ç´¯è®¡
            stats = self.adapter.get_processing_stats()
            BasePcapProcessingTest.verify_encapsulation_stats(
                stats, 
                expected_total=200,  # 100ä¸ªæ··åˆåŒ… * 2åŒ…/æ··åˆ
                expected_encap_count=100  # æ··åˆåŒ…ä¸­ä¸€åŠæ˜¯VLAN
            )
            
            return result
        
        # æ‰§è¡Œå†…å­˜æ•ˆç‡æµ‹è¯•
        memory_result = PerformanceTestSuite.measure_processing_performance(
            memory_test_func,
            large_packets,
            iterations=1  # å¤§æ•°æ®é›†åªè¿è¡Œä¸€æ¬¡
        )
        
        # éªŒè¯æ€§èƒ½åœ¨å¤§æ•°æ®é›†ä¸‹ä»ç„¶å¯æ¥å—
        PerformanceTestSuite.assert_performance_threshold(
            memory_result["avg_time"],
            "large_file_processing"
        )
        
        print(f"\nå¤§æ•°æ®é›†å¤„ç†æ€§èƒ½:")
        print(f"  æ•°æ®åŒ…æ•°: {len(large_packets)}")
        print(f"  å¤„ç†æ—¶é—´: {memory_result['avg_time']:.4f}s")
        print(f"  ååé‡: {len(large_packets)/memory_result['avg_time']:.1f} åŒ…/ç§’")
    
    def test_error_handling_performance_impact(self):
        """é”™è¯¯å¤„ç†æ€§èƒ½å½±å“æµ‹è¯•"""
        # åˆ›å»ºæ­£å¸¸æ•°æ®åŒ…
        normal_packets = BasePcapProcessingTest.create_test_packets("plain")
        
        # åˆ›å»ºåŒ…å«é”™è¯¯çš„æ•°æ®åŒ…
        error_data = ErrorHandlingTestMixin.create_error_inducing_data()
        error_packets = [error_data["invalid_packet"]] + normal_packets
        
        # æµ‹è¯•æ­£å¸¸æƒ…å†µæ€§èƒ½
        normal_result = PerformanceTestSuite.measure_processing_performance(
            lambda p: _process_pcap_data_enhanced(p, self.adapter),
            normal_packets,
            iterations=10
        )
        
        # æµ‹è¯•é”™è¯¯æƒ…å†µæ€§èƒ½
        error_result = PerformanceTestSuite.measure_processing_performance(
            lambda p: ErrorHandlingTestMixin.assert_graceful_error_handling(
                _process_pcap_data_enhanced, p, self.adapter, expected_result_type=tuple
            ),
            error_packets,
            iterations=10
        )
        
        # éªŒè¯é”™è¯¯å¤„ç†ä¸ä¼šæ˜¾è‘—å½±å“æ€§èƒ½ï¼ˆ<30%ï¼‰
        comparison = PerformanceTestSuite.compare_performance(
            normal_result, error_result, tolerance=0.3
        )
        
        self.assertLess(comparison['performance_ratio'], 1.3,
                       "é”™è¯¯å¤„ç†æ€§èƒ½å½±å“è¿‡å¤§")
        
        print(f"\né”™è¯¯å¤„ç†æ€§èƒ½å½±å“åˆ†æ:")
        print(f"  æ­£å¸¸æƒ…å†µ: {normal_result['avg_time']:.4f}s")
        print(f"  é”™è¯¯æƒ…å†µ: {error_result['avg_time']:.4f}s")
        print(f"  å½±å“æ¯”ç‡: {comparison['performance_ratio']:.2f}")
    
    def test_performance_regression_detection(self):
        """æ€§èƒ½å›å½’æ£€æµ‹æµ‹è¯•"""
        # æ¨¡æ‹ŸåŸºçº¿æ€§èƒ½æ•°æ®
        baseline_performance = {
            "avg_time": 0.005,  # 5msåŸºçº¿
            "total_time": 0.050,
            "iterations": 10
        }
        
        # å½“å‰æ€§èƒ½æµ‹è¯•
        packets = BasePcapProcessingTest.create_test_packets("mixed")
        current_result = PerformanceTestSuite.measure_processing_performance(
            lambda p: _process_pcap_data_enhanced(p, self.adapter),
            packets,
            iterations=10
        )
        
        # æ€§èƒ½å›å½’æ£€æµ‹
        comparison = PerformanceTestSuite.compare_performance(
            baseline_performance, current_result, tolerance=0.1
        )
        
        # å¦‚æœæœ‰æ˜¾è‘—å›å½’ï¼Œå‘å‡ºè­¦å‘Š
        if comparison['regression']:
            print(f"\nâš ï¸  æ£€æµ‹åˆ°æ€§èƒ½å›å½’:")
            print(f"   åŸºçº¿æ€§èƒ½: {baseline_performance['avg_time']:.4f}s")
            print(f"   å½“å‰æ€§èƒ½: {current_result['avg_time']:.4f}s")
            print(f"   å›å½’ç¨‹åº¦: {(comparison['performance_ratio'] - 1) * 100:.1f}%")
        
        # éªŒè¯æ€§èƒ½ä»åœ¨å¯æ¥å—èŒƒå›´å†…
        PerformanceTestSuite.assert_performance_threshold(
            current_result["avg_time"],
            "processing_time"
        )


@pytest.mark.performance
class TestPerformanceUtilities(unittest.TestCase):
    """æ€§èƒ½æµ‹è¯•å·¥å…·æµ‹è¯•"""
    
    def test_performance_suite_functionality(self):
        """æµ‹è¯•æ€§èƒ½æµ‹è¯•å¥—ä»¶åŠŸèƒ½"""
        # æµ‹è¯•æ€§èƒ½æµ‹é‡
        def simple_func(data):
            import time
            time.sleep(0.001)  # æ¨¡æ‹Ÿ1mså¤„ç†æ—¶é—´
            return data
        
        result = PerformanceTestSuite.measure_processing_performance(
            simple_func, "test_data", iterations=3
        )
        
        # éªŒè¯æµ‹é‡ç»“æœ
        PerformanceTestSuite.verify_performance_report(result)
        self.assertGreaterEqual(result["avg_time"], 0.001)
        self.assertEqual(result["iterations"], 3)
    
    def test_performance_comparison(self):
        """æµ‹è¯•æ€§èƒ½æ¯”è¾ƒåŠŸèƒ½"""
        baseline = {"avg_time": 0.010}
        current = {"avg_time": 0.012}
        
        comparison = PerformanceTestSuite.compare_performance(baseline, current)
        
        self.assertAlmostEqual(comparison["performance_ratio"], 1.2, places=1)
        self.assertFalse(comparison["regression"])  # 20%å˜åŒ–åœ¨toleranceå†…
    
    def test_performance_threshold_validation(self):
        """æµ‹è¯•æ€§èƒ½é˜ˆå€¼éªŒè¯"""
        # æ­£å¸¸æƒ…å†µ
        PerformanceTestSuite.assert_performance_threshold(0.0005, "detection_time")
        
        # å¼‚å¸¸æƒ…å†µ
        with self.assertRaises(AssertionError):
            PerformanceTestSuite.assert_performance_threshold(0.002, "detection_time")
        
        # è‡ªå®šä¹‰é˜ˆå€¼
        PerformanceTestSuite.assert_performance_threshold(0.015, "custom", custom_threshold=0.020)


if __name__ == '__main__':
    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    print("ğŸš€ è¿è¡ŒPktMaské›†ä¸­æ€§èƒ½æµ‹è¯•...")
    unittest.main(verbosity=2) 