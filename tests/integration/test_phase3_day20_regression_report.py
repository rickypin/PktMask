"""
Phase 3 Day 20: å›å½’æµ‹è¯•å®Œæ•´è¿è¡Œ

éªŒæ”¶æ ‡å‡†: æ‰€æœ‰ç°æœ‰åŠŸèƒ½æ­£å¸¸
äº¤ä»˜ç‰©: å›å½’æŠ¥å‘Š

æœ¬æ¨¡å—æä¾›å…¨é¢çš„å›å½’æµ‹è¯•æŠ¥å‘Šç”Ÿæˆèƒ½åŠ›ï¼Œåˆ†æç³»ç»Ÿå…¼å®¹æ€§ã€åŠŸèƒ½ä¿æŒåº¦ã€æ€§èƒ½å›å½’ç­‰ã€‚
"""

import unittest
import subprocess
import json
import time
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
from enum import Enum


class RegressionSeverity(Enum):
    """å›å½’ä¸¥é‡ç¨‹åº¦"""
    CRITICAL = "critical"      # ç³»ç»Ÿæ— æ³•æ­£å¸¸è¿è¡Œ
    HIGH = "high"             # ä¸»è¦åŠŸèƒ½å—å½±å“
    MEDIUM = "medium"         # éƒ¨åˆ†åŠŸèƒ½å—å½±å“
    LOW = "low"               # è½»å¾®é—®é¢˜ï¼Œä¸å½±å“æ ¸å¿ƒåŠŸèƒ½
    INFO = "info"             # ä¿¡æ¯æ€§é—®é¢˜


@dataclass
class RegressionIssue:
    """å›å½’é—®é¢˜è®°å½•"""
    category: str                    # é—®é¢˜ç±»åˆ«
    description: str                 # é—®é¢˜æè¿°
    severity: RegressionSeverity     # ä¸¥é‡ç¨‹åº¦
    affected_tests: List[str]        # å—å½±å“çš„æµ‹è¯•
    error_pattern: str               # é”™è¯¯æ¨¡å¼
    recommended_action: str          # å»ºè®®æ“ä½œ
    component: str                   # å—å½±å“ç»„ä»¶


@dataclass
class TestCategoryStats:
    """æµ‹è¯•ç±»åˆ«ç»Ÿè®¡"""
    total: int = 0
    passed: int = 0
    failed: int = 0
    errors: int = 0
    skipped: int = 0
    pass_rate: float = 0.0


class RegressionAnalyzer:
    """å›å½’åˆ†æå™¨"""
    
    def __init__(self):
        self.issues: List[RegressionIssue] = []
        self.test_stats: Dict[str, TestCategoryStats] = {}
        self.start_time = datetime.now()
        
    def analyze_test_output(self, test_output: str) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•è¾“å‡ºå¹¶è¯†åˆ«å›å½’é—®é¢˜"""
        
        # è§£æåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        stats = self._extract_test_statistics(test_output)
        
        # è¯†åˆ«å›å½’é—®é¢˜
        issues = self._identify_regression_issues(test_output)
        
        # åˆ†æç³»ç»Ÿå…¼å®¹æ€§
        compatibility = self._analyze_compatibility_issues(test_output)
        
        # æ€§èƒ½å›å½’åˆ†æ
        performance = self._analyze_performance_regression(test_output)
        
        return {
            "statistics": stats,
            "issues": [issue.__dict__ for issue in issues],
            "compatibility": compatibility,
            "performance": performance,
            "analysis_time": datetime.now(),
            "summary": self._generate_summary(stats, issues)
        }
    
    def _extract_test_statistics(self, output: str) -> Dict[str, TestCategoryStats]:
        """æå–æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        
        # åˆ†ç±»ç»Ÿè®¡
        categories = {
            "unit": ["tests/unit/"],
            "integration": ["tests/integration/"],
            "tshark_enhanced": ["tshark_enhanced", "TSharkEnhanced"],
            "mask_stage": ["mask_stage", "MaskStage"],
            "fallback": ["fallback", "Fallback"],
            "pipeline": ["pipeline", "Pipeline"],
            "compatibility": ["compatibility", "AppConfig", "AttributeError"]
        }
        
        for category, patterns in categories.items():
            stats[category] = self._count_tests_by_pattern(output, patterns)
            
        return stats
    
    def _count_tests_by_pattern(self, output: str, patterns: List[str]) -> TestCategoryStats:
        """æ ¹æ®æ¨¡å¼è®¡ç®—æµ‹è¯•ç»Ÿè®¡"""
        lines = output.split('\n')
        category_stats = TestCategoryStats()
        
        for line in lines:
            if any(pattern.lower() in line.lower() for pattern in patterns):
                if "PASSED" in line:
                    category_stats.passed += 1
                elif "FAILED" in line:
                    category_stats.failed += 1
                elif "ERROR" in line:
                    category_stats.errors += 1
                elif "SKIPPED" in line:
                    category_stats.skipped += 1
                    
        category_stats.total = (category_stats.passed + category_stats.failed + 
                               category_stats.errors + category_stats.skipped)
        
        if category_stats.total > 0:
            category_stats.pass_rate = (category_stats.passed / category_stats.total) * 100
            
        return category_stats
    
    def _identify_regression_issues(self, output: str) -> List[RegressionIssue]:
        """è¯†åˆ«å›å½’é—®é¢˜"""
        issues = []
        
        # å®šä¹‰å·²çŸ¥çš„å›å½’æ¨¡å¼
        regression_patterns = [
            {
                "pattern": "'key' is an invalid keyword argument for bisect_left()",
                "category": "Pythonå…¼å®¹æ€§",
                "severity": RegressionSeverity.HIGH,
                "component": "StreamMaskTable",
                "action": "æ›´æ–°bisect_leftè°ƒç”¨è¯­æ³•ï¼Œç§»é™¤keyå‚æ•°æˆ–ä½¿ç”¨Python 3.10+å…¼å®¹å†™æ³•"
            },
            {
                "pattern": "'AppConfig' object has no attribute 'get'",
                "category": "é…ç½®ç³»ç»Ÿå…¼å®¹æ€§",
                "severity": RegressionSeverity.HIGH,
                "component": "Pipelineé…ç½®",
                "action": "ä¿®å¤AppConfigæ¥å£ï¼Œæ·»åŠ get()æ–¹æ³•æˆ–è°ƒæ•´Pipelineé…ç½®è®¿é—®æ–¹å¼"
            },
            {
                "pattern": "isn't a capture file in a format TShark understands",
                "category": "æµ‹è¯•æ•°æ®",
                "severity": RegressionSeverity.MEDIUM,
                "component": "æµ‹è¯•æ¡†æ¶",
                "action": "ä¿®å¤æµ‹è¯•PCAPæ–‡ä»¶æ ¼å¼æˆ–Mock TSharkä¾èµ–"
            },
            {
                "pattern": "Legacy Stepsç³»ç»Ÿå·²ç§»é™¤",
                "category": "æ¶æ„å˜æ›´",
                "severity": RegressionSeverity.MEDIUM,
                "component": "ProcessorFactory",
                "action": "æ›´æ–°æµ‹è¯•ä»£ç ä½¿ç”¨æ–°çš„ProcessorRegistryæ¥å£"
            },
            {
                "pattern": "æ–‡ä»¶ä¸å­˜åœ¨:",
                "category": "æµ‹è¯•èµ„æº",
                "severity": RegressionSeverity.MEDIUM,
                "component": "æµ‹è¯•æ•°æ®",
                "action": "ç¡®ä¿æ‰€æœ‰æµ‹è¯•æ•°æ®æ–‡ä»¶å­˜åœ¨æˆ–æ›´æ–°æµ‹è¯•è·¯å¾„"
            }
        ]
        
        for pattern_config in regression_patterns:
            if pattern_config["pattern"] in output:
                affected_tests = self._find_affected_tests(output, pattern_config["pattern"])
                
                issue = RegressionIssue(
                    category=pattern_config["category"],
                    description=f"æ£€æµ‹åˆ°å›å½’é—®é¢˜: {pattern_config['pattern']}",
                    severity=pattern_config["severity"],
                    affected_tests=affected_tests,
                    error_pattern=pattern_config["pattern"],
                    recommended_action=pattern_config["action"],
                    component=pattern_config["component"]
                )
                issues.append(issue)
                
        return issues
    
    def _find_affected_tests(self, output: str, pattern: str) -> List[str]:
        """æ‰¾åˆ°å—ç‰¹å®šæ¨¡å¼å½±å“çš„æµ‹è¯•"""
        affected = []
        lines = output.split('\n')
        
        for i, line in enumerate(lines):
            if pattern in line:
                # å‘ä¸ŠæŸ¥æ‰¾æµ‹è¯•åç§°
                for j in range(max(0, i-10), i):
                    test_line = lines[j]
                    if "FAILED" in test_line or "ERROR" in test_line:
                        # æå–æµ‹è¯•åç§°
                        parts = test_line.split("::")
                        if len(parts) >= 2:
                            test_name = "::".join(parts[:2])
                            if test_name not in affected:
                                affected.append(test_name)
                        break
                        
        return affected[:5]  # é™åˆ¶è¿”å›æ•°é‡
    
    def _analyze_compatibility_issues(self, output: str) -> Dict[str, Any]:
        """åˆ†æå…¼å®¹æ€§é—®é¢˜"""
        compatibility = {
            "python_version_issues": [],
            "dependency_issues": [],
            "interface_changes": [],
            "overall_compatibility": "unknown"
        }
        
        # Pythonç‰ˆæœ¬å…¼å®¹æ€§
        if "'key' is an invalid keyword argument" in output:
            compatibility["python_version_issues"].append({
                "issue": "bisect_left keyå‚æ•°ä¸å…¼å®¹",
                "python_version": "< 3.10",
                "impact": "StreamMaskTableåŠŸèƒ½å—å½±å“"
            })
            
        # æ¥å£å˜æ›´
        if "'AppConfig' object has no attribute 'get'" in output:
            compatibility["interface_changes"].append({
                "issue": "AppConfigæ¥å£å˜æ›´",
                "component": "é…ç½®ç³»ç»Ÿ",
                "impact": "Pipelineåˆå§‹åŒ–å¤±è´¥"
            })
            
        # ä¾èµ–é—®é¢˜
        if "TShark" in output and "crashed" in output:
            compatibility["dependency_issues"].append({
                "issue": "TSharkå…¼å®¹æ€§é—®é¢˜",
                "component": "åè®®åˆ†æ",
                "impact": "PCAPæ–‡ä»¶å¤„ç†å¤±è´¥"
            })
            
        # æ•´ä½“å…¼å®¹æ€§è¯„ä¼°
        issue_count = (len(compatibility["python_version_issues"]) + 
                      len(compatibility["dependency_issues"]) + 
                      len(compatibility["interface_changes"]))
        
        if issue_count == 0:
            compatibility["overall_compatibility"] = "good"
        elif issue_count <= 2:
            compatibility["overall_compatibility"] = "moderate"
        else:
            compatibility["overall_compatibility"] = "poor"
            
        return compatibility
    
    def _analyze_performance_regression(self, output: str) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½å›å½’"""
        performance = {
            "performance_tests_found": False,
            "speed_regression": False,
            "memory_issues": False,
            "timeout_issues": False,
            "overall_performance": "unknown"
        }
        
        # æŸ¥æ‰¾æ€§èƒ½ç›¸å…³æµ‹è¯•
        if "performance" in output.lower():
            performance["performance_tests_found"] = True
            
        # æŸ¥æ‰¾é€Ÿåº¦å›å½’
        if "é€Ÿåº¦ä¿ç•™ç‡ä¸è¾¾æ ‡" in output or "performance score" in output.lower():
            performance["speed_regression"] = True
            
        # æŸ¥æ‰¾è¶…æ—¶é—®é¢˜
        if "timeout" in output.lower() or "slowest" in output:
            performance["timeout_issues"] = True
            
        # æ•´ä½“æ€§èƒ½è¯„ä¼°
        if performance["speed_regression"]:
            performance["overall_performance"] = "degraded"
        elif performance["timeout_issues"]:
            performance["overall_performance"] = "slow"
        elif not performance["performance_tests_found"]:
            performance["overall_performance"] = "unknown"
        else:
            performance["overall_performance"] = "acceptable"
            
        return performance
    
    def _generate_summary(self, stats: Dict[str, TestCategoryStats], 
                         issues: List[RegressionIssue]) -> Dict[str, Any]:
        """ç”Ÿæˆå›å½’åˆ†ææ‘˜è¦"""
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_tests = sum(stat.total for stat in stats.values())
        total_passed = sum(stat.passed for stat in stats.values())
        total_failed = sum(stat.failed for stat in stats.values())
        total_errors = sum(stat.errors for stat in stats.values())
        
        overall_pass_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
        
        # é—®é¢˜ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
        severity_counts = {}
        for issue in issues:
            severity = issue.severity.value
            severity_counts[severity] = severity_counts.get(severity, 0) + 1
            
        # ç¡®å®šæ•´ä½“çŠ¶æ€
        if severity_counts.get("critical", 0) > 0:
            overall_status = "critical_regression"
        elif severity_counts.get("high", 0) > 2:
            overall_status = "significant_regression"
        elif overall_pass_rate < 70:
            overall_status = "moderate_regression"
        elif overall_pass_rate < 90:
            overall_status = "minor_regression"
        else:
            overall_status = "acceptable"
            
        return {
            "overall_status": overall_status,
            "total_tests": total_tests,
            "overall_pass_rate": round(overall_pass_rate, 1),
            "total_issues": len(issues),
            "severity_breakdown": severity_counts,
            "critical_components": self._identify_critical_components(issues),
            "recommendation": self._generate_recommendation(overall_status, issues)
        }
    
    def _identify_critical_components(self, issues: List[RegressionIssue]) -> List[str]:
        """è¯†åˆ«å…³é”®å—å½±å“ç»„ä»¶"""
        component_impact = {}
        
        for issue in issues:
            component = issue.component
            severity_weight = {
                RegressionSeverity.CRITICAL: 4,
                RegressionSeverity.HIGH: 3,
                RegressionSeverity.MEDIUM: 2,
                RegressionSeverity.LOW: 1,
                RegressionSeverity.INFO: 0
            }
            
            weight = severity_weight.get(issue.severity, 0)
            component_impact[component] = component_impact.get(component, 0) + weight
            
        # æŒ‰å½±å“ç¨‹åº¦æ’åº
        sorted_components = sorted(component_impact.items(), key=lambda x: x[1], reverse=True)
        return [comp for comp, _ in sorted_components[:5]]
    
    def _generate_recommendation(self, status: str, issues: List[RegressionIssue]) -> str:
        """ç”Ÿæˆä¿®å¤å»ºè®®"""
        
        if status == "critical_regression":
            return "ğŸš¨ å‘ç°ä¸¥é‡å›å½’é—®é¢˜ï¼Œå»ºè®®ç«‹å³åœæ­¢å‘å¸ƒå¹¶ä¿®å¤å…³é”®é—®é¢˜"
        elif status == "significant_regression":
            return "âš ï¸ å‘ç°é‡å¤§å›å½’é—®é¢˜ï¼Œå»ºè®®å»¶æœŸå‘å¸ƒç›´åˆ°ä¸»è¦é—®é¢˜è§£å†³"
        elif status == "moderate_regression":
            return "ğŸ“‹ å‘ç°ä¸­ç­‰å›å½’é—®é¢˜ï¼Œå»ºè®®ä¼˜å…ˆä¿®å¤é«˜ä¼˜å…ˆçº§é—®é¢˜åå‘å¸ƒ"
        elif status == "minor_regression":
            return "âœ… å‘ç°è½»å¾®å›å½’é—®é¢˜ï¼Œå¯ä»¥æŒ‰è®¡åˆ’å‘å¸ƒä½†éœ€è·Ÿè¸ªä¿®å¤"
        else:
            return "ğŸ‰ å›å½’æµ‹è¯•é€šè¿‡ï¼Œç³»ç»ŸçŠ¶æ€è‰¯å¥½ï¼Œå¯ä»¥å®‰å…¨å‘å¸ƒ"


class TestPhase3Day20RegressionReport(unittest.TestCase):
    """Phase 3 Day 20: å›å½’æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ"""
    
    def setUp(self):
        self.analyzer = RegressionAnalyzer()
        self.report_dir = Path("output/reports/regression")
        self.report_dir.mkdir(parents=True, exist_ok=True)
        
    def test_unit_tests_regression_analysis(self):
        """æµ‹è¯•1: å•å…ƒæµ‹è¯•å›å½’åˆ†æ"""
        print("\nğŸ“‹ æµ‹è¯•1: å•å…ƒæµ‹è¯•å›å½’åˆ†æ")
        
        # è¿è¡Œå•å…ƒæµ‹è¯•å¹¶æ•è·è¾“å‡º
        try:
            result = subprocess.run([
                "python3", "-m", "pytest", "tests/unit/", 
                "--tb=short", "--quiet", "-x"
            ], capture_output=True, text=True, timeout=300)
            
            output = result.stdout + result.stderr
            analysis = self.analyzer.analyze_test_output(output)
            
            print(f"  å•å…ƒæµ‹è¯•æ•°é‡: {analysis['statistics'].get('unit', TestCategoryStats()).total}")
            print(f"  é€šè¿‡ç‡: {analysis['statistics'].get('unit', TestCategoryStats()).pass_rate:.1f}%")
            print(f"  å‘ç°é—®é¢˜: {len(analysis['issues'])}ä¸ª")
            
            # éªŒè¯å•å…ƒæµ‹è¯•åŸºç¡€åŠŸèƒ½
            unit_stats = analysis['statistics'].get('unit', TestCategoryStats())
            if unit_stats.total > 0:
                self.assertGreaterEqual(unit_stats.pass_rate, 70.0, 
                                      f"å•å…ƒæµ‹è¯•é€šè¿‡ç‡è¿‡ä½: {unit_stats.pass_rate:.1f}%")
            
            self.test_results = analysis
            
        except subprocess.TimeoutExpired:
            self.fail("å•å…ƒæµ‹è¯•æ‰§è¡Œè¶…æ—¶")
        except Exception as e:
            self.fail(f"å•å…ƒæµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            
    def test_integration_tests_regression_analysis(self):
        """æµ‹è¯•2: é›†æˆæµ‹è¯•å›å½’åˆ†æ"""
        print("\nğŸ“‹ æµ‹è¯•2: é›†æˆæµ‹è¯•å›å½’åˆ†æ")
        
        # è¿è¡Œå…³é”®é›†æˆæµ‹è¯•
        key_integration_tests = [
            "tests/integration/test_phase2_day8_integration.py",
            "tests/integration/test_phase2_day9_processor_adapter_integration.py",
            "tests/integration/test_enhanced_mask_stage.py"
        ]
        
        all_results = []
        
        for test_file in key_integration_tests:
            if os.path.exists(test_file):
                try:
                    result = subprocess.run([
                        "python3", "-m", "pytest", test_file, 
                        "--tb=short", "--quiet"
                    ], capture_output=True, text=True, timeout=120)
                    
                    output = result.stdout + result.stderr
                    analysis = self.analyzer.analyze_test_output(output)
                    all_results.append({
                        "test_file": test_file,
                        "analysis": analysis
                    })
                    
                except subprocess.TimeoutExpired:
                    print(f"  âš ï¸ {test_file} æ‰§è¡Œè¶…æ—¶")
                except Exception as e:
                    print(f"  âŒ {test_file} æ‰§è¡Œå¤±è´¥: {e}")
                    
        print(f"  å®Œæˆé›†æˆæµ‹è¯•æ–‡ä»¶: {len(all_results)}/{len(key_integration_tests)}")
        
        # åˆ†ææ•´ä½“é›†æˆæµ‹è¯•çŠ¶æ€
        if all_results:
            total_issues = sum(len(r["analysis"]["issues"]) for r in all_results)
            print(f"  å‘ç°é›†æˆé—®é¢˜: {total_issues}ä¸ª")
            
            self.integration_results = all_results
        else:
            self.skipTest("æ²¡æœ‰å¯æ‰§è¡Œçš„é›†æˆæµ‹è¯•")
            
    def test_compatibility_regression_check(self):
        """æµ‹è¯•3: å…¼å®¹æ€§å›å½’æ£€æŸ¥"""
        print("\nğŸ“‹ æµ‹è¯•3: å…¼å®¹æ€§å›å½’æ£€æŸ¥")
        
        compatibility_issues = []
        
        # æ£€æŸ¥Pythonç‰ˆæœ¬å…¼å®¹æ€§
        try:
            import sys
            python_version = sys.version_info
            if python_version < (3, 8):
                compatibility_issues.append({
                    "type": "python_version",
                    "issue": f"Pythonç‰ˆæœ¬è¿‡ä½: {python_version}",
                    "severity": "high"
                })
                
            # æ£€æŸ¥å…³é”®ä¾èµ–
            critical_imports = [
                "scapy", "pyshark", "yaml", "dataclasses"
            ]
            
            for module_name in critical_imports:
                try:
                    __import__(module_name)
                except ImportError as e:
                    compatibility_issues.append({
                        "type": "dependency",
                        "issue": f"ç¼ºå°‘å…³é”®ä¾èµ–: {module_name}",
                        "severity": "critical"
                    })
                    
            # æ£€æŸ¥TSharkå¯ç”¨æ€§
            try:
                tshark_result = subprocess.run(
                    ["tshark", "-v"], capture_output=True, text=True, timeout=10
                )
                if tshark_result.returncode != 0:
                    compatibility_issues.append({
                        "type": "external_tool",
                        "issue": "TSharkä¸å¯ç”¨æˆ–ç‰ˆæœ¬è¿‡ä½",
                        "severity": "medium"
                    })
            except (subprocess.TimeoutExpired, FileNotFoundError):
                compatibility_issues.append({
                    "type": "external_tool", 
                    "issue": "TSharkæœªå®‰è£…æˆ–ä¸åœ¨PATHä¸­",
                    "severity": "medium"
                })
                
            print(f"  å…¼å®¹æ€§é—®é¢˜: {len(compatibility_issues)}ä¸ª")
            for issue in compatibility_issues:
                print(f"    - {issue['severity'].upper()}: {issue['issue']}")
                
            # å…¼å®¹æ€§å›å½’éªŒæ”¶æ ‡å‡†
            critical_issues = [i for i in compatibility_issues if i["severity"] == "critical"]
            high_issues = [i for i in compatibility_issues if i["severity"] == "high"]
            
            self.assertEqual(len(critical_issues), 0, 
                           f"å‘ç°{len(critical_issues)}ä¸ªä¸¥é‡å…¼å®¹æ€§é—®é¢˜")
            self.assertLessEqual(len(high_issues), 1, 
                               f"å‘ç°{len(high_issues)}ä¸ªé«˜çº§å…¼å®¹æ€§é—®é¢˜ï¼Œè¶…è¿‡å¯æ¥å—èŒƒå›´")
                               
            self.compatibility_results = compatibility_issues
            
        except Exception as e:
            self.fail(f"å…¼å®¹æ€§æ£€æŸ¥å¤±è´¥: {e}")
            
    def test_generate_comprehensive_regression_report(self):
        """æµ‹è¯•4: ç”Ÿæˆç»¼åˆå›å½’æŠ¥å‘Š"""
        print("\nğŸ“‹ æµ‹è¯•4: ç”Ÿæˆç»¼åˆå›å½’æŠ¥å‘Š")
        
        # æ”¶é›†æ‰€æœ‰æµ‹è¯•ç»“æœ
        all_test_results = getattr(self, 'test_results', {})
        integration_results = getattr(self, 'integration_results', [])
        compatibility_results = getattr(self, 'compatibility_results', [])
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = {
            "metadata": {
                "report_type": "Phase 3 Day 20 å›å½’æµ‹è¯•æŠ¥å‘Š",
                "generated_at": datetime.now().isoformat(),
                "test_environment": {
                    "python_version": f"{os.sys.version_info.major}.{os.sys.version_info.minor}.{os.sys.version_info.micro}",
                    "platform": os.name,
                    "working_directory": os.getcwd()
                }
            },
            "executive_summary": {
                "overall_status": "éœ€è¦è¿›ä¸€æ­¥åˆ†æ",
                "critical_findings": [],
                "recommendations": []
            },
            "detailed_analysis": {
                "unit_tests": all_test_results,
                "integration_tests": integration_results,
                "compatibility": compatibility_results
            },
            "action_items": []
        }
        
        # ç”Ÿæˆæ‘˜è¦å’Œå»ºè®®
        total_issues = len(compatibility_results)
        if hasattr(self, 'test_results'):
            total_issues += len(all_test_results.get('issues', []))
            
        for result in integration_results:
            total_issues += len(result["analysis"].get("issues", []))
            
        if total_issues == 0:
            report["executive_summary"]["overall_status"] = "âœ… ä¼˜ç§€ - æœªå‘ç°ä¸¥é‡å›å½’é—®é¢˜"
            report["executive_summary"]["recommendations"].append("å¯ä»¥å®‰å…¨è¿›è¡ŒPhase 3åç»­é˜¶æ®µ")
        elif total_issues <= 3:
            report["executive_summary"]["overall_status"] = "âš ï¸ è‰¯å¥½ - å‘ç°å°‘é‡é—®é¢˜"
            report["executive_summary"]["recommendations"].append("å»ºè®®ä¿®å¤å·²çŸ¥é—®é¢˜åç»§ç»­")
        elif total_issues <= 8:
            report["executive_summary"]["overall_status"] = "ğŸ“‹ éœ€è¦å…³æ³¨ - å‘ç°å¤šä¸ªå›å½’é—®é¢˜"
            report["executive_summary"]["recommendations"].append("å»ºè®®ä¼˜å…ˆä¿®å¤é«˜ä¼˜å…ˆçº§é—®é¢˜")
        else:
            report["executive_summary"]["overall_status"] = "ğŸš¨ éœ€è¦ç«‹å³å¤„ç† - å‘ç°å¤§é‡å›å½’é—®é¢˜"
            report["executive_summary"]["recommendations"].append("å»ºè®®æš‚åœå¼€å‘ï¼Œé›†ä¸­ç²¾åŠ›ä¿®å¤å›å½’é—®é¢˜")
            
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.report_dir / f"phase3_day20_regression_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            
        print(f"  ğŸ“Š å›å½’æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        print(f"  ğŸ“ˆ æ•´ä½“çŠ¶æ€: {report['executive_summary']['overall_status']}")
        print(f"  ğŸ“‹ å‘ç°é—®é¢˜æ€»æ•°: {total_issues}")
        
        # æ˜¾ç¤ºå…³é”®å‘ç°
        if total_issues > 0:
            print("  ğŸ” å…³é”®å‘ç°:")
            if compatibility_results:
                for issue in compatibility_results[:3]:
                    print(f"    - {issue['severity'].upper()}: {issue['issue']}")
                    
        # éªŒæ”¶æ ‡å‡†æ£€æŸ¥
        self.assertIsNotNone(report["metadata"]["generated_at"])
        self.assertIn("overall_status", report["executive_summary"])
        self.assertIsInstance(report["detailed_analysis"], dict)
        
        print(f"  âœ… å›å½’æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        
        # è¿”å›æŠ¥å‘Šç”¨äºåç»­åˆ†æ
        self.regression_report = report


if __name__ == "__main__":
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    suite = unittest.TestLoader().loadTestsFromTestCase(TestPhase3Day20RegressionReport)
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # è¾“å‡ºæœ€ç»ˆç»“æœ
    if result.wasSuccessful():
        print("\nğŸ‰ Phase 3 Day 20 å›å½’æµ‹è¯•å®Œæˆ - æ‰€æœ‰éªŒæ”¶æ ‡å‡†è¾¾æˆ")
    else:
        print(f"\nâš ï¸ Phase 3 Day 20 å›å½’æµ‹è¯•å‘ç°é—®é¢˜:")
        print(f"   å¤±è´¥æµ‹è¯•: {len(result.failures)}")
        print(f"   é”™è¯¯æµ‹è¯•: {len(result.errors)}") 