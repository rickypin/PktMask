#!/usr/bin/env python3
"""
PktMask é‡æ„éªŒè¯å™¨

éªŒè¯é‡æ„åçš„ä»£ç è´¨é‡ã€æ€§èƒ½å’ŒåŠŸèƒ½å®Œæ•´æ€§ã€‚
"""

import os
import sys
import subprocess
import time
import psutil
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / "src"))

@dataclass
class ValidationResult:
    """éªŒè¯ç»“æœ"""
    test_name: str
    success: bool
    duration_seconds: float
    details: Dict[str, any]
    error_message: Optional[str] = None

class RefactorValidator:
    """é‡æ„éªŒè¯å™¨"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.results: List[ValidationResult] = []
        
    def run_all_validations(self) -> bool:
        """è¿è¡Œæ‰€æœ‰éªŒè¯æµ‹è¯•"""
        print("ğŸ” å¼€å§‹é‡æ„éªŒè¯...")
        print("=" * 50)
        
        validations = [
            ("åŠŸèƒ½å®Œæ•´æ€§æµ‹è¯•", self._validate_functionality),
            ("æ€§èƒ½åŸºå‡†æµ‹è¯•", self._validate_performance),
            ("ä»£ç è´¨é‡æ£€æŸ¥", self._validate_code_quality),
            ("ä¾èµ–å…³ç³»éªŒè¯", self._validate_dependencies),
            ("æ¥å£å…¼å®¹æ€§æµ‹è¯•", self._validate_interfaces),
            ("å†…å­˜ä½¿ç”¨æµ‹è¯•", self._validate_memory_usage),
            ("é”™è¯¯å¤„ç†æµ‹è¯•", self._validate_error_handling),
            ("æ–‡æ¡£ä¸€è‡´æ€§æ£€æŸ¥", self._validate_documentation)
        ]
        
        all_passed = True
        
        for test_name, test_func in validations:
            print(f"\nğŸ“‹ æ‰§è¡Œ: {test_name}")
            result = test_func()
            self.results.append(result)
            
            if result.success:
                print(f"âœ… {test_name} é€šè¿‡")
            else:
                print(f"âŒ {test_name} å¤±è´¥: {result.error_message}")
                all_passed = False
        
        return all_passed
    
    def _validate_functionality(self) -> ValidationResult:
        """éªŒè¯åŠŸèƒ½å®Œæ•´æ€§"""
        start_time = time.time()
        
        try:
            # è¿è¡Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶
            test_commands = [
                "python -m pytest tests/unit/ -v --tb=short",
                "python -m pytest tests/integration/ -v --tb=short",
                "python -m pytest tests/e2e/ -v --tb=short -k 'not performance'"
            ]
            
            test_results = {}
            all_passed = True
            
            for cmd in test_commands:
                test_type = cmd.split()[3].split('/')[1]  # æå–æµ‹è¯•ç±»å‹
                print(f"   è¿è¡Œ {test_type} æµ‹è¯•...")
                
                result = subprocess.run(
                    cmd.split(),
                    cwd=self.project_root,
                    capture_output=True,
                    text=True,
                    timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
                )
                
                test_results[test_type] = {
                    "returncode": result.returncode,
                    "stdout": result.stdout,
                    "stderr": result.stderr
                }
                
                if result.returncode != 0:
                    all_passed = False
            
            duration = time.time() - start_time
            
            return ValidationResult(
                test_name="functionality",
                success=all_passed,
                duration_seconds=duration,
                details=test_results,
                error_message=None if all_passed else "éƒ¨åˆ†æµ‹è¯•å¤±è´¥"
            )
            
        except Exception as e:
            duration = time.time() - start_time
            return ValidationResult(
                test_name="functionality",
                success=False,
                duration_seconds=duration,
                details={},
                error_message=str(e)
            )
    
    def _validate_performance(self) -> ValidationResult:
        """éªŒè¯æ€§èƒ½åŸºå‡†"""
        start_time = time.time()
        
        try:
            # è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
            benchmark_script = self.project_root / "scripts" / "performance" / "benchmark_simplification.py"
            
            if not benchmark_script.exists():
                # åˆ›å»ºç®€å•çš„æ€§èƒ½æµ‹è¯•
                performance_data = self._run_simple_performance_test()
            else:
                result = subprocess.run(
                    [sys.executable, str(benchmark_script)],
                    cwd=self.project_root,
                    capture_output=True,
                    text=True,
                    timeout=300
                )
                
                if result.returncode == 0:
                    performance_data = json.loads(result.stdout)
                else:
                    raise Exception(f"æ€§èƒ½æµ‹è¯•å¤±è´¥: {result.stderr}")
            
            # æ£€æŸ¥æ€§èƒ½æŒ‡æ ‡
            performance_ok = self._check_performance_metrics(performance_data)
            
            duration = time.time() - start_time
            
            return ValidationResult(
                test_name="performance",
                success=performance_ok,
                duration_seconds=duration,
                details=performance_data,
                error_message=None if performance_ok else "æ€§èƒ½æŒ‡æ ‡ä¸è¾¾æ ‡"
            )
            
        except Exception as e:
            duration = time.time() - start_time
            return ValidationResult(
                test_name="performance",
                success=False,
                duration_seconds=duration,
                details={},
                error_message=str(e)
            )
    
    def _run_simple_performance_test(self) -> Dict[str, any]:
        """è¿è¡Œç®€å•çš„æ€§èƒ½æµ‹è¯•"""
        print("   è¿è¡Œç®€å•æ€§èƒ½æµ‹è¯•...")
        
        # æµ‹è¯•å¯¼å…¥æ—¶é—´
        import_start = time.time()
        try:
            import pktmask
            import_time = time.time() - import_start
            import_success = True
        except Exception as e:
            import_time = time.time() - import_start
            import_success = False
        
        # æµ‹è¯•å†…å­˜ä½¿ç”¨
        process = psutil.Process()
        memory_info = process.memory_info()
        
        return {
            "import_time_seconds": import_time,
            "import_success": import_success,
            "memory_rss_mb": memory_info.rss / 1024 / 1024,
            "memory_vms_mb": memory_info.vms / 1024 / 1024
        }
    
    def _check_performance_metrics(self, data: Dict[str, any]) -> bool:
        """æ£€æŸ¥æ€§èƒ½æŒ‡æ ‡æ˜¯å¦è¾¾æ ‡"""
        # å®šä¹‰æ€§èƒ½é˜ˆå€¼
        thresholds = {
            "import_time_seconds": 2.0,  # å¯¼å…¥æ—¶é—´ä¸è¶…è¿‡2ç§’
            "memory_rss_mb": 100.0,      # å†…å­˜ä½¿ç”¨ä¸è¶…è¿‡100MB
        }
        
        for metric, threshold in thresholds.items():
            if metric in data and data[metric] > threshold:
                print(f"   âš ï¸  æ€§èƒ½æŒ‡æ ‡ {metric} è¶…è¿‡é˜ˆå€¼: {data[metric]} > {threshold}")
                return False
        
        return True
    
    def _validate_code_quality(self) -> ValidationResult:
        """éªŒè¯ä»£ç è´¨é‡"""
        start_time = time.time()
        
        try:
            quality_checks = {}
            
            # æ£€æŸ¥ä»£ç å¤æ‚åº¦
            print("   æ£€æŸ¥ä»£ç å¤æ‚åº¦...")
            complexity_result = self._check_code_complexity()
            quality_checks["complexity"] = complexity_result
            
            # æ£€æŸ¥ä»£ç é‡å¤
            print("   æ£€æŸ¥ä»£ç é‡å¤...")
            duplication_result = self._check_code_duplication()
            quality_checks["duplication"] = duplication_result
            
            # æ£€æŸ¥å¯¼å…¥ä¾èµ–
            print("   æ£€æŸ¥å¯¼å…¥ä¾èµ–...")
            import_result = self._check_import_structure()
            quality_checks["imports"] = import_result
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            print("   æ£€æŸ¥æ–‡ä»¶å¤§å°...")
            file_size_result = self._check_file_sizes()
            quality_checks["file_sizes"] = file_size_result
            
            all_passed = all(check["passed"] for check in quality_checks.values())
            
            duration = time.time() - start_time
            
            return ValidationResult(
                test_name="code_quality",
                success=all_passed,
                duration_seconds=duration,
                details=quality_checks,
                error_message=None if all_passed else "ä»£ç è´¨é‡æ£€æŸ¥æœªé€šè¿‡"
            )
            
        except Exception as e:
            duration = time.time() - start_time
            return ValidationResult(
                test_name="code_quality",
                success=False,
                duration_seconds=duration,
                details={},
                error_message=str(e)
            )
    
    def _check_code_complexity(self) -> Dict[str, any]:
        """æ£€æŸ¥ä»£ç å¤æ‚åº¦"""
        # ç®€å•çš„å¤æ‚åº¦æ£€æŸ¥ï¼šç»Ÿè®¡æ–‡ä»¶è¡Œæ•°å’Œå‡½æ•°æ•°é‡
        src_dir = self.project_root / "src"
        total_lines = 0
        total_files = 0
        large_files = []
        
        for py_file in src_dir.rglob("*.py"):
            if "__pycache__" in str(py_file):
                continue
                
            with open(py_file, 'r', encoding='utf-8') as f:
                lines = len(f.readlines())
                total_lines += lines
                total_files += 1
                
                if lines > 500:  # è¶…è¿‡500è¡Œçš„æ–‡ä»¶
                    large_files.append({"file": str(py_file.relative_to(src_dir)), "lines": lines})
        
        avg_lines_per_file = total_lines / total_files if total_files > 0 else 0
        
        return {
            "passed": len(large_files) < 5,  # å¤§æ–‡ä»¶ä¸è¶…è¿‡5ä¸ª
            "total_lines": total_lines,
            "total_files": total_files,
            "avg_lines_per_file": avg_lines_per_file,
            "large_files": large_files
        }
    
    def _check_code_duplication(self) -> Dict[str, any]:
        """æ£€æŸ¥ä»£ç é‡å¤"""
        # ç®€å•çš„é‡å¤æ£€æŸ¥ï¼šæŸ¥æ‰¾ç›¸ä¼¼çš„ç±»åå’Œå‡½æ•°å
        src_dir = self.project_root / "src"
        class_names = []
        function_names = []
        
        for py_file in src_dir.rglob("*.py"):
            if "__pycache__" in str(py_file):
                continue
                
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                    # ç®€å•çš„æ­£åˆ™åŒ¹é…ç±»åå’Œå‡½æ•°å
                    import re
                    classes = re.findall(r'^class\s+(\w+)', content, re.MULTILINE)
                    functions = re.findall(r'^def\s+(\w+)', content, re.MULTILINE)
                    
                    class_names.extend(classes)
                    function_names.extend(functions)
            except Exception:
                continue
        
        # æ£€æŸ¥é‡å¤
        duplicate_classes = [name for name in set(class_names) if class_names.count(name) > 1]
        duplicate_functions = [name for name in set(function_names) if function_names.count(name) > 3]  # å‡½æ•°é‡å¤é˜ˆå€¼æ›´é«˜
        
        return {
            "passed": len(duplicate_classes) < 3 and len(duplicate_functions) < 10,
            "duplicate_classes": duplicate_classes,
            "duplicate_functions": duplicate_functions
        }
    
    def _check_import_structure(self) -> Dict[str, any]:
        """æ£€æŸ¥å¯¼å…¥ç»“æ„"""
        # æ£€æŸ¥æ˜¯å¦æœ‰å¾ªç¯å¯¼å…¥å’Œè¿‡æ·±çš„å¯¼å…¥å±‚æ¬¡
        src_dir = self.project_root / "src"
        import_issues = []
        
        for py_file in src_dir.rglob("*.py"):
            if "__pycache__" in str(py_file):
                continue
                
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                    # æ£€æŸ¥ç›¸å¯¹å¯¼å…¥å±‚æ¬¡
                    import re
                    relative_imports = re.findall(r'from\s+(\.{2,})', content)
                    if any(len(imp) > 3 for imp in relative_imports):  # è¶…è¿‡3å±‚çš„ç›¸å¯¹å¯¼å…¥
                        import_issues.append(f"æ·±å±‚ç›¸å¯¹å¯¼å…¥: {py_file.relative_to(src_dir)}")
                        
            except Exception:
                continue
        
        return {
            "passed": len(import_issues) == 0,
            "issues": import_issues
        }
    
    def _check_file_sizes(self) -> Dict[str, any]:
        """æ£€æŸ¥æ–‡ä»¶å¤§å°"""
        src_dir = self.project_root / "src"
        large_files = []
        total_size = 0
        
        for py_file in src_dir.rglob("*.py"):
            if "__pycache__" in str(py_file):
                continue
                
            size = py_file.stat().st_size
            total_size += size
            
            if size > 50000:  # è¶…è¿‡50KBçš„æ–‡ä»¶
                large_files.append({
                    "file": str(py_file.relative_to(src_dir)),
                    "size_kb": size / 1024
                })
        
        return {
            "passed": len(large_files) < 3,  # å¤§æ–‡ä»¶ä¸è¶…è¿‡3ä¸ª
            "total_size_mb": total_size / 1024 / 1024,
            "large_files": large_files
        }
    
    def _validate_dependencies(self) -> ValidationResult:
        """éªŒè¯ä¾èµ–å…³ç³»"""
        start_time = time.time()
        
        try:
            # æ£€æŸ¥æ˜¯å¦èƒ½æ­£å¸¸å¯¼å…¥ä¸»è¦æ¨¡å—
            import_tests = [
                "pktmask",
                "pktmask.core",
                "pktmask.gui",
                "pktmask.cli"
            ]
            
            import_results = {}
            all_passed = True
            
            for module in import_tests:
                try:
                    __import__(module)
                    import_results[module] = {"success": True, "error": None}
                except Exception as e:
                    import_results[module] = {"success": False, "error": str(e)}
                    all_passed = False
            
            duration = time.time() - start_time
            
            return ValidationResult(
                test_name="dependencies",
                success=all_passed,
                duration_seconds=duration,
                details=import_results,
                error_message=None if all_passed else "æ¨¡å—å¯¼å…¥å¤±è´¥"
            )
            
        except Exception as e:
            duration = time.time() - start_time
            return ValidationResult(
                test_name="dependencies",
                success=False,
                duration_seconds=duration,
                details={},
                error_message=str(e)
            )
    
    def _validate_interfaces(self) -> ValidationResult:
        """éªŒè¯æ¥å£å…¼å®¹æ€§"""
        start_time = time.time()
        
        try:
            # æ£€æŸ¥å…³é”®æ¥å£æ˜¯å¦å­˜åœ¨
            interface_checks = {}
            
            # æ£€æŸ¥CLIæ¥å£
            cli_result = subprocess.run(
                [sys.executable, "-m", "pktmask", "--help"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=30
            )
            interface_checks["cli"] = {"success": cli_result.returncode == 0}
            
            # æ£€æŸ¥GUIæ¥å£ï¼ˆåœ¨æµ‹è¯•æ¨¡å¼ä¸‹ï¼‰
            env = os.environ.copy()
            env["PKTMASK_TEST_MODE"] = "true"
            env["PKTMASK_HEADLESS"] = "true"
            
            gui_result = subprocess.run(
                [sys.executable, "-c", "from pktmask.gui.main_window import main; main()"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=30,
                env=env
            )
            interface_checks["gui"] = {"success": gui_result.returncode == 0}
            
            all_passed = all(check["success"] for check in interface_checks.values())
            
            duration = time.time() - start_time
            
            return ValidationResult(
                test_name="interfaces",
                success=all_passed,
                duration_seconds=duration,
                details=interface_checks,
                error_message=None if all_passed else "æ¥å£éªŒè¯å¤±è´¥"
            )
            
        except Exception as e:
            duration = time.time() - start_time
            return ValidationResult(
                test_name="interfaces",
                success=False,
                duration_seconds=duration,
                details={},
                error_message=str(e)
            )
    
    def _validate_memory_usage(self) -> ValidationResult:
        """éªŒè¯å†…å­˜ä½¿ç”¨"""
        start_time = time.time()
        
        try:
            # æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ
            process = psutil.Process()
            initial_memory = process.memory_info().rss
            
            # å¯¼å…¥ä¸»è¦æ¨¡å—
            import pktmask
            
            after_import_memory = process.memory_info().rss
            memory_increase = (after_import_memory - initial_memory) / 1024 / 1024  # MB
            
            # æ£€æŸ¥å†…å­˜å¢é•¿æ˜¯å¦åˆç†
            memory_ok = memory_increase < 50  # å¯¼å…¥ä¸åº”è¯¥å¢åŠ è¶…è¿‡50MBå†…å­˜
            
            duration = time.time() - start_time
            
            return ValidationResult(
                test_name="memory_usage",
                success=memory_ok,
                duration_seconds=duration,
                details={
                    "initial_memory_mb": initial_memory / 1024 / 1024,
                    "after_import_memory_mb": after_import_memory / 1024 / 1024,
                    "memory_increase_mb": memory_increase
                },
                error_message=None if memory_ok else f"å†…å­˜å¢é•¿è¿‡å¤š: {memory_increase:.1f}MB"
            )
            
        except Exception as e:
            duration = time.time() - start_time
            return ValidationResult(
                test_name="memory_usage",
                success=False,
                duration_seconds=duration,
                details={},
                error_message=str(e)
            )
    
    def _validate_error_handling(self) -> ValidationResult:
        """éªŒè¯é”™è¯¯å¤„ç†"""
        start_time = time.time()
        
        try:
            # æµ‹è¯•é”™è¯¯å¤„ç†æœºåˆ¶
            error_tests = {}
            
            # æµ‹è¯•æ— æ•ˆè¾“å…¥å¤„ç†
            cli_error_result = subprocess.run(
                [sys.executable, "-m", "pktmask", "mask", "nonexistent.pcap", "-o", "output.pcap"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            # åº”è¯¥è¿”å›éé›¶é€€å‡ºç ï¼Œä½†ä¸åº”è¯¥å´©æºƒ
            error_tests["invalid_input"] = {
                "handled_gracefully": cli_error_result.returncode != 0 and "Traceback" not in cli_error_result.stderr
            }
            
            all_passed = all(test["handled_gracefully"] for test in error_tests.values())
            
            duration = time.time() - start_time
            
            return ValidationResult(
                test_name="error_handling",
                success=all_passed,
                duration_seconds=duration,
                details=error_tests,
                error_message=None if all_passed else "é”™è¯¯å¤„ç†éªŒè¯å¤±è´¥"
            )
            
        except Exception as e:
            duration = time.time() - start_time
            return ValidationResult(
                test_name="error_handling",
                success=False,
                duration_seconds=duration,
                details={},
                error_message=str(e)
            )
    
    def _validate_documentation(self) -> ValidationResult:
        """éªŒè¯æ–‡æ¡£ä¸€è‡´æ€§"""
        start_time = time.time()
        
        try:
            docs_dir = self.project_root / "docs"
            doc_checks = {}
            
            # æ£€æŸ¥å…³é”®æ–‡æ¡£æ˜¯å¦å­˜åœ¨
            required_docs = [
                "README.md",
                "docs/architecture/ABSTRACTION_LAYER_SIMPLIFICATION_PLAN.md"
            ]
            
            missing_docs = []
            for doc in required_docs:
                doc_path = self.project_root / doc
                if not doc_path.exists():
                    missing_docs.append(doc)
            
            doc_checks["required_docs"] = {
                "all_present": len(missing_docs) == 0,
                "missing": missing_docs
            }
            
            all_passed = doc_checks["required_docs"]["all_present"]
            
            duration = time.time() - start_time
            
            return ValidationResult(
                test_name="documentation",
                success=all_passed,
                duration_seconds=duration,
                details=doc_checks,
                error_message=None if all_passed else f"ç¼ºå°‘æ–‡æ¡£: {missing_docs}"
            )
            
        except Exception as e:
            duration = time.time() - start_time
            return ValidationResult(
                test_name="documentation",
                success=False,
                duration_seconds=duration,
                details={},
                error_message=str(e)
            )
    
    def generate_report(self) -> str:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        report_path = self.project_root / "reports" / f"refactor_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_path.parent.mkdir(exist_ok=True)
        
        report_data = {
            "timestamp": datetime.now().isoformat(),
            "total_tests": len(self.results),
            "passed_tests": sum(1 for r in self.results if r.success),
            "failed_tests": sum(1 for r in self.results if not r.success),
            "total_duration": sum(r.duration_seconds for r in self.results),
            "overall_success": all(r.success for r in self.results),
            "tests": [
                {
                    "name": r.test_name,
                    "success": r.success,
                    "duration": r.duration_seconds,
                    "details": r.details,
                    "error": r.error_message
                }
                for r in self.results
            ]
        }
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        return str(report_path)

def main():
    """ä¸»å‡½æ•°"""
    validator = RefactorValidator(project_root)
    
    print("ğŸ” PktMask é‡æ„éªŒè¯å™¨")
    print("=" * 50)
    
    success = validator.run_all_validations()
    
    # ç”ŸæˆæŠ¥å‘Š
    report_path = validator.generate_report()
    
    print(f"\nğŸ“Š éªŒè¯æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    if success:
        print("\nğŸ‰ æ‰€æœ‰éªŒè¯æµ‹è¯•é€šè¿‡ï¼é‡æ„æˆåŠŸï¼")
        sys.exit(0)
    else:
        print("\nâŒ éƒ¨åˆ†éªŒè¯æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æŠ¥å‘Šè¯¦æƒ…")
        sys.exit(1)

if __name__ == "__main__":
    main()
