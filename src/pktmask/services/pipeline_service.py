"""
Pipeline æœåŠ¡æ¥å£
æä¾› GUI ä¸æ ¸å¿ƒç®¡é“çš„è§£è€¦æ¥å£
"""

from typing import Any, Callable, Dict, Optional, Tuple

from pktmask.core.events import PipelineEvents
from pktmask.infrastructure.logging import get_logger


# å®šä¹‰æœåŠ¡å±‚å¼‚å¸¸
class PipelineServiceError(Exception):
    """æœåŠ¡å±‚åŸºç¡€å¼‚å¸¸"""


class ConfigurationError(PipelineServiceError):
    """é…ç½®é”™è¯¯"""


logger = get_logger("PipelineService")

# åˆ›å»ºç®¡é“æ‰§è¡Œå™¨
# Dummy implementation; replace ... with real logic


def create_pipeline_executor(config: Dict) -> object:
    """
    åˆ›å»ºç®¡é“æ‰§è¡Œå™¨

    Args:
        config: ç®¡é“é…ç½®å­—å…¸ï¼ŒåŒ…å«å„é˜¶æ®µçš„å¯ç”¨çŠ¶æ€å’Œå‚æ•°

    Returns:
        æ‰§è¡Œå™¨å¯¹è±¡ï¼ˆå¯¹ GUI ä¸é€æ˜ï¼‰
    """
    try:
        from pktmask.core.pipeline.executor import PipelineExecutor

        return PipelineExecutor(config)
    except Exception as e:
        logger.error(f"[Service] Failed to create executor: {e}")
        raise PipelineServiceError("Failed to create executor")


# å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰ PCAP æ–‡ä»¶
# Dummy implementation; replace ... with real logic


def process_directory(
    executor: object,
    input_dir: str,
    output_dir: str,
    progress_callback: Callable[[PipelineEvents, Dict], None],
    is_running_check: Callable[[], bool],
) -> None:
    """
    å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰ PCAP æ–‡ä»¶

    Args:
        executor: æ‰§è¡Œå™¨å¯¹è±¡
        input_dir: è¾“å…¥ç›®å½•è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        is_running_check: æ£€æŸ¥æ˜¯å¦ç»§ç»­è¿è¡Œçš„å‡½æ•°
    """
    try:
        import os

        logger.info(f"[Service] Starting directory processing: {input_dir}")

        # å‘é€ç®¡é“å¼€å§‹äº‹ä»¶
        progress_callback(PipelineEvents.PIPELINE_START, {"total_subdirs": 1})

        # æ‰«æç›®å½•ä¸­çš„PCAPæ–‡ä»¶
        pcap_files = []
        for file in os.scandir(input_dir):
            if file.name.endswith((".pcap", ".pcapng")):
                pcap_files.append(file.path)

        if not pcap_files:
            progress_callback(
                PipelineEvents.LOG, {"message": "No PCAP files found in directory"}
            )
            progress_callback(PipelineEvents.PIPELINE_END, {})
            return

        # å‘é€å­ç›®å½•å¼€å§‹äº‹ä»¶
        rel_subdir = os.path.relpath(input_dir, input_dir)
        progress_callback(
            PipelineEvents.SUBDIR_START,
            {
                "name": rel_subdir,
                "current": 1,
                "total": 1,
                "file_count": len(pcap_files),
            },
        )

        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for input_path in pcap_files:
            if not is_running_check():
                break

            # å‘é€æ–‡ä»¶å¼€å§‹äº‹ä»¶
            progress_callback(PipelineEvents.FILE_START, {"path": input_path})

            try:
                # æ„é€ è¾“å‡ºæ–‡ä»¶å
                base_name, ext = os.path.splitext(os.path.basename(input_path))
                output_path = os.path.join(output_dir, f"{base_name}_processed{ext}")

                # ä½¿ç”¨ executor å¤„ç†æ–‡ä»¶
                result = executor.run(
                    input_path,
                    output_path,
                    progress_cb=lambda stage, stats: _handle_stage_progress(
                        stage, stats, progress_callback
                    ),
                )

                # Check if processing was successful
                if not result.success:
                    # Send error information to GUI for failed processing
                    for error in result.errors:
                        progress_callback(
                            PipelineEvents.ERROR,
                            {
                                "message": f"File {os.path.basename(input_path)}: {error}"
                            },
                        )

                    # Send user-friendly error messages from stage statistics
                    for stage_stats in result.stage_stats:
                        if "user_message" in stage_stats.extra_metrics:
                            progress_callback(
                                PipelineEvents.ERROR,
                                {
                                    "message": f"File {os.path.basename(input_path)}: {stage_stats.extra_metrics['user_message']}"
                                },
                            )

                # å‘é€æ­¥éª¤æ‘˜è¦äº‹ä»¶ (for both successful and failed stages)
                for stage_stats in result.stage_stats:
                    progress_callback(
                        PipelineEvents.STEP_SUMMARY,
                        {
                            "step_name": stage_stats.stage_name,
                            "filename": os.path.basename(input_path),
                            "packets_processed": stage_stats.packets_processed,
                            "packets_modified": stage_stats.packets_modified,
                            "duration_ms": stage_stats.duration_ms,
                            **stage_stats.extra_metrics,
                        },
                    )

            except Exception as e:
                # Log the exception with full context
                logger.error(
                    f"[Service] Unexpected error processing file {input_path}: {e}",
                    exc_info=True,
                )

                # Send user-friendly error message to GUI
                progress_callback(
                    PipelineEvents.ERROR,
                    {
                        "message": f"Unexpected error processing file {os.path.basename(input_path)}: {str(e)}"
                    },
                )

            # å‘é€æ–‡ä»¶å®Œæˆäº‹ä»¶
            progress_callback(PipelineEvents.FILE_END, {"path": input_path})

        # å‘é€å­ç›®å½•ç»“æŸäº‹ä»¶
        progress_callback(PipelineEvents.SUBDIR_END, {"name": rel_subdir})

        # å‘é€ç®¡é“ç»“æŸäº‹ä»¶
        progress_callback(PipelineEvents.PIPELINE_END, {})

        logger.info(f"[Service] Completed directory processing: {input_dir}")

    except Exception as e:
        logger.error(f"[Service] Directory processing failed: {e}")
        raise PipelineServiceError(f"Directory processing failed: {str(e)}")


def _handle_stage_progress(stage, stats, progress_callback):
    """å¤„ç†é˜¶æ®µè¿›åº¦å›è°ƒ"""
    # Get standardized display name for the stage
    stage_display_name = _get_stage_display_name(stage.name)

    # Emit log with stage-specific action wording and correct statistics
    if stage.name in ["DeduplicationStage", "UnifiedDeduplicationStage"]:
        msg = f"- {stage_display_name}: processed {stats.packets_processed} pkts, removed {stats.packets_modified} pkts"
    elif stage.name in ["AnonStage", "IPAnonymizationStage", "UnifiedIPAnonymizationStage"]:
        # For IP anonymization, show IP statistics instead of packet statistics
        original_ips = getattr(stats, "original_ips", 0) or stats.extra_metrics.get(
            "original_ips", 0
        )
        anonymized_ips = getattr(stats, "anonymized_ips", 0) or stats.extra_metrics.get(
            "anonymized_ips", 0
        )
        if original_ips > 0:
            msg = f"- {stage_display_name}: processed {original_ips} IPs, anonymized {anonymized_ips} IPs"
        else:
            # Fallback to packet count if IP statistics are not available
            msg = f"- {stage_display_name}: processed {stats.packets_processed} pkts, anonymized {stats.packets_modified} IPs"
    else:
        msg = f"- {stage_display_name}: processed {stats.packets_processed} pkts, masked {stats.packets_modified} pkts"
    progress_callback(PipelineEvents.LOG, {"message": msg})


def _get_stage_display_name(stage_name: str) -> str:
    """Get standardized display name for stage based on naming consistency guide"""
    stage_name_mapping = {
        "DeduplicationStage": "Deduplication Stage",
        "UnifiedDeduplicationStage": "Deduplication Stage",  # New Unified implementation
        "AnonStage": "Anonymize IPs Stage",
        "IPAnonymizationStage": "Anonymize IPs Stage",  # Old StageBase implementation
        "UnifiedIPAnonymizationStage": "Anonymize IPs Stage",  # New Unified implementation
        "NewMaskPayloadStage": "Mask Payloads Stage",
        "MaskStage": "Mask Payloads Stage",
        "MaskPayloadStage": "Mask Payloads Stage",
        "Mask Payloads (v2)": "Mask Payloads Stage",
    }
    return stage_name_mapping.get(stage_name, stage_name)


# åœæ­¢ç®¡é“æ‰§è¡Œ
# Dummy implementation; replace ... with real logic


def stop_pipeline(executor: object) -> None:
    """åœæ­¢ç®¡é“æ‰§è¡Œ"""
    try:
        # å°è¯•è°ƒç”¨æ‰§è¡Œå™¨çš„stopæ–¹æ³•
        if hasattr(executor, "stop"):
            executor.stop()
            logger.info("[Service] Pipeline stopped")
        else:
            logger.warning("[Service] Executor does not support stop method")
    except Exception as e:
        logger.error(f"[Service] Failed to stop pipeline: {e}")
        raise PipelineServiceError(f"Failed to stop pipeline: {str(e)}")


# è¿”å›å½“å‰æ‰§è¡Œå™¨çš„ç»Ÿè®¡ä¿¡æ¯
# Dummy implementation; replace ... with real logic


def get_pipeline_status(executor: object) -> Dict[str, Any]:
    """è¿”å›å½“å‰æ‰§è¡Œå™¨çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œä¾‹å¦‚å·²å¤„ç†æ–‡ä»¶æ•°ç­‰"""
    return {}


# åœ¨çœŸæ­£åˆ›å»ºæ‰§è¡Œå™¨å‰éªŒè¯é…ç½®
# Dummy implementation; replace ... with real logic


def validate_config(config: Dict) -> Tuple[bool, Optional[str]]:
    """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
    if not config:
        return False, "Configuration is empty"
    return True, None


# æ ¹æ®åŠŸèƒ½å¼€å…³æ„å»ºç®¡é“é…ç½®
# Dummy implementation; replace ... with real logic


def build_pipeline_config(
    enable_anon: bool, enable_dedup: bool, enable_mask: bool
) -> Dict:
    """Build pipeline configuration based on feature switches (using standard naming conventions)"""
    # ä½¿ç”¨ç»Ÿä¸€çš„é…ç½®æœåŠ¡
    from pktmask.services.config_service import get_config_service

    service = get_config_service()
    options = service.create_options_from_gui(
        dedup_checked=enable_dedup, anon_checked=enable_anon, mask_checked=enable_mask
    )

    return service.build_pipeline_config(options)


# ============================================================================
# CLI ç»Ÿä¸€æœåŠ¡æ¥å£
# ============================================================================


def process_single_file(
    executor: object,
    input_file: str,
    output_file: str,
    progress_callback: Optional[Callable[[PipelineEvents, Dict], None]] = None,
    verbose: bool = False,
) -> Dict[str, Any]:
    """
    å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆCLIä¸“ç”¨æ¥å£ï¼‰

    Args:
        executor: æ‰§è¡Œå™¨å¯¹è±¡
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        verbose: æ˜¯å¦å¯ç”¨è¯¦ç»†è¾“å‡º

    Returns:
        å¤„ç†ç»“æœå­—å…¸ï¼ŒåŒ…å«ç»Ÿè®¡ä¿¡æ¯å’ŒçŠ¶æ€
    """
    try:
        logger.info(f"[Service] Processing single file: {input_file}")

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        from pathlib import Path

        output_path = Path(output_file)
        output_dir = output_path.parent

        if not output_dir.exists():
            try:
                output_dir.mkdir(parents=True, exist_ok=True)
                logger.info(f"[Service] Created output directory: {output_dir}")
            except Exception as e:
                logger.error(
                    f"[Service] Failed to create output directory {output_dir}: {e}"
                )
                raise PipelineServiceError(
                    f"Failed to create output directory: {str(e)}"
                )

        # å‘é€å¤„ç†å¼€å§‹äº‹ä»¶
        if progress_callback:
            progress_callback(PipelineEvents.PIPELINE_START, {"total_files": 1})
            progress_callback(PipelineEvents.FILE_START, {"path": input_file})

        # åˆ›å»ºè¿›åº¦å›è°ƒåŒ…è£…å™¨
        def stage_progress_wrapper(stage, stats):
            if progress_callback:
                _handle_stage_progress(stage, stats, progress_callback)

        # æ‰§è¡Œå¤„ç†
        result = executor.run(
            input_file,
            output_file,
            progress_cb=stage_progress_wrapper if verbose else None,
        )

        # å‘é€å¤„ç†å®Œæˆäº‹ä»¶
        if progress_callback:
            progress_callback(
                PipelineEvents.FILE_END,
                {
                    "path": input_file,
                    "output_path": output_file,
                    "success": result.success,
                },
            )
            progress_callback(PipelineEvents.PIPELINE_END, {})

        # è¿”å›ç»Ÿä¸€æ ¼å¼çš„ç»“æœ
        return {
            "success": result.success,
            "input_file": result.input_file,
            "output_file": result.output_file,
            "duration_ms": result.duration_ms,
            "stage_stats": [
                stats.model_dump() if hasattr(stats, "model_dump") else stats.__dict__
                for stats in result.stage_stats
            ],
            "errors": result.errors,
            "total_files": 1,
            "processed_files": 1 if result.success else 0,
        }

    except Exception as e:
        logger.error(f"[Service] Failed to process single file: {e}")
        if progress_callback:
            progress_callback(PipelineEvents.ERROR, {"message": str(e)})
            progress_callback(PipelineEvents.PIPELINE_END, {})
        raise PipelineServiceError(f"Failed to process file: {str(e)}")


def process_directory_cli(
    executor: object,
    input_dir: str,
    output_dir: str,
    progress_callback: Optional[Callable[[PipelineEvents, Dict], None]] = None,
    verbose: bool = False,
    file_pattern: str = "*.pcap,*.pcapng",
) -> Dict[str, Any]:
    """
    å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆCLIä¸“ç”¨æ¥å£ï¼‰

    Args:
        executor: æ‰§è¡Œå™¨å¯¹è±¡
        input_dir: è¾“å…¥ç›®å½•è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        verbose: æ˜¯å¦å¯ç”¨è¯¦ç»†è¾“å‡º
        file_pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼

    Returns:
        å¤„ç†ç»“æœå­—å…¸ï¼ŒåŒ…å«ç»Ÿè®¡ä¿¡æ¯å’ŒçŠ¶æ€
    """
    try:
        import glob
        import os

        logger.info(f"[Service] Processing directory: {input_dir}")

        # æ‰«æåŒ¹é…çš„æ–‡ä»¶
        pcap_files = []
        patterns = file_pattern.split(",")
        for pattern in patterns:
            pattern = pattern.strip()
            files = glob.glob(os.path.join(input_dir, pattern))
            pcap_files.extend(files)

        if not pcap_files:
            logger.warning(
                f"[Service] No matching files found in directory: {input_dir}"
            )
            return {
                "success": True,
                "input_dir": input_dir,
                "output_dir": output_dir,
                "duration_ms": 0.0,
                "total_files": 0,
                "processed_files": 0,
                "failed_files": 0,
                "errors": [],
            }

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)

        # å‘é€å¤„ç†å¼€å§‹äº‹ä»¶
        if progress_callback:
            progress_callback(
                PipelineEvents.PIPELINE_START, {"total_files": len(pcap_files)}
            )

        # å¤„ç†ç»Ÿè®¡
        processed_files = 0
        failed_files = 0
        all_errors = []
        total_duration = 0.0

        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for i, input_file in enumerate(pcap_files):
            try:
                # æ„é€ è¾“å‡ºæ–‡ä»¶å
                base_name = os.path.splitext(os.path.basename(input_file))[0]
                ext = os.path.splitext(input_file)[1]
                output_file = os.path.join(output_dir, f"{base_name}_processed{ext}")

                # å¤„ç†å•ä¸ªæ–‡ä»¶
                result = process_single_file(
                    executor, input_file, output_file, progress_callback, verbose
                )

                if result["success"]:
                    processed_files += 1
                else:
                    failed_files += 1
                    all_errors.extend(result.get("errors", []))

                total_duration += result["duration_ms"]

            except Exception as e:
                failed_files += 1
                error_msg = f"Failed to process {input_file}: {str(e)}"
                all_errors.append(error_msg)
                logger.error(f"[Service] {error_msg}")

                if progress_callback:
                    progress_callback(PipelineEvents.ERROR, {"message": error_msg})

        # å‘é€å¤„ç†å®Œæˆäº‹ä»¶
        if progress_callback:
            progress_callback(PipelineEvents.PIPELINE_END, {})

        # è¿”å›ç»Ÿä¸€æ ¼å¼çš„ç»“æœ
        return {
            "success": failed_files == 0,
            "input_dir": input_dir,
            "output_dir": output_dir,
            "duration_ms": total_duration,
            "total_files": len(pcap_files),
            "processed_files": processed_files,
            "failed_files": failed_files,
            "errors": all_errors,
        }

    except Exception as e:
        logger.error(f"[Service] Failed to process directory: {e}")
        if progress_callback:
            progress_callback(PipelineEvents.ERROR, {"message": str(e)})
            progress_callback(PipelineEvents.PIPELINE_END, {})
        raise PipelineServiceError(f"Failed to process directory: {str(e)}")


def create_gui_compatible_report_data(result: Dict[str, Any]) -> Dict[str, Any]:
    """åˆ›å»ºä¸GUIå…¼å®¹çš„æŠ¥å‘Šæ•°æ®æ ¼å¼"""
    # è½¬æ¢CLIç»“æœä¸ºGUIæŠ¥å‘Šç®¡ç†å™¨æœŸæœ›çš„æ ¼å¼
    gui_report_data = {
        "step_results": {},
        "total_files": result.get("total_files", 1),
        "processed_files": result.get("processed_files", 0),
        "failed_files": result.get("failed_files", 0),
        "duration_ms": result.get("duration_ms", 0.0),
        "output_directory": result.get("output_dir") or result.get("output_file"),
        "success": result.get("success", False),
    }

    # è½¬æ¢é˜¶æ®µç»Ÿè®¡æ•°æ®
    stage_stats = result.get("stage_stats", [])
    for stage_stat in stage_stats:
        if isinstance(stage_stat, dict):
            stage_name = stage_stat.get("stage_name", "Unknown")

            # æ˜ å°„åˆ°GUIæœŸæœ›çš„æ ¼å¼
            if "dedup" in stage_name.lower():
                gui_report_data["step_results"]["Deduplication"] = {
                    "packets_processed": stage_stat.get("packets_processed", 0),
                    "packets_modified": stage_stat.get("packets_modified", 0),
                    "duration_ms": stage_stat.get("duration_ms", 0.0),
                    "summary": f"Processed {stage_stat.get('packets_processed', 0)} packets, removed {stage_stat.get('packets_modified', 0)} duplicates",
                }
            elif "anon" in stage_name.lower() or "ip" in stage_name.lower():
                gui_report_data["step_results"]["Anonymize IPs"] = {
                    "packets_processed": stage_stat.get("packets_processed", 0),
                    "packets_modified": stage_stat.get("packets_modified", 0),
                    "ips_anonymized": stage_stat.get("packets_modified", 0),
                    "duration_ms": stage_stat.get("duration_ms", 0.0),
                    "summary": f"Anonymized {stage_stat.get('packets_modified', 0)} IP addresses",
                }
            elif "mask" in stage_name.lower() or "payload" in stage_name.lower():
                gui_report_data["step_results"]["Mask Payloads"] = {
                    "packets_processed": stage_stat.get("packets_processed", 0),
                    "packets_modified": stage_stat.get("packets_modified", 0),
                    "total_packets": stage_stat.get("packets_processed", 0),
                    "duration_ms": stage_stat.get("duration_ms", 0.0),
                    "summary": f"Masked {stage_stat.get('packets_modified', 0)} payload packets",
                    "data": {
                        "total_packets": stage_stat.get("packets_processed", 0),
                        "output_filename": result.get("output_file"),
                    },
                }

    return gui_report_data


def generate_gui_style_report(result: Dict[str, Any]) -> str:
    """ç”ŸæˆGUIé£æ ¼çš„æŠ¥å‘Šæ–‡æœ¬"""
    gui_data = create_gui_compatible_report_data(result)

    # ä½¿ç”¨GUIæŠ¥å‘Šç®¡ç†å™¨çš„æ ¼å¼
    report_lines = []

    # æ ‡é¢˜
    report_lines.append("=" * 70)
    report_lines.append("ğŸ“‹ PROCESSING SUMMARY")
    report_lines.append("=" * 70)
    report_lines.append("")

    # åŸºæœ¬ä¿¡æ¯
    report_lines.append(
        f"ğŸ“Š Files Processed: {gui_data['processed_files']}/{gui_data['total_files']}"
    )
    if gui_data["failed_files"] > 0:
        report_lines.append(f"âŒ Failed Files: {gui_data['failed_files']}")
    report_lines.append(f"â±ï¸  Total Duration: {gui_data['duration_ms']:.1f} ms")
    report_lines.append("")

    # é˜¶æ®µç»“æœ
    if gui_data["step_results"]:
        report_lines.append("ğŸ“ˆ Step Statistics:")
        report_lines.append("")

        for step_name, step_data in gui_data["step_results"].items():
            report_lines.append(f"ğŸ”§ {step_name}:")
            report_lines.append(
                f"  â€¢ Packets Processed: {step_data.get('packets_processed', 0):,}"
            )
            report_lines.append(
                f"  â€¢ Packets Modified: {step_data.get('packets_modified', 0):,}"
            )
            report_lines.append(
                f"  â€¢ Duration: {step_data.get('duration_ms', 0.0):.1f} ms"
            )
            if "summary" in step_data:
                report_lines.append(f"  â€¢ Summary: {step_data['summary']}")
            report_lines.append("")

    # è¾“å‡ºä¿¡æ¯
    if gui_data["output_directory"]:
        report_lines.append(f"ğŸ“ Output: {gui_data['output_directory']}")
        report_lines.append("")

    # çŠ¶æ€
    status = "âœ… Success" if gui_data["success"] else "âŒ Failed"
    report_lines.append(f"Status: {status}")

    return "\n".join(report_lines)
