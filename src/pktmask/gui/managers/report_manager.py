#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Report Manager - Responsible for report generation and display
"""

import os
from typing import TYPE_CHECKING, Dict, Any, Optional, List
from datetime import datetime

if TYPE_CHECKING:
    from ..main_window import MainWindow

from pktmask.infrastructure.logging import get_logger

class ReportManager:
    """Report Manager - Responsible for report generation and display"""

    def __init__(self, main_window: 'MainWindow'):
        self.main_window = main_window
        self.config = main_window.config
        self._logger = get_logger(__name__)

    def update_log(self, message: str):
        """Update log display"""
        try:
            timestamp = datetime.now().strftime("%H:%M:%S")
            formatted_message = f"[{timestamp}] {message}"

            # Add to log text area
            self.main_window.log_text.append(formatted_message)

            # Auto-scroll to bottom
            cursor = self.main_window.log_text.textCursor()
            cursor.movePosition(cursor.MoveOperation.End)
            self.main_window.log_text.setTextCursor(cursor)

            self._logger.debug(f"UI log updated: {message}")

        except Exception as e:
            self._logger.error(f"Error occurred while updating log display: {e}")

    def generate_partial_summary_on_stop(self):
        """Generate partial summary statistics when user stops processing"""
        separator_length = 70
        
        # Calculate current time
        if self.main_window.timer:
            self.main_window.timer.stop()

        # Stop statistics manager timing
        if hasattr(self.main_window, 'pipeline_manager') and hasattr(self.main_window.pipeline_manager, 'statistics'):
            self.main_window.pipeline_manager.statistics.stop_timing()
            
        self.main_window.update_time_elapsed()
        
        partial_time = self.main_window.time_elapsed_label.text()
        partial_files = self.main_window.files_processed_count
        partial_packets = self.main_window.packets_processed_count
        
        # Generate stop summary report
        stop_report = f"\n{'='*separator_length}\n⏹️ PROCESSING STOPPED BY USER\n{'='*separator_length}\n"
        stop_report += f"📊 Partial Statistics (Completed Portion):\n"
        stop_report += f"   • Files Processed: {partial_files}\n"
        stop_report += f"   • Packets Processed: {partial_packets:,}\n"
        stop_report += f"   • Processing Time: {partial_time}\n"

        # Calculate partial processing speed
        try:
            time_parts = partial_time.split(':')
            if len(time_parts) >= 2:
                minutes = int(time_parts[-2])
                seconds_with_ms = time_parts[-1].split('.')
                seconds = int(seconds_with_ms[0])
                total_seconds = minutes * 60 + seconds
                if total_seconds > 0 and partial_packets > 0:
                    speed = partial_packets / total_seconds
                    stop_report += f"   • Average Speed: {speed:,.0f} packets/second\n\n"
                else:
                    stop_report += f"   • Average Speed: N/A\n\n"
            else:
                stop_report += f"   • Average Speed: N/A\n\n"
        except:
            stop_report += f"   • Average Speed: N/A\n\n"

        # Display enabled processing steps
        enabled_steps = []
        if self.main_window.anonymize_ips_cb.isChecked():
            enabled_steps.append("IP Anonymization")
        if self.main_window.remove_dupes_cb.isChecked():
            enabled_steps.append("Deduplication")
        if self.main_window.mask_payloads_cb.isChecked():
            enabled_steps.append("Payload Masking")

        stop_report += f"🔧 Configured Processing Steps: {', '.join(enabled_steps)}\n"
        stop_report += f"📁 Working Directory: {os.path.basename(self.main_window.base_dir) if self.main_window.base_dir else 'N/A'}\n"
        stop_report += f"⚠️ Processing was interrupted. All intermediate files have been cleaned up.\n"
        stop_report += f"❌ No completed output files were generated due to interruption.\n"
        stop_report += f"{'='*separator_length}\n"

        self.main_window.summary_text.append(stop_report)

        # Check and display file processing status
        if self.main_window.file_processing_results:
            files_status_report = self._generate_files_status_report(separator_length)
            self.main_window.summary_text.append(files_status_report)

        # Display global IP mapping summary (only when there are fully completed files)
        if self.main_window.processed_files_count >= 1 and self.main_window.global_ip_mappings:
            global_partial_report = self._generate_global_ip_mappings_report(separator_length, True)
            if global_partial_report:
                self.main_window.summary_text.append(global_partial_report)
        
        # Display Enhanced Masking intelligent processing statistics (if any)
        enhanced_partial_report = self._generate_enhanced_masking_report(separator_length, is_partial=True)
        if enhanced_partial_report:
            self.main_window.summary_text.append(enhanced_partial_report)

        # Corrected restart hint
        restart_hint = f"\n💡 RESTART INFORMATION:\n"
        restart_hint += f"   • Clicking 'Start' will restart processing from the beginning\n"
        restart_hint += f"   • All files will be reprocessed (no partial resume capability)\n"
        restart_hint += f"   • Any existing output files will be skipped to avoid overwriting\n"
        restart_hint += f"   • Processing will be performed completely for each file\n"
        self.main_window.summary_text.append(restart_hint)
    
    def _generate_files_status_report(self, separator_length: int) -> str:
        """Generate file processing status report"""
        files_status_report = f"\n{'='*separator_length}\n📋 FILES PROCESSING STATUS (At Stop)\n{'='*separator_length}\n"
        
        completed_files = 0
        partial_files = 0
        
        for filename, file_result in self.main_window.file_processing_results.items():
            steps_data = file_result['steps']
            if not steps_data:
                continue
            
            # Check if file is fully processed (all configured steps completed)
            expected_steps = set()
            if self.main_window.anonymize_ips_cb.isChecked():
                expected_steps.add("IP Anonymization")
            if self.main_window.remove_dupes_cb.isChecked():
                expected_steps.add("Deduplication")
            if self.main_window.mask_payloads_cb.isChecked():
                expected_steps.add("Payload Masking")
            
            completed_steps = set(steps_data.keys())
            is_fully_completed = expected_steps.issubset(completed_steps)
            
            if is_fully_completed:
                completed_files += 1
                files_status_report += self._generate_completed_file_report(filename, steps_data)
            else:
                partial_files += 1
                files_status_report += self._generate_partial_file_report(filename, completed_steps, expected_steps)
        
        if completed_files == 0 and partial_files > 0:
            files_status_report += f"\n⚠️ All files were only partially processed.\n"
            files_status_report += f"   No final output files were created.\n"
        elif completed_files > 0:
            files_status_report += f"\n📈 Summary: {completed_files} completed, {partial_files} partial\n"
        
        files_status_report += f"{'='*separator_length}\n"
        return files_status_report
    
    def _generate_completed_file_report(self, filename: str, steps_data: Dict) -> str:
        """Generate report for completed file"""
        report = f"\n✅ {filename}\n"
        report += f"   Status: FULLY COMPLETED\n"
        
        # Get final output filename
        step_order = ['Deduplication', 'IP Anonymization', 'Payload Masking']
        final_output = None
        for step_name in reversed(step_order):
            if step_name in steps_data:
                output_file = steps_data[step_name]['data'].get('output_filename')
                if output_file and not output_file.startswith('tmp'):
                    final_output = output_file
                    break
        
        if final_output:
            report += f"   Output File: {final_output}\n"
        
        # Display detailed results
        original_packets = 0
        file_ip_mappings = {}

        # Prioritize getting original packet count from Deduplication step
        if 'Deduplication' in steps_data:
            original_packets = steps_data['Deduplication']['data'].get('total_packets', 0)
        elif 'IP Anonymization' in steps_data:
            original_packets = steps_data['IP Anonymization']['data'].get('total_packets', 0)
        elif 'Payload Masking' in steps_data:
            original_packets = steps_data['Payload Masking']['data'].get('total_packets', 0)

        for step_name in step_order:
            if step_name in steps_data:
                data = steps_data[step_name]['data']

                if step_name == 'IP Anonymization':
                    # Support new AnonStage field names (retrieved from extra_metrics)
                    extra_metrics = data.get('extra_metrics', {})
                    original_ips = data.get('original_ips', extra_metrics.get('original_ips', 0))
                    masked_ips = data.get('anonymized_ips', extra_metrics.get('anonymized_ips', 0))
                    rate = (masked_ips / original_ips * 100) if original_ips > 0 else 0
                    report += f"   🛡️  IP Anonymization: {original_ips} → {masked_ips} IPs ({rate:.1f}%)\n"
                    file_ip_mappings = data.get('file_ip_mappings', extra_metrics.get('file_ip_mappings', {}))

                elif step_name == 'Deduplication':
                    unique = data.get('unique_packets', 0)
                    removed = data.get('removed_count', 0)
                    rate = (removed / original_packets * 100) if original_packets > 0 else 0
                    report += f"   🔄 Deduplication: {removed} removed ({rate:.1f}%)\n"

                elif step_name == 'Payload Masking':
                    # Support new MaskPayloadStage field names
                    masked = data.get('packets_modified', data.get('masked_packets', 0))
                    rate = (masked / original_packets * 100) if original_packets > 0 else 0
                    report += f"   ✂️  Payload Masking: {masked} masked ({rate:.1f}%)\n"
        
        # Display IP mappings (if any)
        if file_ip_mappings:
            report += f"   🔗 IP Mappings ({len(file_ip_mappings)}):\n"
            for i, (orig_ip, new_ip) in enumerate(sorted(file_ip_mappings.items()), 1):
                if i <= 5:  # Only display first 5
                    report += f"      {i}. {orig_ip} → {new_ip}\n"
                elif i == 6:
                    report += f"      ... and {len(file_ip_mappings) - 5} more\n"
                    break
        
        return report
    
    def _generate_partial_file_report(self, filename: str, completed_steps: set, expected_steps: set) -> str:
        """Generate report for partially completed file"""
        report = f"\n🔄 {filename}\n"
        report += f"   Status: PARTIALLY PROCESSED (Interrupted)\n"
        report += f"   Completed Steps: {', '.join(completed_steps)}\n"
        report += f"   Missing Steps: {', '.join(expected_steps - completed_steps)}\n"
        report += f"   ❌ No final output file generated\n"
        report += f"   🗑️ Temporary files cleaned up automatically\n"
        return report

    def _generate_global_ip_mappings_report(self, separator_length: int, is_partial: bool = False) -> Optional[str]:
        """Generate global IP mapping report"""
        # First check if IP anonymization processing is enabled
        if not self.main_window.anonymize_ips_cb.isChecked():
            return None

        # Check if global IP mapping data exists
        if not self.main_window.global_ip_mappings:
            return None

        # Check if there are fully completed files
        has_completed_files = False
        for filename, file_result in self.main_window.file_processing_results.items():
            expected_steps = set()
            if self.main_window.anonymize_ips_cb.isChecked():
                expected_steps.add("IP Anonymization")
            if self.main_window.remove_dupes_cb.isChecked():
                expected_steps.add("Deduplication")
            if self.main_window.mask_payloads_cb.isChecked():
                expected_steps.add("Payload Masking")

            completed_steps = set(file_result['steps'].keys())
            if expected_steps.issubset(completed_steps):
                has_completed_files = True
                break
        
        if not has_completed_files:
            return None
        
        if is_partial:
            title = "🌐 IP MAPPINGS FROM COMPLETED FILES"
            subtitle = "📝 IP Mapping Table - From Successfully Completed Files Only:"
        else:
            title = "🌐 GLOBAL IP MAPPINGS (All Files Combined)"
            subtitle = "📝 Complete IP Mapping Table - Unique Entries Across All Files:"
        
        global_partial_report = f"\n{'='*separator_length}\n{title}\n{'='*separator_length}\n"
        global_partial_report += f"{subtitle}\n"
        global_partial_report += f"   • Total Unique IPs Mapped: {len(self.main_window.global_ip_mappings)}\n\n"
        
        sorted_global_mappings = sorted(self.main_window.global_ip_mappings.items())
        for i, (orig_ip, new_ip) in enumerate(sorted_global_mappings, 1):
            global_partial_report += f"   {i:2d}. {orig_ip:<16} → {new_ip}\n"
        
        if is_partial:
            global_partial_report += f"\n✅ All unique IP addresses across files have been\n"
            global_partial_report += f"   successfully anonymized with consistent mappings.\n"
        else:
            global_partial_report += f"\n✅ All unique IP addresses across {self.main_window.processed_files_count} files have been\n"
            global_partial_report += f"   successfully anonymized with consistent mappings.\n"
        
        global_partial_report += f"{'='*separator_length}\n"
        return global_partial_report
    
    def generate_file_complete_report(self, original_filename: str):
        """Generate complete processing report for a single file"""
        if original_filename not in self.main_window.file_processing_results:
            return

        file_results = self.main_window.file_processing_results[original_filename]
        steps_data = file_results['steps']

        if not steps_data:
            return

        # **Fix**: Remove duplicate file count increment (already counted in main_window.py FILE_END event)
        # self.main_window.processed_files_count += 1  # Remove this line to avoid double counting

        separator_length = 70
        filename_display = original_filename

        # File processing title
        header = f"\n{'='*separator_length}\n📄 FILE PROCESSING RESULTS: {filename_display}\n{'='*separator_length}"
        self.main_window.summary_text.append(header)

        # Get original packet count (prioritize from Deduplication step as it contains the true original packet count)
        original_packets = 0
        output_filename = None
        if 'Deduplication' in steps_data:
            # Deduplication step's total_packets is the true original packet count
            original_packets = steps_data['Deduplication']['data'].get('total_packets', 0)
            output_filename = steps_data['Deduplication']['data'].get('output_filename')
        elif 'IP Anonymization' in steps_data:
            # If no deduplication step, get from IP anonymization step
            original_packets = steps_data['IP Anonymization']['data'].get('total_packets', 0)
            output_filename = steps_data['IP Anonymization']['data'].get('output_filename')
        elif 'Payload Masking' in steps_data:
            # Finally get from payload masking step
            original_packets = steps_data['Payload Masking']['data'].get('total_packets', 0)
            output_filename = steps_data['Payload Masking']['data'].get('output_filename')
        
        # Get final output filename from the last processing step
        step_order = ['Deduplication', 'IP Anonymization', 'Payload Masking']
        for step_name in reversed(step_order):
            if step_name in steps_data:
                final_output = steps_data[step_name]['data'].get('output_filename')
                if final_output:
                    output_filename = final_output
                    break
        
        # Display original packet count and output filename
        self.main_window.summary_text.append(f"📦 Original Packets: {original_packets:,}")
        if output_filename:
            self.main_window.summary_text.append(f"📄 Output File: {output_filename}")
        self.main_window.summary_text.append("")

        # Display step results in processing order
        file_ip_mappings = {}  # Store current file's IP mappings

        # **Debug log**: Display collected step data
        self._logger.info(f"🔍 Generating file report: {original_filename}")
        self._logger.info(f"🔍 Collected steps: {list(steps_data.keys())}")
        for step_name, step_info in steps_data.items():
            self._logger.info(f"🔍   {step_name}: type={step_info.get('type')}, data_fields={list(step_info.get('data', {}).keys())}")

        # Fix: Get IP mapping information from file-level IP mapping cache
        if hasattr(self.main_window, '_current_file_ips') and original_filename in self.main_window._current_file_ips:
            file_ip_mappings = self.main_window._current_file_ips[original_filename]

        for step_name in step_order:
            if step_name in steps_data:
                step_result = steps_data[step_name]
                step_type = step_result['type']
                data = step_result['data']

                self._logger.info(f"🔍 Processing step {step_name}: type={step_type}")

                # For Payload Masking, record detailed data fields
                if step_name == 'Payload Masking':
                    self._logger.info(f"🔍 Payload Masking data: packets_processed={data.get('packets_processed')}, packets_modified={data.get('packets_modified')}")
                    self._logger.info(f"🔍 Payload Masking data: total_packets={data.get('total_packets')}, masked_packets={data.get('masked_packets')}")
                
                if step_type in ['anonymize_ips', 'mask_ip', 'mask_ips']:  # Support standard naming and legacy naming
                    # Use new IP statistics data
                    original_ips = data.get('original_ips', 0)
                    masked_ips = data.get('anonymized_ips', 0)
                    rate = (masked_ips / original_ips * 100) if original_ips > 0 else 0
                    line = f"  🛡️  {step_name:<18} | Original IPs: {original_ips:>3} | Anonymized IPs: {masked_ips:>3} | Rate: {rate:5.1f}%"

                    # IP mappings already retrieved from cache above

                elif step_type == 'remove_dupes':
                    unique = data.get('unique_packets', 0)
                    removed = data.get('removed_count', 0)
                    total_before = data.get('total_packets', 0)
                    rate = (removed / total_before * 100) if total_before > 0 else 0
                    line = f"  🔄 {step_name:<18} | Unique Pkts: {unique:>4} | Removed Pkts: {removed:>4} | Rate: {rate:5.1f}%"

                elif step_type in ['mask_payloads']:  # Use standard naming
                    # Fix: MaskStage returns different field names
                    total = data.get('total_packets', data.get('packets_processed', 0))
                    masked = data.get('masked_packets', data.get('packets_modified', 0))
                    rate = (masked / total * 100) if total > 0 else 0

                    # Check if this is Enhanced Masking intelligent processing result
                    if self._is_enhanced_masking(data):
                        line = self._generate_enhanced_masking_report_line(step_name, data)
                    else:
                        line = f"  ✂️  {step_name:<18} | Total Pkts: {total:>5} | Masked Pkts: {masked:>4} | Rate: {rate:5.1f}%"
                else:
                    continue

                self.main_window.summary_text.append(line)

        # If IP mappings exist, display file-level IP mappings
        if file_ip_mappings:
            self.main_window.summary_text.append("")
            self.main_window.summary_text.append("🔗 IP Mappings for this file:")
            sorted_mappings = sorted(file_ip_mappings.items())
            for i, (orig_ip, new_ip) in enumerate(sorted_mappings, 1):
                self.main_window.summary_text.append(f"   {i:2d}. {orig_ip:<16} → {new_ip}")
        
        # If Enhanced Masking was used, display intelligent processing details
        enhanced_report = self._generate_enhanced_masking_report_for_file(original_filename, separator_length)
        if enhanced_report:
            self.main_window.summary_text.append(enhanced_report)

        self.main_window.summary_text.append(f"{'='*separator_length}")

    def generate_processing_finished_report(self):
        """Generate report when processing is complete"""
        separator_length = 70  # Maintain consistent separator length

        # **Fix**: Save current statistics data before stopping timer and resetting statistics
        # Ensure Live Dashboard display data is not affected by reset
        current_files_processed = self.main_window.files_processed_count
        current_packets_processed = self.main_window.packets_processed_count
        current_time_elapsed = self.main_window.time_elapsed_label.text()

        # Stop timer
        if self.main_window.timer and self.main_window.timer.isActive():
            self.main_window.timer.stop()

        # Stop statistics manager timing
        if hasattr(self.main_window, 'pipeline_manager') and hasattr(self.main_window.pipeline_manager, 'statistics'):
            self.main_window.pipeline_manager.statistics.stop_timing()

        self.main_window.update_time_elapsed()

        enabled_steps = []
        if self.main_window.remove_dupes_cb.isChecked():
            enabled_steps.append("Deduplication")
        if self.main_window.anonymize_ips_cb.isChecked():
            enabled_steps.append("IP Anonymization")
        if self.main_window.mask_payloads_cb.isChecked():
            enabled_steps.append("Payload Masking")

        completion_report = f"\n{'='*separator_length}\n🎉 PROCESSING COMPLETED!\n{'='*separator_length}\n"
        completion_report += f"🎯 All {current_files_processed} files have been successfully processed.\n"
        completion_report += f"📈 Files Processed: {current_files_processed}\n"
        completion_report += f"📊 Total Packets Processed: {current_packets_processed}\n"
        completion_report += f"⏱️ Time Elapsed: {current_time_elapsed}\n"
        completion_report += f"🔧 Applied Processing Steps: {', '.join(enabled_steps)}\n"
        
        # Safely handle output directory display
        if self.main_window.current_output_dir:
            completion_report += f"📁 Output Location: {os.path.basename(self.main_window.current_output_dir)}\n"
        else:
            completion_report += f"📁 Output Location: Not specified\n"

        completion_report += f"📝 All processed files saved to output directory.\n"
        completion_report += f"{'='*separator_length}\n"

        self.main_window.summary_text.append(completion_report)

        # Fix: Add global IP mapping summary report (display deduplicated global IP mappings for multi-file processing)
        global_ip_report = self._generate_global_ip_mappings_report(separator_length, is_partial=False)
        if global_ip_report:
            self.main_window.summary_text.append(global_ip_report)

        # Add Enhanced Masking intelligent processing total report
        enhanced_masking_report = self._generate_enhanced_masking_report(separator_length, is_partial=False)
        if enhanced_masking_report:
            self.main_window.summary_text.append(enhanced_masking_report)

        # Fix: Automatically save Summary Report to output directory after processing completion
        self._save_summary_report_to_output()

    def set_final_summary_report(self, report: dict):
        """Set final summary report, including detailed IP mapping information."""
        subdir = report.get('path', 'N/A')
        stats = report.get('stats', {})
        total_mapping = report.get('data', {}).get('total_mapping', {})
        
        separator_length = 70  # Maintain consistent separator length

        # Add IP mapping summary information, including detailed mapping table
        text = f"\n{'='*separator_length}\n📋 DIRECTORY PROCESSING SUMMARY\n{'='*separator_length}\n"
        text += f"📂 Directory: {subdir}\n\n"
        text += f"🔒 IP Anonymization Summary:\n"
        text += f"   • Total Unique IPs Discovered: {stats.get('total_unique_ips', 'N/A')}\n"
        text += f"   • Total IPs Anonymized: {stats.get('total_mapped_ips', 'N/A')}\n\n"
        
        if total_mapping:
            text += f"📝 Complete IP Mapping Table (All Files):\n"
            # Display mappings sorted by original IP
            sorted_mappings = sorted(total_mapping.items())
            for i, (orig_ip, new_ip) in enumerate(sorted_mappings, 1):
                text += f"   {i:2d}. {orig_ip:<16} → {new_ip}\n"
            text += "\n"

        text += f"✅ All IP addresses have been successfully anonymized while\n"
        text += f"   preserving network structure and subnet relationships.\n"
        text += f"{'='*separator_length}\n"

        self.main_window.summary_text.append(text)

    def update_summary_report(self, data: Dict[str, Any]):
        """Update summary report display"""
        try:
            # Generate different reports based on data type
            if 'filename' in data:
                # Single file processing report
                self._update_file_summary(data)
            elif 'step_results' in data:
                # Overall processing summary
                self._update_overall_summary(data)
            else:
                step_type = data.get('type')
                if step_type and step_type.endswith('_final'):
                    report_data = data.get('report')
                    if report_data and 'mask_ip' in step_type:
                        self.set_final_summary_report(report_data)
                else:
                    self._logger.warning(f"Unknown summary report data format: {data.keys()}")

        except Exception as e:
            self._logger.error(f"Error occurred while updating summary report: {e}")

    def _update_file_summary(self, data: Dict[str, Any]):
        """Update single file processing summary"""
        filename = data.get('filename', 'Unknown file')

        # Get current summary text
        current_text = self.main_window.summary_text.toPlainText()

        # Generate file summary
        file_summary = self._generate_file_summary_text(data)

        # Append to existing text
        if current_text.strip():
            updated_text = current_text + "\n\n" + file_summary
        else:
            updated_text = file_summary

        self.main_window.summary_text.setPlainText(updated_text)

        # Scroll to bottom
        cursor = self.main_window.summary_text.textCursor()
        cursor.movePosition(cursor.MoveOperation.End)
        self.main_window.summary_text.setTextCursor(cursor)
    
    def _update_overall_summary(self, data: Dict[str, Any]):
        """Update overall processing summary"""
        summary_text = self._generate_overall_summary_text(data)
        self.main_window.summary_text.setPlainText(summary_text)
    
    def _generate_file_summary_text(self, data: Dict[str, Any]) -> str:
        """Generate single file summary text"""
        filename = data.get('filename', 'Unknown file')
        summary_parts = [f"📄 {filename}"]
        
        # Processing result statistics
        results = data.get('results', {})
        for step_name, result in results.items():
            if isinstance(result, dict):
                if 'summary' in result:
                    summary_parts.append(f"  • {step_name}: {result['summary']}")
                elif 'packets_processed' in result:
                    summary_parts.append(f"  • {step_name}: {result['packets_processed']} packets")
                elif 'ips_anonymized' in result:
                    summary_parts.append(f"  • {step_name}: {result['ips_anonymized']} IPs anonymized")
        
        return '\n'.join(summary_parts)
    
    def _generate_overall_summary_text(self, data: Dict[str, Any]) -> str:
        """生成整体摘要文本"""
        summary_parts = ["📊 Processing Summary", "=" * 50]
        
        # 基本统计
        files_processed = data.get('files_processed', 0)
        total_files = data.get('total_files', 0)
        status = data.get('status', 'unknown')
        
        summary_parts.append(f"Status: {status.title().replace('_', ' ')}")
        summary_parts.append(f"Files Processed: {files_processed}/{total_files}")
        
        if files_processed > 0 and total_files > 0:
            percentage = (files_processed / total_files) * 100
            summary_parts.append(f"Completion: {percentage:.1f}%")
        
        summary_parts.append("")
        
        # 步骤统计
        step_results = data.get('step_results', {})
        if step_results:
            summary_parts.append("📈 Step Statistics:")
            
            # 聚合各步骤的统计
            step_stats = self._aggregate_step_statistics(step_results)
            
            for step_name, stats in step_stats.items():
                summary_parts.append(f"\n🔧 {step_name}:")
                for key, value in stats.items():
                    summary_parts.append(f"  • {key}: {value}")
        
        # 输出目录信息
        output_dir = data.get('output_directory')
        if output_dir:
            summary_parts.append(f"\n📁 Output Directory:")
            summary_parts.append(f"  {output_dir}")
        
        # 时间戳
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        summary_parts.append(f"\n⏰ Generated: {timestamp}")
        
        return '\n'.join(summary_parts)
    
    def _generate_final_summary_text(self, data: Dict[str, Any]) -> str:
        """生成最终摘要文本"""
        summary_parts = ["🎯 Final Processing Report", "=" * 60]
        
        # 处理状态
        status = data.get('status', 'unknown')
        files_processed = data.get('files_processed', 0)
        total_files = data.get('total_files', 0)
        
        if status == 'completed':
            summary_parts.append("✅ Status: Successfully Completed")
        elif status == 'stopped_by_user':
            summary_parts.append("⏹️ Status: Stopped by User")
        else:
            summary_parts.append(f"❓ Status: {status.title()}")
        
        summary_parts.append(f"📊 Files Processed: {files_processed} of {total_files}")
        
        if files_processed > 0 and total_files > 0:
            percentage = (files_processed / total_files) * 100
            summary_parts.append(f"📈 Completion Rate: {percentage:.1f}%")
        
        summary_parts.append("")
        
        # 详细统计
        step_results = data.get('step_results', {})
        if step_results:
            summary_parts.append("📋 Detailed Statistics:")
            summary_parts.append("-" * 40)
            
            # 聚合统计
            aggregated_stats = self._aggregate_step_statistics(step_results)
            
            for step_name, stats in aggregated_stats.items():
                summary_parts.append(f"\n🔧 {step_name}:")
                for stat_name, stat_value in stats.items():
                    summary_parts.append(f"  • {stat_name}: {stat_value}")
        
        # 文件详情
        if step_results:
            summary_parts.append("\n📄 File Details:")
            summary_parts.append("-" * 30)
            
            for filename, file_results in step_results.items():
                summary_parts.append(f"\n📁 {filename}:")
                for step_name, result in file_results.items():
                    if isinstance(result, dict):
                        if 'summary' in result:
                            summary_parts.append(f"  • {step_name}: {result['summary']}")
                        else:
                            summary_parts.append(f"  • {step_name}: Processed")
        
        # 输出信息
        output_dir = data.get('output_directory')
        if output_dir:
            summary_parts.append(f"\n📂 Output Location:")
            summary_parts.append(f"  {output_dir}")
        
        # 生成时间
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        summary_parts.append(f"\n🕒 Report Generated: {timestamp}")
        
        # 工具信息
        summary_parts.append("\n" + "=" * 60)
        summary_parts.append("🛠️ Generated by PktMask - Network Packet Processing Tool")
        
        return '\n'.join(summary_parts)
    
    def _aggregate_step_statistics(self, step_results: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """聚合步骤统计信息"""
        aggregated = {}
        
        for filename, file_results in step_results.items():
            for step_name, result in file_results.items():
                if step_name not in aggregated:
                    aggregated[step_name] = {}
                
                if isinstance(result, dict):
                    for key, value in result.items():
                        if isinstance(value, (int, float)):
                            # 数值类型进行累加
                            if key in aggregated[step_name]:
                                aggregated[step_name][key] += value
                            else:
                                aggregated[step_name][key] = value
                        elif key == 'summary' and isinstance(value, str):
                            # 摘要信息收集到列表
                            summary_key = 'summaries'
                            if summary_key not in aggregated[step_name]:
                                aggregated[step_name][summary_key] = []
                            aggregated[step_name][summary_key].append(f"{filename}: {value}")
        
        # 后处理：计算平均值、格式化显示等
        for step_name, stats in aggregated.items():
            if 'summaries' in stats:
                # 将摘要列表转换为计数
                summaries = stats.pop('summaries')
                stats['files_processed'] = len(summaries)
        
        return aggregated
    
    def clear_displays(self):
        """清空显示区域"""
        try:
            self.main_window.log_text.clear()
            self.main_window.summary_text.clear()
            self._logger.debug("Cleared log and summary display")
            
        except Exception as e:
            self._logger.error(f"Error occurred while clearing display area: {e}")
    
    def export_summary_report(self, filepath: str) -> bool:
        """导出摘要报告到文件"""
        try:
            content = self.main_window.summary_text.toPlainText()
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            
            self._logger.info(f"Summary report exported to: {filepath}")
            return True
            
        except Exception as e:
            self._logger.error(f"Failed to export summary report: {e}")
            return False
    
    def _save_summary_report_to_output(self):
        """私有方法：保存摘要报告到输出目录"""
        try:
            # 委托给FileManager或使用MainWindow的现有方法
            if hasattr(self.main_window, 'save_summary_report_to_output_dir'):
                self.main_window.save_summary_report_to_output_dir()
            elif hasattr(self.main_window, 'file_manager'):
                self.main_window.file_manager.save_summary_report_to_output_dir()
            else:
                self._logger.warning("Cannot find method to save Summary Report")
        except Exception as e:
            self._logger.error(f"Failed to save Summary Report to output directory: {e}")

    def collect_step_result(self, data: dict):
        """收集每个步骤的处理结果，但不立即显示"""
        if not self.main_window.current_processing_file:
            return
            
        step_type = data.get('type')
        step_name_raw = data.get('step_name', '')
        
        # **调试日志**: 记录收集的步骤结果
        self._logger.info(f"🔍 Collecting step results: file={self.main_window.current_processing_file}, step={step_name_raw}, type={step_type}")
        self._logger.info(f"🔍 Data fields: {list(data.keys())}")
        
        # **修复**: 支持新Pipeline系统的步骤名称
        # 从step_name推断步骤类型，而不是仅依赖type字段
        if not step_type:
            # 新Pipeline系统没有type字段，从step_name推断
            if step_name_raw == 'AnonStage':
                step_type = 'anonymize_ips'  # Use standard naming
            elif step_name_raw in ['DedupStage', 'DeduplicationStage']:
                step_type = 'remove_dupes'
            elif step_name_raw in ['MaskStage', 'MaskPayloadStage', 'NewMaskPayloadStage', 'Mask Payloads (v2)', 'Payload Masking Stage']:
                step_type = 'mask_payloads'  # Use standard naming
            else:
                step_type = step_name_raw.lower()
        
        self._logger.info(f"🔍 Inferred step type: {step_type}")
        
        if not step_type or step_type.endswith('_final'):
            if step_type and step_type.endswith('_final'):
                # 处理最终报告，提取IP映射信息
                report_data = data.get('report')
                if report_data and 'anonymize_ips' in step_type:
                    self.set_final_summary_report(report_data)
            return
        
        # 标准化步骤名称 - 修复Pipeline和ReportManager之间的映射不匹配
        step_display_names = {
            'anonymize_ips': 'IP Anonymization',  # Standard naming
            'remove_dupes': 'Deduplication',
            'mask_payloads': 'Payload Masking',   # Standard naming
        }
        
        step_name = step_display_names.get(step_type, step_type)
        
        # 存储步骤结果
        self.main_window.file_processing_results[self.main_window.current_processing_file]['steps'][step_name] = {
            'type': step_type,
            'data': data
        }
        
        # **关键修复**: 如果是IP匿名化步骤，提取并累积IP映射到全局映射
        is_ip_anonymization = (
            step_type in ['anonymize_ips'] or
            step_name_raw == 'AnonStage' or
            'ip_mappings' in data or
            'file_ip_mappings' in data
        )
        
        if is_ip_anonymization:
            # 从step数据中提取IP映射
            ip_mappings = None
            if 'file_ip_mappings' in data:
                ip_mappings = data['file_ip_mappings']
            elif 'ip_mappings' in data:
                ip_mappings = data['ip_mappings']
            elif 'extra_metrics' in data:
                # 检查extra_metrics中的IP映射（新Pipeline系统）
                extra_metrics = data['extra_metrics']
                if 'file_ip_mappings' in extra_metrics:
                    ip_mappings = extra_metrics['file_ip_mappings']
                elif 'ip_mappings' in extra_metrics:
                    ip_mappings = extra_metrics['ip_mappings']
            
            if ip_mappings and isinstance(ip_mappings, dict):
                # 保存文件级IP映射
                if not hasattr(self.main_window, '_current_file_ips'):
                    self.main_window._current_file_ips = {}
                self.main_window._current_file_ips[self.main_window.current_processing_file] = ip_mappings
                
                # **关键修复**: 将当前文件的IP映射累积到全局映射中（不覆盖）
                if not hasattr(self.main_window, 'global_ip_mappings') or self.main_window.global_ip_mappings is None:
                    self.main_window.global_ip_mappings = {}
                
                # 累积映射而不是覆盖
                self.main_window.global_ip_mappings.update(ip_mappings)

                self._logger.info(f"✅ Collected IP mappings: file={self.main_window.current_processing_file}, new_mappings={len(ip_mappings)}, global_total={len(self.main_window.global_ip_mappings)}")
            else:
                self._logger.warning(f"IP anonymization step completed, but no valid IP mapping data found: {list(data.keys())}")
        else:
            self._logger.debug(f"Non-IP anonymization step: {step_name_raw}")

    def _is_enhanced_masking(self, data: Dict[str, Any]) -> bool:
        """检查是否是增强掩码处理结果 - 基于双模块架构"""
        step_name = data.get('step_name', '')

        # 检查Enhanced Masking特有的字段组合 - 必须是真正的Enhanced Intelligent Mode
        enhanced_indicators = [
            'processing_mode' in data and data.get('processing_mode') == 'Enhanced Intelligent Mode',
            'protocol_stats' in data,
            'strategies_applied' in data,
            'enhancement_level' in data,
        ]

        # Protocol adaptation mode has different processing mode identifiers, not 'Enhanced Intelligent Mode'
        # 如果有真正的Enhanced Masking特有字段组合，认为是智能处理
        return all(enhanced_indicators[:3])  # 前3个字段必须都存在

    def _generate_enhanced_masking_report_line(self, step_name: str, data: Dict[str, Any]) -> str:
        """生成Enhanced Masking的处理结果报告行（移除HTTP统计）"""
        total = data.get('total_packets', 0)
        masked = data.get('masked_packets', 0)

        # 获取协议统计（移除HTTP）
        protocol_stats = data.get('protocol_stats', {})
        tls_packets = protocol_stats.get('tls_packets', 0)
        other_packets = protocol_stats.get('other_packets', 0)

        # 基础报告行（增强模式标识）
        rate = (masked / total * 100) if total > 0 else 0
        line = f"  ✂️  {step_name:<18} | Enhanced Mode | Total: {total:>4} | Masked: {masked:>4} | Rate: {rate:5.1f}%"

        return line

    def _generate_enhanced_masking_report(self, separator_length: int, is_partial: bool = False) -> Optional[str]:
        """生成Enhanced Masking的智能处理统计总报告（移除HTTP支持）"""
        # 检查是否有Enhanced Masking处理的文件
        enhanced_files = []
        total_enhanced_stats = {
            'total_packets': 0,
            'tls_packets': 0,
            'other_packets': 0,
            'strategies_applied': set(),
            'files_processed': 0
        }

        # 遍历所有处理过的文件，找出使用Enhanced Masking的文件
        for filename, file_result in self.main_window.file_processing_results.items():
            steps_data = file_result.get('steps', {})
            payload_step = steps_data.get('Payload Masking')

            if payload_step and self._is_enhanced_masking(payload_step.get('data', {})):
                enhanced_files.append(filename)
                data = payload_step['data']
                
                # 汇总统计（移除HTTP）
                total_enhanced_stats['files_processed'] += 1
                total_enhanced_stats['total_packets'] += data.get('total_packets', 0)
                
                protocol_stats = data.get('protocol_stats', {})
                total_enhanced_stats['tls_packets'] += protocol_stats.get('tls_packets', 0)
                total_enhanced_stats['other_packets'] += protocol_stats.get('other_packets', 0)
                
                strategies = data.get('strategies_applied', [])
                total_enhanced_stats['strategies_applied'].update(strategies)
        
        # 如果没有Enhanced Masking处理，返回None
        if not enhanced_files:
            return None

        # 生成智能处理总报告
        title = "🧠 ENHANCED MASKING INTELLIGENCE REPORT"
        if is_partial:
            title += " (Partial)"

        report = f"\n{'='*separator_length}\n{title}\n{'='*separator_length}\n"

        # 处理模式和增强信息
        report += f"🎯 Processing Mode: Intelligent Auto-Detection\n"
        report += f"⚡ Enhancement Level: 4x accuracy improvement over simple masking\n"
        report += f"📁 Enhanced Files: {total_enhanced_stats['files_processed']}/{len(self.main_window.file_processing_results)}\n\n"
        
        # 协议检测统计（移除HTTP）
        total_packets = total_enhanced_stats['total_packets']
        if total_packets > 0:
            tls_rate = (total_enhanced_stats['tls_packets'] / total_packets) * 100
            other_rate = (total_enhanced_stats['other_packets'] / total_packets) * 100
            
            report += f"📊 Protocol Detection Results:\n"
            report += f"   • TLS packets: {total_enhanced_stats['tls_packets']:,} ({tls_rate:.1f}%) - Intelligent TLS strategy\n"
            report += f"   • Other packets: {total_enhanced_stats['other_packets']:,} ({other_rate:.1f}%) - General strategy\n"
            report += f"   • Total processed: {total_packets:,} packets in 4 stages\n\n"
        
        # 策略应用统计
        strategies_list = list(total_enhanced_stats['strategies_applied'])
        if strategies_list:
            report += f"🔧 Applied Strategies:\n"
            for strategy in sorted(strategies_list):
                report += f"   • {strategy}\n"
            report += "\n"
        
        # 智能处理优势说明（移除HTTP）
        report += f"🚀 Enhanced Processing Benefits:\n"
        report += f"   • Automatic protocol detection and strategy selection\n"
        report += f"   • TLS handshake preserved, ApplicationData masked\n"
        report += f"   • Improved accuracy while maintaining network analysis capability\n"
        
        report += f"{'='*separator_length}\n"
        
        return report

    def _generate_enhanced_masking_report_for_file(self, filename: str, separator_length: int) -> Optional[str]:
        """为单个文件生成Enhanced Masking的处理结果报告（移除HTTP统计）"""
        if filename not in self.main_window.file_processing_results:
            return None

        file_result = self.main_window.file_processing_results[filename]
        steps_data = file_result.get('steps', {})
        payload_step = steps_data.get('Payload Masking')

        if not payload_step or not self._is_enhanced_masking(payload_step.get('data', {})):
            return None

        data = payload_step['data']
        protocol_stats = data.get('protocol_stats', {})

        report = f"\n🧠 Enhanced Masking Details for {filename}:\n"
        report += f"   📊 Protocol Analysis:\n"
        
        total_packets = data.get('total_packets', 0)
        if total_packets > 0:
            tls_packets = protocol_stats.get('tls_packets', 0)
            other_packets = protocol_stats.get('other_packets', 0)
            
            if tls_packets > 0:
                tls_rate = (tls_packets / total_packets) * 100
                report += f"      • TLS: {tls_packets} packets ({tls_rate:.1f}%) - Handshake preserved\n"
            
            if other_packets > 0:
                other_rate = (other_packets / total_packets) * 100
                report += f"      • Other: {other_packets} packets ({other_rate:.1f}%) - Generic strategy\n"
        
        # 策略应用信息
        strategies = data.get('strategies_applied', [])
        if strategies:
            report += f"   🔧 Applied Strategies: {', '.join(strategies)}\n"
        
        # 处理效率
        enhancement_level = data.get('enhancement_level', 'Not specified')
        report += f"   ⚡ Enhancement: {enhancement_level}\n"
        
        return report

    def _generate_enhanced_masking_report_for_directory(self, separator_length: int) -> Optional[str]:
        """生成整个目录的Enhanced Masking的处理结果报告"""
        # 这个方法在目录级别处理完成时调用
        # 目前先返回通用的Enhanced报告
        return self._generate_enhanced_masking_report(separator_length, is_partial=False)