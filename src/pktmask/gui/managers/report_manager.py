#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Report Manager - Responsible for report generation and display
"""

import os
from typing import TYPE_CHECKING, Dict, Any, Optional, List
from datetime import datetime

if TYPE_CHECKING:
    from ..main_window import MainWindow

from pktmask.infrastructure.logging import get_logger

class ReportManager:
    """Report Manager - Responsible for report generation and display"""

    def __init__(self, main_window: 'MainWindow'):
        self.main_window = main_window
        self.config = main_window.config
        self._logger = get_logger(__name__)

    def update_log(self, message: str):
        """Update log display"""
        try:
            timestamp = datetime.now().strftime("%H:%M:%S")
            formatted_message = f"[{timestamp}] {message}"

            # Add to log text area
            self.main_window.log_text.append(formatted_message)

            # Auto-scroll to bottom
            cursor = self.main_window.log_text.textCursor()
            cursor.movePosition(cursor.MoveOperation.End)
            self.main_window.log_text.setTextCursor(cursor)

            self._logger.debug(f"UI log updated: {message}")

        except Exception as e:
            self._logger.error(f"Error occurred while updating log display: {e}")

    def generate_partial_summary_on_stop(self):
        """Generate partial summary statistics when user stops processing"""
        separator_length = 70
        
        # Calculate current time
        if self.main_window.timer:
            self.main_window.timer.stop()

        # Stop statistics manager timing
        if hasattr(self.main_window, 'pipeline_manager') and hasattr(self.main_window.pipeline_manager, 'statistics'):
            self.main_window.pipeline_manager.statistics.stop_timing()
            
        self.main_window.update_time_elapsed()
        
        partial_time = self.main_window.time_elapsed_label.text()
        partial_files = self.main_window.files_processed_count
        partial_packets = self.main_window.packets_processed_count
        
        # Generate stop summary report
        stop_report = f"\n{'='*separator_length}\nâ¹ï¸ PROCESSING STOPPED BY USER\n{'='*separator_length}\n"
        stop_report += f"ğŸ“Š Partial Statistics (Completed Portion):\n"
        stop_report += f"   â€¢ Files Processed: {partial_files}\n"
        stop_report += f"   â€¢ Packets Processed: {partial_packets:,}\n"
        stop_report += f"   â€¢ Processing Time: {partial_time}\n"

        # Calculate partial processing speed
        try:
            time_parts = partial_time.split(':')
            if len(time_parts) >= 2:
                minutes = int(time_parts[-2])
                seconds_with_ms = time_parts[-1].split('.')
                seconds = int(seconds_with_ms[0])
                total_seconds = minutes * 60 + seconds
                if total_seconds > 0 and partial_packets > 0:
                    speed = partial_packets / total_seconds
                    stop_report += f"   â€¢ Average Speed: {speed:,.0f} packets/second\n\n"
                else:
                    stop_report += f"   â€¢ Average Speed: N/A\n\n"
            else:
                stop_report += f"   â€¢ Average Speed: N/A\n\n"
        except:
            stop_report += f"   â€¢ Average Speed: N/A\n\n"

        # Display enabled processing steps
        enabled_steps = []
        if self.main_window.anonymize_ips_cb.isChecked():
            enabled_steps.append("IP Anonymization")
        if self.main_window.remove_dupes_cb.isChecked():
            enabled_steps.append("Deduplication")
        if self.main_window.mask_payloads_cb.isChecked():
            enabled_steps.append("Payload Masking")

        stop_report += f"ğŸ”§ Configured Processing Steps: {', '.join(enabled_steps)}\n"
        stop_report += f"ğŸ“ Working Directory: {os.path.basename(self.main_window.base_dir) if self.main_window.base_dir else 'N/A'}\n"
        stop_report += f"âš ï¸ Processing was interrupted. All intermediate files have been cleaned up.\n"
        stop_report += f"âŒ No completed output files were generated due to interruption.\n"
        stop_report += f"{'='*separator_length}\n"

        self.main_window.summary_text.append(stop_report)

        # Check and display file processing status
        if self.main_window.file_processing_results:
            files_status_report = self._generate_files_status_report(separator_length)
            self.main_window.summary_text.append(files_status_report)

        # Display global IP mapping summary (only when there are fully completed files)
        if self.main_window.processed_files_count >= 1 and self.main_window.global_ip_mappings:
            global_partial_report = self._generate_global_ip_mappings_report(separator_length, True)
            if global_partial_report:
                self.main_window.summary_text.append(global_partial_report)
        
        # Display Enhanced Masking intelligent processing statistics (if any)
        enhanced_partial_report = self._generate_enhanced_masking_report(separator_length, is_partial=True)
        if enhanced_partial_report:
            self.main_window.summary_text.append(enhanced_partial_report)

        # Corrected restart hint
        restart_hint = f"\nğŸ’¡ RESTART INFORMATION:\n"
        restart_hint += f"   â€¢ Clicking 'Start' will restart processing from the beginning\n"
        restart_hint += f"   â€¢ All files will be reprocessed (no partial resume capability)\n"
        restart_hint += f"   â€¢ Any existing output files will be skipped to avoid overwriting\n"
        restart_hint += f"   â€¢ Processing will be performed completely for each file\n"
        self.main_window.summary_text.append(restart_hint)
    
    def _generate_files_status_report(self, separator_length: int) -> str:
        """Generate file processing status report"""
        files_status_report = f"\n{'='*separator_length}\nğŸ“‹ FILES PROCESSING STATUS (At Stop)\n{'='*separator_length}\n"
        
        completed_files = 0
        partial_files = 0
        
        for filename, file_result in self.main_window.file_processing_results.items():
            steps_data = file_result['steps']
            if not steps_data:
                continue
            
            # Check if file is fully processed (all configured steps completed)
            expected_steps = set()
            if self.main_window.anonymize_ips_cb.isChecked():
                expected_steps.add("IP Anonymization")
            if self.main_window.remove_dupes_cb.isChecked():
                expected_steps.add("Deduplication")
            if self.main_window.mask_payloads_cb.isChecked():
                expected_steps.add("Payload Masking")
            
            completed_steps = set(steps_data.keys())
            is_fully_completed = expected_steps.issubset(completed_steps)
            
            if is_fully_completed:
                completed_files += 1
                files_status_report += self._generate_completed_file_report(filename, steps_data)
            else:
                partial_files += 1
                files_status_report += self._generate_partial_file_report(filename, completed_steps, expected_steps)
        
        if completed_files == 0 and partial_files > 0:
            files_status_report += f"\nâš ï¸ All files were only partially processed.\n"
            files_status_report += f"   No final output files were created.\n"
        elif completed_files > 0:
            files_status_report += f"\nğŸ“ˆ Summary: {completed_files} completed, {partial_files} partial\n"
        
        files_status_report += f"{'='*separator_length}\n"
        return files_status_report
    
    def _generate_completed_file_report(self, filename: str, steps_data: Dict) -> str:
        """Generate report for completed file"""
        report = f"\nâœ… {filename}\n"
        report += f"   Status: FULLY COMPLETED\n"
        
        # Get final output filename
        step_order = ['Deduplication', 'IP Anonymization', 'Payload Masking']
        final_output = None
        for step_name in reversed(step_order):
            if step_name in steps_data:
                output_file = steps_data[step_name]['data'].get('output_filename')
                if output_file and not output_file.startswith('tmp'):
                    final_output = output_file
                    break
        
        if final_output:
            report += f"   Output File: {final_output}\n"
        
        # Display detailed results
        original_packets = 0
        file_ip_mappings = {}

        # Prioritize getting original packet count from Deduplication step
        if 'Deduplication' in steps_data:
            original_packets = steps_data['Deduplication']['data'].get('total_packets', 0)
        elif 'IP Anonymization' in steps_data:
            original_packets = steps_data['IP Anonymization']['data'].get('total_packets', 0)
        elif 'Payload Masking' in steps_data:
            original_packets = steps_data['Payload Masking']['data'].get('total_packets', 0)

        for step_name in step_order:
            if step_name in steps_data:
                data = steps_data[step_name]['data']

                if step_name == 'IP Anonymization':
                    # Support new AnonStage field names (retrieved from extra_metrics)
                    extra_metrics = data.get('extra_metrics', {})
                    original_ips = data.get('original_ips', extra_metrics.get('original_ips', 0))
                    masked_ips = data.get('anonymized_ips', extra_metrics.get('anonymized_ips', 0))
                    rate = (masked_ips / original_ips * 100) if original_ips > 0 else 0
                    report += f"   ğŸ›¡ï¸  IP Anonymization: {original_ips} â†’ {masked_ips} IPs ({rate:.1f}%)\n"
                    file_ip_mappings = data.get('file_ip_mappings', extra_metrics.get('file_ip_mappings', {}))

                elif step_name == 'Deduplication':
                    unique = data.get('unique_packets', 0)
                    removed = data.get('removed_count', 0)
                    rate = (removed / original_packets * 100) if original_packets > 0 else 0
                    report += f"   ğŸ”„ Deduplication: {removed} removed ({rate:.1f}%)\n"

                elif step_name == 'Payload Masking':
                    # Support new MaskPayloadStage field names
                    masked = data.get('packets_modified', data.get('masked_packets', 0))
                    rate = (masked / original_packets * 100) if original_packets > 0 else 0
                    report += f"   âœ‚ï¸  Payload Masking: {masked} masked ({rate:.1f}%)\n"
        
        # Display IP mappings (if any)
        if file_ip_mappings:
            report += f"   ğŸ”— IP Mappings ({len(file_ip_mappings)}):\n"
            for i, (orig_ip, new_ip) in enumerate(sorted(file_ip_mappings.items()), 1):
                if i <= 5:  # Only display first 5
                    report += f"      {i}. {orig_ip} â†’ {new_ip}\n"
                elif i == 6:
                    report += f"      ... and {len(file_ip_mappings) - 5} more\n"
                    break
        
        return report
    
    def _generate_partial_file_report(self, filename: str, completed_steps: set, expected_steps: set) -> str:
        """Generate report for partially completed file"""
        report = f"\nğŸ”„ {filename}\n"
        report += f"   Status: PARTIALLY PROCESSED (Interrupted)\n"
        report += f"   Completed Steps: {', '.join(completed_steps)}\n"
        report += f"   Missing Steps: {', '.join(expected_steps - completed_steps)}\n"
        report += f"   âŒ No final output file generated\n"
        report += f"   ğŸ—‘ï¸ Temporary files cleaned up automatically\n"
        return report

    def _generate_global_ip_mappings_report(self, separator_length: int, is_partial: bool = False) -> Optional[str]:
        """Generate global IP mapping report"""
        # First check if IP anonymization processing is enabled
        if not self.main_window.anonymize_ips_cb.isChecked():
            return None

        # Check if global IP mapping data exists
        if not self.main_window.global_ip_mappings:
            return None

        # Check if there are fully completed files
        has_completed_files = False
        for filename, file_result in self.main_window.file_processing_results.items():
            expected_steps = set()
            if self.main_window.anonymize_ips_cb.isChecked():
                expected_steps.add("IP Anonymization")
            if self.main_window.remove_dupes_cb.isChecked():
                expected_steps.add("Deduplication")
            if self.main_window.mask_payloads_cb.isChecked():
                expected_steps.add("Payload Masking")

            completed_steps = set(file_result['steps'].keys())
            if expected_steps.issubset(completed_steps):
                has_completed_files = True
                break
        
        if not has_completed_files:
            return None
        
        if is_partial:
            title = "ğŸŒ IP MAPPINGS FROM COMPLETED FILES"
            subtitle = "ğŸ“ IP Mapping Table - From Successfully Completed Files Only:"
        else:
            title = "ğŸŒ GLOBAL IP MAPPINGS (All Files Combined)"
            subtitle = "ğŸ“ Complete IP Mapping Table - Unique Entries Across All Files:"
        
        global_partial_report = f"\n{'='*separator_length}\n{title}\n{'='*separator_length}\n"
        global_partial_report += f"{subtitle}\n"
        global_partial_report += f"   â€¢ Total Unique IPs Mapped: {len(self.main_window.global_ip_mappings)}\n\n"
        
        sorted_global_mappings = sorted(self.main_window.global_ip_mappings.items())
        for i, (orig_ip, new_ip) in enumerate(sorted_global_mappings, 1):
            global_partial_report += f"   {i:2d}. {orig_ip:<16} â†’ {new_ip}\n"
        
        if is_partial:
            global_partial_report += f"\nâœ… All unique IP addresses across files have been\n"
            global_partial_report += f"   successfully anonymized with consistent mappings.\n"
        else:
            global_partial_report += f"\nâœ… All unique IP addresses across {self.main_window.processed_files_count} files have been\n"
            global_partial_report += f"   successfully anonymized with consistent mappings.\n"
        
        global_partial_report += f"{'='*separator_length}\n"
        return global_partial_report
    
    def generate_file_complete_report(self, original_filename: str):
        """Generate complete processing report for a single file"""
        if original_filename not in self.main_window.file_processing_results:
            return

        file_results = self.main_window.file_processing_results[original_filename]
        steps_data = file_results['steps']

        if not steps_data:
            return

        # **Fix**: Remove duplicate file count increment (already counted in main_window.py FILE_END event)
        # self.main_window.processed_files_count += 1  # Remove this line to avoid double counting

        separator_length = 70
        filename_display = original_filename

        # File processing title
        header = f"\n{'='*separator_length}\nğŸ“„ FILE PROCESSING RESULTS: {filename_display}\n{'='*separator_length}"
        self.main_window.summary_text.append(header)

        # Get original packet count (prioritize from Deduplication step as it contains the true original packet count)
        original_packets = 0
        output_filename = None
        if 'Deduplication' in steps_data:
            # Deduplication step's total_packets is the true original packet count
            original_packets = steps_data['Deduplication']['data'].get('total_packets', 0)
            output_filename = steps_data['Deduplication']['data'].get('output_filename')
        elif 'IP Anonymization' in steps_data:
            # If no deduplication step, get from IP anonymization step
            original_packets = steps_data['IP Anonymization']['data'].get('total_packets', 0)
            output_filename = steps_data['IP Anonymization']['data'].get('output_filename')
        elif 'Payload Masking' in steps_data:
            # Finally get from payload masking step
            original_packets = steps_data['Payload Masking']['data'].get('total_packets', 0)
            output_filename = steps_data['Payload Masking']['data'].get('output_filename')
        
        # Get final output filename from the last processing step
        step_order = ['Deduplication', 'IP Anonymization', 'Payload Masking']
        for step_name in reversed(step_order):
            if step_name in steps_data:
                final_output = steps_data[step_name]['data'].get('output_filename')
                if final_output:
                    output_filename = final_output
                    break
        
        # Display original packet count and output filename
        self.main_window.summary_text.append(f"ğŸ“¦ Original Packets: {original_packets:,}")
        if output_filename:
            self.main_window.summary_text.append(f"ğŸ“„ Output File: {output_filename}")
        self.main_window.summary_text.append("")

        # Display step results in processing order
        file_ip_mappings = {}  # Store current file's IP mappings

        # **Debug log**: Display collected step data
        self._logger.info(f"ğŸ” Generating file report: {original_filename}")
        self._logger.info(f"ğŸ” Collected steps: {list(steps_data.keys())}")
        for step_name, step_info in steps_data.items():
            self._logger.info(f"ğŸ”   {step_name}: type={step_info.get('type')}, data_fields={list(step_info.get('data', {}).keys())}")

        # Fix: Get IP mapping information from file-level IP mapping cache
        if hasattr(self.main_window, '_current_file_ips') and original_filename in self.main_window._current_file_ips:
            file_ip_mappings = self.main_window._current_file_ips[original_filename]

        for step_name in step_order:
            if step_name in steps_data:
                step_result = steps_data[step_name]
                step_type = step_result['type']
                data = step_result['data']

                self._logger.info(f"ğŸ” Processing step {step_name}: type={step_type}")

                # For Payload Masking, record detailed data fields
                if step_name == 'Payload Masking':
                    self._logger.info(f"ğŸ” Payload Masking data: packets_processed={data.get('packets_processed')}, packets_modified={data.get('packets_modified')}")
                    self._logger.info(f"ğŸ” Payload Masking data: total_packets={data.get('total_packets')}, masked_packets={data.get('masked_packets')}")
                
                if step_type in ['anonymize_ips', 'mask_ip', 'mask_ips']:  # Support standard naming and legacy naming
                    # Use new IP statistics data
                    original_ips = data.get('original_ips', 0)
                    masked_ips = data.get('anonymized_ips', 0)
                    rate = (masked_ips / original_ips * 100) if original_ips > 0 else 0
                    line = f"  ğŸ›¡ï¸  {step_name:<18} | Original IPs: {original_ips:>3} | Anonymized IPs: {masked_ips:>3} | Rate: {rate:5.1f}%"

                    # IP mappings already retrieved from cache above

                elif step_type == 'remove_dupes':
                    unique = data.get('unique_packets', 0)
                    removed = data.get('removed_count', 0)
                    total_before = data.get('total_packets', 0)
                    rate = (removed / total_before * 100) if total_before > 0 else 0
                    line = f"  ğŸ”„ {step_name:<18} | Unique Pkts: {unique:>4} | Removed Pkts: {removed:>4} | Rate: {rate:5.1f}%"

                elif step_type in ['mask_payloads']:  # Use standard naming
                    # Fix: MaskStage returns different field names
                    total = data.get('total_packets', data.get('packets_processed', 0))
                    masked = data.get('masked_packets', data.get('packets_modified', 0))
                    rate = (masked / total * 100) if total > 0 else 0

                    # Check if this is Enhanced Masking intelligent processing result
                    if self._is_enhanced_masking(data):
                        line = self._generate_enhanced_masking_report_line(step_name, data)
                    else:
                        line = f"  âœ‚ï¸  {step_name:<18} | Total Pkts: {total:>5} | Masked Pkts: {masked:>4} | Rate: {rate:5.1f}%"
                else:
                    continue

                self.main_window.summary_text.append(line)

        # If IP mappings exist, display file-level IP mappings
        if file_ip_mappings:
            self.main_window.summary_text.append("")
            self.main_window.summary_text.append("ğŸ”— IP Mappings for this file:")
            sorted_mappings = sorted(file_ip_mappings.items())
            for i, (orig_ip, new_ip) in enumerate(sorted_mappings, 1):
                self.main_window.summary_text.append(f"   {i:2d}. {orig_ip:<16} â†’ {new_ip}")
        
        # If Enhanced Masking was used, display intelligent processing details
        enhanced_report = self._generate_enhanced_masking_report_for_file(original_filename, separator_length)
        if enhanced_report:
            self.main_window.summary_text.append(enhanced_report)

        self.main_window.summary_text.append(f"{'='*separator_length}")

    def generate_processing_finished_report(self):
        """Generate report when processing is complete"""
        separator_length = 70  # Maintain consistent separator length

        # **Fix**: Save current statistics data before stopping timer and resetting statistics
        # Ensure Live Dashboard display data is not affected by reset
        current_files_processed = self.main_window.files_processed_count
        current_packets_processed = self.main_window.packets_processed_count
        current_time_elapsed = self.main_window.time_elapsed_label.text()

        # Stop timer
        if self.main_window.timer and self.main_window.timer.isActive():
            self.main_window.timer.stop()

        # Stop statistics manager timing
        if hasattr(self.main_window, 'pipeline_manager') and hasattr(self.main_window.pipeline_manager, 'statistics'):
            self.main_window.pipeline_manager.statistics.stop_timing()

        self.main_window.update_time_elapsed()

        enabled_steps = []
        if self.main_window.remove_dupes_cb.isChecked():
            enabled_steps.append("Deduplication")
        if self.main_window.anonymize_ips_cb.isChecked():
            enabled_steps.append("IP Anonymization")
        if self.main_window.mask_payloads_cb.isChecked():
            enabled_steps.append("Payload Masking")

        completion_report = f"\n{'='*separator_length}\nğŸ‰ PROCESSING COMPLETED!\n{'='*separator_length}\n"
        completion_report += f"ğŸ¯ All {current_files_processed} files have been successfully processed.\n"
        completion_report += f"ğŸ“ˆ Files Processed: {current_files_processed}\n"
        completion_report += f"ğŸ“Š Total Packets Processed: {current_packets_processed}\n"
        completion_report += f"â±ï¸ Time Elapsed: {current_time_elapsed}\n"
        completion_report += f"ğŸ”§ Applied Processing Steps: {', '.join(enabled_steps)}\n"
        
        # Safely handle output directory display
        if self.main_window.current_output_dir:
            completion_report += f"ğŸ“ Output Location: {os.path.basename(self.main_window.current_output_dir)}\n"
        else:
            completion_report += f"ğŸ“ Output Location: Not specified\n"

        completion_report += f"ğŸ“ All processed files saved to output directory.\n"
        completion_report += f"{'='*separator_length}\n"

        self.main_window.summary_text.append(completion_report)

        # Fix: Add global IP mapping summary report (display deduplicated global IP mappings for multi-file processing)
        global_ip_report = self._generate_global_ip_mappings_report(separator_length, is_partial=False)
        if global_ip_report:
            self.main_window.summary_text.append(global_ip_report)

        # Add Enhanced Masking intelligent processing total report
        enhanced_masking_report = self._generate_enhanced_masking_report(separator_length, is_partial=False)
        if enhanced_masking_report:
            self.main_window.summary_text.append(enhanced_masking_report)

        # Fix: Automatically save Summary Report to output directory after processing completion
        self._save_summary_report_to_output()

    def set_final_summary_report(self, report: dict):
        """Set final summary report, including detailed IP mapping information."""
        subdir = report.get('path', 'N/A')
        stats = report.get('stats', {})
        total_mapping = report.get('data', {}).get('total_mapping', {})
        
        separator_length = 70  # Maintain consistent separator length

        # Add IP mapping summary information, including detailed mapping table
        text = f"\n{'='*separator_length}\nğŸ“‹ DIRECTORY PROCESSING SUMMARY\n{'='*separator_length}\n"
        text += f"ğŸ“‚ Directory: {subdir}\n\n"
        text += f"ğŸ”’ IP Anonymization Summary:\n"
        text += f"   â€¢ Total Unique IPs Discovered: {stats.get('total_unique_ips', 'N/A')}\n"
        text += f"   â€¢ Total IPs Anonymized: {stats.get('total_mapped_ips', 'N/A')}\n\n"
        
        if total_mapping:
            text += f"ğŸ“ Complete IP Mapping Table (All Files):\n"
            # Display mappings sorted by original IP
            sorted_mappings = sorted(total_mapping.items())
            for i, (orig_ip, new_ip) in enumerate(sorted_mappings, 1):
                text += f"   {i:2d}. {orig_ip:<16} â†’ {new_ip}\n"
            text += "\n"

        text += f"âœ… All IP addresses have been successfully anonymized while\n"
        text += f"   preserving network structure and subnet relationships.\n"
        text += f"{'='*separator_length}\n"

        self.main_window.summary_text.append(text)

    def update_summary_report(self, data: Dict[str, Any]):
        """Update summary report display"""
        try:
            # Generate different reports based on data type
            if 'filename' in data:
                # Single file processing report
                self._update_file_summary(data)
            elif 'step_results' in data:
                # Overall processing summary
                self._update_overall_summary(data)
            else:
                step_type = data.get('type')
                if step_type and step_type.endswith('_final'):
                    report_data = data.get('report')
                    if report_data and 'mask_ip' in step_type:
                        self.set_final_summary_report(report_data)
                else:
                    self._logger.warning(f"Unknown summary report data format: {data.keys()}")

        except Exception as e:
            self._logger.error(f"Error occurred while updating summary report: {e}")

    def _update_file_summary(self, data: Dict[str, Any]):
        """Update single file processing summary"""
        filename = data.get('filename', 'Unknown file')

        # Get current summary text
        current_text = self.main_window.summary_text.toPlainText()

        # Generate file summary
        file_summary = self._generate_file_summary_text(data)

        # Append to existing text
        if current_text.strip():
            updated_text = current_text + "\n\n" + file_summary
        else:
            updated_text = file_summary

        self.main_window.summary_text.setPlainText(updated_text)

        # Scroll to bottom
        cursor = self.main_window.summary_text.textCursor()
        cursor.movePosition(cursor.MoveOperation.End)
        self.main_window.summary_text.setTextCursor(cursor)
    
    def _update_overall_summary(self, data: Dict[str, Any]):
        """Update overall processing summary"""
        summary_text = self._generate_overall_summary_text(data)
        self.main_window.summary_text.setPlainText(summary_text)
    
    def _generate_file_summary_text(self, data: Dict[str, Any]) -> str:
        """Generate single file summary text"""
        filename = data.get('filename', 'Unknown file')
        summary_parts = [f"ğŸ“„ {filename}"]
        
        # Processing result statistics
        results = data.get('results', {})
        for step_name, result in results.items():
            if isinstance(result, dict):
                if 'summary' in result:
                    summary_parts.append(f"  â€¢ {step_name}: {result['summary']}")
                elif 'packets_processed' in result:
                    summary_parts.append(f"  â€¢ {step_name}: {result['packets_processed']} packets")
                elif 'ips_anonymized' in result:
                    summary_parts.append(f"  â€¢ {step_name}: {result['ips_anonymized']} IPs anonymized")
        
        return '\n'.join(summary_parts)
    
    def _generate_overall_summary_text(self, data: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ•´ä½“æ‘˜è¦æ–‡æœ¬"""
        summary_parts = ["ğŸ“Š Processing Summary", "=" * 50]
        
        # åŸºæœ¬ç»Ÿè®¡
        files_processed = data.get('files_processed', 0)
        total_files = data.get('total_files', 0)
        status = data.get('status', 'unknown')
        
        summary_parts.append(f"Status: {status.title().replace('_', ' ')}")
        summary_parts.append(f"Files Processed: {files_processed}/{total_files}")
        
        if files_processed > 0 and total_files > 0:
            percentage = (files_processed / total_files) * 100
            summary_parts.append(f"Completion: {percentage:.1f}%")
        
        summary_parts.append("")
        
        # æ­¥éª¤ç»Ÿè®¡
        step_results = data.get('step_results', {})
        if step_results:
            summary_parts.append("ğŸ“ˆ Step Statistics:")
            
            # èšåˆå„æ­¥éª¤çš„ç»Ÿè®¡
            step_stats = self._aggregate_step_statistics(step_results)
            
            for step_name, stats in step_stats.items():
                summary_parts.append(f"\nğŸ”§ {step_name}:")
                for key, value in stats.items():
                    summary_parts.append(f"  â€¢ {key}: {value}")
        
        # è¾“å‡ºç›®å½•ä¿¡æ¯
        output_dir = data.get('output_directory')
        if output_dir:
            summary_parts.append(f"\nğŸ“ Output Directory:")
            summary_parts.append(f"  {output_dir}")
        
        # æ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        summary_parts.append(f"\nâ° Generated: {timestamp}")
        
        return '\n'.join(summary_parts)
    
    def _generate_final_summary_text(self, data: Dict[str, Any]) -> str:
        """ç”Ÿæˆæœ€ç»ˆæ‘˜è¦æ–‡æœ¬"""
        summary_parts = ["ğŸ¯ Final Processing Report", "=" * 60]
        
        # å¤„ç†çŠ¶æ€
        status = data.get('status', 'unknown')
        files_processed = data.get('files_processed', 0)
        total_files = data.get('total_files', 0)
        
        if status == 'completed':
            summary_parts.append("âœ… Status: Successfully Completed")
        elif status == 'stopped_by_user':
            summary_parts.append("â¹ï¸ Status: Stopped by User")
        else:
            summary_parts.append(f"â“ Status: {status.title()}")
        
        summary_parts.append(f"ğŸ“Š Files Processed: {files_processed} of {total_files}")
        
        if files_processed > 0 and total_files > 0:
            percentage = (files_processed / total_files) * 100
            summary_parts.append(f"ğŸ“ˆ Completion Rate: {percentage:.1f}%")
        
        summary_parts.append("")
        
        # è¯¦ç»†ç»Ÿè®¡
        step_results = data.get('step_results', {})
        if step_results:
            summary_parts.append("ğŸ“‹ Detailed Statistics:")
            summary_parts.append("-" * 40)
            
            # èšåˆç»Ÿè®¡
            aggregated_stats = self._aggregate_step_statistics(step_results)
            
            for step_name, stats in aggregated_stats.items():
                summary_parts.append(f"\nğŸ”§ {step_name}:")
                for stat_name, stat_value in stats.items():
                    summary_parts.append(f"  â€¢ {stat_name}: {stat_value}")
        
        # æ–‡ä»¶è¯¦æƒ…
        if step_results:
            summary_parts.append("\nğŸ“„ File Details:")
            summary_parts.append("-" * 30)
            
            for filename, file_results in step_results.items():
                summary_parts.append(f"\nğŸ“ {filename}:")
                for step_name, result in file_results.items():
                    if isinstance(result, dict):
                        if 'summary' in result:
                            summary_parts.append(f"  â€¢ {step_name}: {result['summary']}")
                        else:
                            summary_parts.append(f"  â€¢ {step_name}: Processed")
        
        # è¾“å‡ºä¿¡æ¯
        output_dir = data.get('output_directory')
        if output_dir:
            summary_parts.append(f"\nğŸ“‚ Output Location:")
            summary_parts.append(f"  {output_dir}")
        
        # ç”Ÿæˆæ—¶é—´
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        summary_parts.append(f"\nğŸ•’ Report Generated: {timestamp}")
        
        # å·¥å…·ä¿¡æ¯
        summary_parts.append("\n" + "=" * 60)
        summary_parts.append("ğŸ› ï¸ Generated by PktMask - Network Packet Processing Tool")
        
        return '\n'.join(summary_parts)
    
    def _aggregate_step_statistics(self, step_results: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """èšåˆæ­¥éª¤ç»Ÿè®¡ä¿¡æ¯"""
        aggregated = {}
        
        for filename, file_results in step_results.items():
            for step_name, result in file_results.items():
                if step_name not in aggregated:
                    aggregated[step_name] = {}
                
                if isinstance(result, dict):
                    for key, value in result.items():
                        if isinstance(value, (int, float)):
                            # æ•°å€¼ç±»å‹è¿›è¡Œç´¯åŠ 
                            if key in aggregated[step_name]:
                                aggregated[step_name][key] += value
                            else:
                                aggregated[step_name][key] = value
                        elif key == 'summary' and isinstance(value, str):
                            # æ‘˜è¦ä¿¡æ¯æ”¶é›†åˆ°åˆ—è¡¨
                            summary_key = 'summaries'
                            if summary_key not in aggregated[step_name]:
                                aggregated[step_name][summary_key] = []
                            aggregated[step_name][summary_key].append(f"{filename}: {value}")
        
        # åå¤„ç†ï¼šè®¡ç®—å¹³å‡å€¼ã€æ ¼å¼åŒ–æ˜¾ç¤ºç­‰
        for step_name, stats in aggregated.items():
            if 'summaries' in stats:
                # å°†æ‘˜è¦åˆ—è¡¨è½¬æ¢ä¸ºè®¡æ•°
                summaries = stats.pop('summaries')
                stats['files_processed'] = len(summaries)
        
        return aggregated
    
    def clear_displays(self):
        """æ¸…ç©ºæ˜¾ç¤ºåŒºåŸŸ"""
        try:
            self.main_window.log_text.clear()
            self.main_window.summary_text.clear()
            self._logger.debug("Cleared log and summary display")
            
        except Exception as e:
            self._logger.error(f"Error occurred while clearing display area: {e}")
    
    def export_summary_report(self, filepath: str) -> bool:
        """å¯¼å‡ºæ‘˜è¦æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        try:
            content = self.main_window.summary_text.toPlainText()
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            
            self._logger.info(f"Summary report exported to: {filepath}")
            return True
            
        except Exception as e:
            self._logger.error(f"Failed to export summary report: {e}")
            return False
    
    def _save_summary_report_to_output(self):
        """ç§æœ‰æ–¹æ³•ï¼šä¿å­˜æ‘˜è¦æŠ¥å‘Šåˆ°è¾“å‡ºç›®å½•"""
        try:
            # å§”æ‰˜ç»™FileManageræˆ–ä½¿ç”¨MainWindowçš„ç°æœ‰æ–¹æ³•
            if hasattr(self.main_window, 'save_summary_report_to_output_dir'):
                self.main_window.save_summary_report_to_output_dir()
            elif hasattr(self.main_window, 'file_manager'):
                self.main_window.file_manager.save_summary_report_to_output_dir()
            else:
                self._logger.warning("Cannot find method to save Summary Report")
        except Exception as e:
            self._logger.error(f"Failed to save Summary Report to output directory: {e}")

    def collect_step_result(self, data: dict):
        """æ”¶é›†æ¯ä¸ªæ­¥éª¤çš„å¤„ç†ç»“æœï¼Œä½†ä¸ç«‹å³æ˜¾ç¤º"""
        if not self.main_window.current_processing_file:
            return
            
        step_type = data.get('type')
        step_name_raw = data.get('step_name', '')
        
        # **è°ƒè¯•æ—¥å¿—**: è®°å½•æ”¶é›†çš„æ­¥éª¤ç»“æœ
        self._logger.info(f"ğŸ” Collecting step results: file={self.main_window.current_processing_file}, step={step_name_raw}, type={step_type}")
        self._logger.info(f"ğŸ” Data fields: {list(data.keys())}")
        
        # **ä¿®å¤**: æ”¯æŒæ–°Pipelineç³»ç»Ÿçš„æ­¥éª¤åç§°
        # ä»step_nameæ¨æ–­æ­¥éª¤ç±»å‹ï¼Œè€Œä¸æ˜¯ä»…ä¾èµ–typeå­—æ®µ
        if not step_type:
            # æ–°Pipelineç³»ç»Ÿæ²¡æœ‰typeå­—æ®µï¼Œä»step_nameæ¨æ–­
            if step_name_raw == 'AnonStage':
                step_type = 'anonymize_ips'  # Use standard naming
            elif step_name_raw in ['DedupStage', 'DeduplicationStage']:
                step_type = 'remove_dupes'
            elif step_name_raw in ['MaskStage', 'MaskPayloadStage', 'NewMaskPayloadStage', 'Mask Payloads (v2)', 'Payload Masking Stage']:
                step_type = 'mask_payloads'  # Use standard naming
            else:
                step_type = step_name_raw.lower()
        
        self._logger.info(f"ğŸ” Inferred step type: {step_type}")
        
        if not step_type or step_type.endswith('_final'):
            if step_type and step_type.endswith('_final'):
                # å¤„ç†æœ€ç»ˆæŠ¥å‘Šï¼Œæå–IPæ˜ å°„ä¿¡æ¯
                report_data = data.get('report')
                if report_data and 'anonymize_ips' in step_type:
                    self.set_final_summary_report(report_data)
            return
        
        # æ ‡å‡†åŒ–æ­¥éª¤åç§° - ä¿®å¤Pipelineå’ŒReportManagerä¹‹é—´çš„æ˜ å°„ä¸åŒ¹é…
        step_display_names = {
            'anonymize_ips': 'IP Anonymization',  # Standard naming
            'remove_dupes': 'Deduplication',
            'mask_payloads': 'Payload Masking',   # Standard naming
        }
        
        step_name = step_display_names.get(step_type, step_type)
        
        # å­˜å‚¨æ­¥éª¤ç»“æœ
        self.main_window.file_processing_results[self.main_window.current_processing_file]['steps'][step_name] = {
            'type': step_type,
            'data': data
        }
        
        # **å…³é”®ä¿®å¤**: å¦‚æœæ˜¯IPåŒ¿ååŒ–æ­¥éª¤ï¼Œæå–å¹¶ç´¯ç§¯IPæ˜ å°„åˆ°å…¨å±€æ˜ å°„
        is_ip_anonymization = (
            step_type in ['anonymize_ips'] or
            step_name_raw == 'AnonStage' or
            'ip_mappings' in data or
            'file_ip_mappings' in data
        )
        
        if is_ip_anonymization:
            # ä»stepæ•°æ®ä¸­æå–IPæ˜ å°„
            ip_mappings = None
            if 'file_ip_mappings' in data:
                ip_mappings = data['file_ip_mappings']
            elif 'ip_mappings' in data:
                ip_mappings = data['ip_mappings']
            elif 'extra_metrics' in data:
                # æ£€æŸ¥extra_metricsä¸­çš„IPæ˜ å°„ï¼ˆæ–°Pipelineç³»ç»Ÿï¼‰
                extra_metrics = data['extra_metrics']
                if 'file_ip_mappings' in extra_metrics:
                    ip_mappings = extra_metrics['file_ip_mappings']
                elif 'ip_mappings' in extra_metrics:
                    ip_mappings = extra_metrics['ip_mappings']
            
            if ip_mappings and isinstance(ip_mappings, dict):
                # ä¿å­˜æ–‡ä»¶çº§IPæ˜ å°„
                if not hasattr(self.main_window, '_current_file_ips'):
                    self.main_window._current_file_ips = {}
                self.main_window._current_file_ips[self.main_window.current_processing_file] = ip_mappings
                
                # **å…³é”®ä¿®å¤**: å°†å½“å‰æ–‡ä»¶çš„IPæ˜ å°„ç´¯ç§¯åˆ°å…¨å±€æ˜ å°„ä¸­ï¼ˆä¸è¦†ç›–ï¼‰
                if not hasattr(self.main_window, 'global_ip_mappings') or self.main_window.global_ip_mappings is None:
                    self.main_window.global_ip_mappings = {}
                
                # ç´¯ç§¯æ˜ å°„è€Œä¸æ˜¯è¦†ç›–
                self.main_window.global_ip_mappings.update(ip_mappings)

                self._logger.info(f"âœ… Collected IP mappings: file={self.main_window.current_processing_file}, new_mappings={len(ip_mappings)}, global_total={len(self.main_window.global_ip_mappings)}")
            else:
                self._logger.warning(f"IP anonymization step completed, but no valid IP mapping data found: {list(data.keys())}")
        else:
            self._logger.debug(f"Non-IP anonymization step: {step_name_raw}")

    def _is_enhanced_masking(self, data: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯å¢å¼ºæ©ç å¤„ç†ç»“æœ - åŸºäºåŒæ¨¡å—æ¶æ„"""
        step_name = data.get('step_name', '')

        # æ£€æŸ¥Enhanced Maskingç‰¹æœ‰çš„å­—æ®µç»„åˆ - å¿…é¡»æ˜¯çœŸæ­£çš„Enhanced Intelligent Mode
        enhanced_indicators = [
            'processing_mode' in data and data.get('processing_mode') == 'Enhanced Intelligent Mode',
            'protocol_stats' in data,
            'strategies_applied' in data,
            'enhancement_level' in data,
        ]

        # Protocol adaptation mode has different processing mode identifiers, not 'Enhanced Intelligent Mode'
        # å¦‚æœæœ‰çœŸæ­£çš„Enhanced Maskingç‰¹æœ‰å­—æ®µç»„åˆï¼Œè®¤ä¸ºæ˜¯æ™ºèƒ½å¤„ç†
        return all(enhanced_indicators[:3])  # å‰3ä¸ªå­—æ®µå¿…é¡»éƒ½å­˜åœ¨

    def _generate_enhanced_masking_report_line(self, step_name: str, data: Dict[str, Any]) -> str:
        """ç”ŸæˆEnhanced Maskingçš„å¤„ç†ç»“æœæŠ¥å‘Šè¡Œï¼ˆç§»é™¤HTTPç»Ÿè®¡ï¼‰"""
        total = data.get('total_packets', 0)
        masked = data.get('masked_packets', 0)

        # è·å–åè®®ç»Ÿè®¡ï¼ˆç§»é™¤HTTPï¼‰
        protocol_stats = data.get('protocol_stats', {})
        tls_packets = protocol_stats.get('tls_packets', 0)
        other_packets = protocol_stats.get('other_packets', 0)

        # åŸºç¡€æŠ¥å‘Šè¡Œï¼ˆå¢å¼ºæ¨¡å¼æ ‡è¯†ï¼‰
        rate = (masked / total * 100) if total > 0 else 0
        line = f"  âœ‚ï¸  {step_name:<18} | Enhanced Mode | Total: {total:>4} | Masked: {masked:>4} | Rate: {rate:5.1f}%"

        return line

    def _generate_enhanced_masking_report(self, separator_length: int, is_partial: bool = False) -> Optional[str]:
        """ç”ŸæˆEnhanced Maskingçš„æ™ºèƒ½å¤„ç†ç»Ÿè®¡æ€»æŠ¥å‘Šï¼ˆç§»é™¤HTTPæ”¯æŒï¼‰"""
        # æ£€æŸ¥æ˜¯å¦æœ‰Enhanced Maskingå¤„ç†çš„æ–‡ä»¶
        enhanced_files = []
        total_enhanced_stats = {
            'total_packets': 0,
            'tls_packets': 0,
            'other_packets': 0,
            'strategies_applied': set(),
            'files_processed': 0
        }

        # éå†æ‰€æœ‰å¤„ç†è¿‡çš„æ–‡ä»¶ï¼Œæ‰¾å‡ºä½¿ç”¨Enhanced Maskingçš„æ–‡ä»¶
        for filename, file_result in self.main_window.file_processing_results.items():
            steps_data = file_result.get('steps', {})
            payload_step = steps_data.get('Payload Masking')

            if payload_step and self._is_enhanced_masking(payload_step.get('data', {})):
                enhanced_files.append(filename)
                data = payload_step['data']
                
                # æ±‡æ€»ç»Ÿè®¡ï¼ˆç§»é™¤HTTPï¼‰
                total_enhanced_stats['files_processed'] += 1
                total_enhanced_stats['total_packets'] += data.get('total_packets', 0)
                
                protocol_stats = data.get('protocol_stats', {})
                total_enhanced_stats['tls_packets'] += protocol_stats.get('tls_packets', 0)
                total_enhanced_stats['other_packets'] += protocol_stats.get('other_packets', 0)
                
                strategies = data.get('strategies_applied', [])
                total_enhanced_stats['strategies_applied'].update(strategies)
        
        # å¦‚æœæ²¡æœ‰Enhanced Maskingå¤„ç†ï¼Œè¿”å›None
        if not enhanced_files:
            return None

        # ç”Ÿæˆæ™ºèƒ½å¤„ç†æ€»æŠ¥å‘Š
        title = "ğŸ§  ENHANCED MASKING INTELLIGENCE REPORT"
        if is_partial:
            title += " (Partial)"

        report = f"\n{'='*separator_length}\n{title}\n{'='*separator_length}\n"

        # å¤„ç†æ¨¡å¼å’Œå¢å¼ºä¿¡æ¯
        report += f"ğŸ¯ Processing Mode: Intelligent Auto-Detection\n"
        report += f"âš¡ Enhancement Level: 4x accuracy improvement over simple masking\n"
        report += f"ğŸ“ Enhanced Files: {total_enhanced_stats['files_processed']}/{len(self.main_window.file_processing_results)}\n\n"
        
        # åè®®æ£€æµ‹ç»Ÿè®¡ï¼ˆç§»é™¤HTTPï¼‰
        total_packets = total_enhanced_stats['total_packets']
        if total_packets > 0:
            tls_rate = (total_enhanced_stats['tls_packets'] / total_packets) * 100
            other_rate = (total_enhanced_stats['other_packets'] / total_packets) * 100
            
            report += f"ğŸ“Š Protocol Detection Results:\n"
            report += f"   â€¢ TLS packets: {total_enhanced_stats['tls_packets']:,} ({tls_rate:.1f}%) - Intelligent TLS strategy\n"
            report += f"   â€¢ Other packets: {total_enhanced_stats['other_packets']:,} ({other_rate:.1f}%) - General strategy\n"
            report += f"   â€¢ Total processed: {total_packets:,} packets in 4 stages\n\n"
        
        # ç­–ç•¥åº”ç”¨ç»Ÿè®¡
        strategies_list = list(total_enhanced_stats['strategies_applied'])
        if strategies_list:
            report += f"ğŸ”§ Applied Strategies:\n"
            for strategy in sorted(strategies_list):
                report += f"   â€¢ {strategy}\n"
            report += "\n"
        
        # æ™ºèƒ½å¤„ç†ä¼˜åŠ¿è¯´æ˜ï¼ˆç§»é™¤HTTPï¼‰
        report += f"ğŸš€ Enhanced Processing Benefits:\n"
        report += f"   â€¢ Automatic protocol detection and strategy selection\n"
        report += f"   â€¢ TLS handshake preserved, ApplicationData masked\n"
        report += f"   â€¢ Improved accuracy while maintaining network analysis capability\n"
        
        report += f"{'='*separator_length}\n"
        
        return report

    def _generate_enhanced_masking_report_for_file(self, filename: str, separator_length: int) -> Optional[str]:
        """ä¸ºå•ä¸ªæ–‡ä»¶ç”ŸæˆEnhanced Maskingçš„å¤„ç†ç»“æœæŠ¥å‘Šï¼ˆç§»é™¤HTTPç»Ÿè®¡ï¼‰"""
        if filename not in self.main_window.file_processing_results:
            return None

        file_result = self.main_window.file_processing_results[filename]
        steps_data = file_result.get('steps', {})
        payload_step = steps_data.get('Payload Masking')

        if not payload_step or not self._is_enhanced_masking(payload_step.get('data', {})):
            return None

        data = payload_step['data']
        protocol_stats = data.get('protocol_stats', {})

        report = f"\nğŸ§  Enhanced Masking Details for {filename}:\n"
        report += f"   ğŸ“Š Protocol Analysis:\n"
        
        total_packets = data.get('total_packets', 0)
        if total_packets > 0:
            tls_packets = protocol_stats.get('tls_packets', 0)
            other_packets = protocol_stats.get('other_packets', 0)
            
            if tls_packets > 0:
                tls_rate = (tls_packets / total_packets) * 100
                report += f"      â€¢ TLS: {tls_packets} packets ({tls_rate:.1f}%) - Handshake preserved\n"
            
            if other_packets > 0:
                other_rate = (other_packets / total_packets) * 100
                report += f"      â€¢ Other: {other_packets} packets ({other_rate:.1f}%) - Generic strategy\n"
        
        # ç­–ç•¥åº”ç”¨ä¿¡æ¯
        strategies = data.get('strategies_applied', [])
        if strategies:
            report += f"   ğŸ”§ Applied Strategies: {', '.join(strategies)}\n"
        
        # å¤„ç†æ•ˆç‡
        enhancement_level = data.get('enhancement_level', 'Not specified')
        report += f"   âš¡ Enhancement: {enhancement_level}\n"
        
        return report

    def _generate_enhanced_masking_report_for_directory(self, separator_length: int) -> Optional[str]:
        """ç”Ÿæˆæ•´ä¸ªç›®å½•çš„Enhanced Maskingçš„å¤„ç†ç»“æœæŠ¥å‘Š"""
        # è¿™ä¸ªæ–¹æ³•åœ¨ç›®å½•çº§åˆ«å¤„ç†å®Œæˆæ—¶è°ƒç”¨
        # ç›®å‰å…ˆè¿”å›é€šç”¨çš„EnhancedæŠ¥å‘Š
        return self._generate_enhanced_masking_report(separator_length, is_partial=False)